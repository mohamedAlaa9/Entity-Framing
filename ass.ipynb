{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi\n"
     ]
    }
   ],
   "source": [
    "print(\"Hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Climate Change Hoaxer Greta Thunberg Charged for Disobeying Law Enforcement in Sweden’s Climate Protest – May Face Up to Six Months in Prison \n",
      "\n",
      " Sweden says enough!\n",
      "\n",
      "Sweden’s Prosecution Authority is bringing charges against none other than their homegrown eco-hoaxer, Greta Thunberg, for disobeying law enforcement during a climate protest in June.\n",
      "\n",
      "The 20-year-old is facing potential fines or imprisonment of up to six months.\n",
      "\n",
      "The charges stem from her involvement in a protest that allegedly led to major traffic disruption in Malmö in June, authorities said.\n",
      "\n",
      "“The prosecutor has filed charges against a young woman who, on June 19 this year, participated in a climate demonstration which, according to the prosecution, caused disruption to traffic in Malmö,” the statement said. The woman “refused to obey the police command to leave the scene,” it added according to CNN.\n",
      "\n",
      "While the prosecution authority’s statement did not specifically name Thunberg, a spokesperson for the Swedish Prosecution Authority, Annika Collin, confirmed to CNN that the individual referred to in the charges is indeed Greta Thunberg.\n",
      "\n",
      "“The criminal classification is disobedience to law enforcement. According to the prosecutor, the act was committed with intent,” the prosecution authority’s statement added.\n",
      "\n",
      "Irma Kjellström, a spokesperson for Ta Tillbaka Framtiden, told CNN that Thunberg was among several young people who participated in blocking the oil tankers.\n",
      "\n",
      "Kjellström added, “After having blocked the industry which is burning our future, we have now been charged with crime. While charges are being brought against us, the real crime continues inside the gates that we blocked.”\n",
      "\n",
      "Recently, Greta Thunberg was back in the news after her prediction that the world would end in 2023 was a conspiracy.\n",
      "\n",
      "On June 21, 2018, she made a bold claim on Twitter, stating that humanity had a narrow five-year window to stop the use of fossil fuels or face inevitable extinction.\n",
      "\n",
      "“A top climate scientist is warning that climate change will wipe out all of humanity unless we stop using fossil fuels over the next five years.”\n",
      "\n",
      "Thunberg shared a now-deleted Grit Post article by Scott Alden citing a prediction from James Anderson, a professor of atmospheric chemistry at Harvard University, titled, “Top Climate Scientist: Humans Will Go Extinct if We Don’t Fix Climate Change by 2023.”\n"
     ]
    }
   ],
   "source": [
    "file_path = '/Users/mohamedelzeftawy/Documents/MBZUAI/NLP/Assignment2/training_data_16_October_release/EN/raw-documents/EN_CC_100095.txt'\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    text_data = file.read()\n",
    "\n",
    "print(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Greta Thunber'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data[269:282]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'file_id': 'EN_UA_103861.txt', 'entity': 'Chinese', 'start_offset': 791, 'end_offset': 797, 'broad_roles': 'Antagonist', 'fine_grained_roles': ['Spy']}\n",
      "{'file_id': 'EN_UA_103861.txt', 'entity': 'China', 'start_offset': 1516, 'end_offset': 1520, 'broad_roles': 'Antagonist', 'fine_grained_roles': ['Instigator']}\n",
      "{'file_id': 'EN_UA_103861.txt', 'entity': 'Hamas', 'start_offset': 2121, 'end_offset': 2125, 'broad_roles': 'Antagonist', 'fine_grained_roles': ['Terrorist']}\n",
      "{'file_id': 'EN_UA_103861.txt', 'entity': 'Donald Trump', 'start_offset': 4909, 'end_offset': 4920, 'broad_roles': 'Protagonist', 'fine_grained_roles': ['Peacemaker', 'Guardian']}\n",
      "{'file_id': 'EN_UA_021270.txt', 'entity': 'Yermak', 'start_offset': 667, 'end_offset': 672, 'broad_roles': 'Antagonist', 'fine_grained_roles': ['Incompetent']}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Path to your annotations file\n",
    "annotations_file = \"/Users/mohamedelzeftawy/Documents/MBZUAI/NLP/Assignment2/training_data_16_October_release/EN/subtask-1-annotations.txt\"\n",
    "\n",
    "# Read the file\n",
    "annotations = []\n",
    "with open(annotations_file, 'r') as file:\n",
    "    for line in file:\n",
    "        # Split the line into columns\n",
    "        parts = line.strip().split('\\t')\n",
    "        \n",
    "        # Store the parsed line as a dictionary\n",
    "        annotation = {\n",
    "            \"file_id\": parts[0],\n",
    "            \"entity\": parts[1],\n",
    "            \"start_offset\": int(parts[2]),\n",
    "            \"end_offset\": int(parts[3]),\n",
    "            \"broad_roles\": parts[4] if len(parts) > 4 else [],\n",
    "            \"fine_grained_roles\": parts[5:] if len(parts) > 5 else []\n",
    "        }\n",
    "        # if len(parts) > 6:\n",
    "        #     print(parts[5]) if parts[5] == \"Antagonist\" or parts[5] == \"Protagonist\" or parts[5] == \"Innocent\" else print(\"Not Found\")\n",
    "        annotations.append(annotation)\n",
    "\n",
    "# Print the first few annotations to verify\n",
    "for annotation in annotations[:5]:\n",
    "    print(annotation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_df = pd.DataFrame(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fine_grained_roles\n",
       "1    389\n",
       "2     25\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations_df['fine_grained_roles'].apply(lambda x: len(x)).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Instigator', 'Foreign Adversary']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations_df.iloc[47]['fine_grained_roles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_df.to_csv('en_annotations.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached torch-2.5.1-cp39-none-macosx_11_0_arm64.whl.metadata (28 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Using cached filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.9/site-packages (from torch) (4.12.2)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Using cached fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Using cached MarkupSafe-3.0.2-cp39-cp39-macosx_11_0_arm64.whl.metadata (4.0 kB)\n",
      "Using cached torch-2.5.1-cp39-none-macosx_11_0_arm64.whl (63.9 MB)\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Using cached filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Using cached fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
      "Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Using cached networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "Using cached MarkupSafe-3.0.2-cp39-cp39-macosx_11_0_arm64.whl (12 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, networkx, MarkupSafe, fsspec, filelock, jinja2, torch\n",
      "Successfully installed MarkupSafe-3.0.2 filelock-3.16.1 fsspec-2024.10.0 jinja2-3.1.4 mpmath-1.3.0 networkx-3.2.1 sympy-1.13.1 torch-2.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.9/site-packages (from transformers) (3.16.1)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Downloading huggingface_hub-0.26.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.9/site-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.9/site-packages (from transformers) (24.2)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Using cached PyYAML-6.0.2-cp39-cp39-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2024.11.6-cp39-cp39-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting requests (from transformers)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers)\n",
      "  Using cached tokenizers-0.20.3-cp39-cp39-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Using cached safetensors-0.4.5-cp39-cp39-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->transformers)\n",
      "  Using cached charset_normalizer-3.4.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->transformers)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers)\n",
      "  Using cached urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers)\n",
      "  Using cached certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Using cached transformers-4.46.3-py3-none-any.whl (10.0 MB)\n",
      "Downloading huggingface_hub-0.26.3-py3-none-any.whl (447 kB)\n",
      "Using cached PyYAML-6.0.2-cp39-cp39-macosx_11_0_arm64.whl (172 kB)\n",
      "Using cached regex-2024.11.6-cp39-cp39-macosx_11_0_arm64.whl (284 kB)\n",
      "Using cached safetensors-0.4.5-cp39-cp39-macosx_11_0_arm64.whl (383 kB)\n",
      "Using cached tokenizers-0.20.3-cp39-cp39-macosx_11_0_arm64.whl (2.6 MB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Using cached charset_normalizer-3.4.0-cp39-cp39-macosx_11_0_arm64.whl (120 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Installing collected packages: urllib3, tqdm, safetensors, regex, pyyaml, idna, charset-normalizer, certifi, requests, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed certifi-2024.8.30 charset-normalizer-3.4.0 huggingface-hub-0.26.3 idna-3.10 pyyaml-6.0.2 regex-2024.11.6 requests-2.32.3 safetensors-0.4.5 tokenizers-0.20.3 tqdm-4.67.1 transformers-4.46.3 urllib3-2.2.3\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.5.2-cp39-cp39-macosx_12_0_arm64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in ./.venv/lib/python3.9/site-packages (from scikit-learn) (2.0.2)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Using cached scipy-1.13.1-cp39-cp39-macosx_12_0_arm64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Using cached scikit_learn-1.5.2-cp39-cp39-macosx_12_0_arm64.whl (11.0 MB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached scipy-1.13.1-cp39-cp39-macosx_12_0_arm64.whl (30.3 MB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.5.2 scipy-1.13.1 threadpoolctl-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohamedelzeftawy/Documents/MBZUAI/NLP/Assignment2/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/mohamedelzeftawy/Documents/MBZUAI/NLP/Assignment2/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EvalPrediction,\n",
    "    TrainerCallback,\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, hamming_loss, precision_recall_fscore_support\n",
    "from transformers import TrainingArguments, EarlyStoppingCallback\n",
    "from transformers.trainer_utils import IntervalStrategy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('en_annotations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = {}\n",
    "for file_id in data['file_id'].unique():\n",
    "    with open(f\"training_data_16_October_release/EN/raw-documents/{file_id}\", \"r\") as f:\n",
    "        articles[file_id] = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1011 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def extract_context_within_512(article, start, end, tokenizer, max_len=512):\n",
    "    \"\"\"\n",
    "    Extract a context window around the entity mention, ensuring the total length \n",
    "    (entity + context) fits within max_len tokens, using correct mapping of character \n",
    "    offsets to token indices.\n",
    "    \"\"\"\n",
    "    def add_entity_markers(article, start, end):\n",
    "        \"\"\"\n",
    "        Add special markers around the entity in the text and adjust offsets.\n",
    "        \"\"\"\n",
    "        before = article[:start]\n",
    "        entity = article[start:end]\n",
    "        after = article[end:]\n",
    "        marked_article = before + \"[ENTITY] \" + entity + \" [/ENTITY]\" + after\n",
    "        # 9 for len(\"[ENTITY] \")\n",
    "        adjusted_start = start + 9\n",
    "        adjusted_end = adjusted_start + len(entity)\n",
    "        return marked_article, adjusted_start, adjusted_end\n",
    "    \n",
    "    # Add entity markers and adjust offsets\n",
    "    marked_article, adjusted_start, adjusted_end = add_entity_markers(article, start, end)\n",
    "    # print(adjusted_start, adjusted_end)\n",
    "    # print(marked_article[adjusted_start:adjusted_end])\n",
    "    # Tokenize the marked article\n",
    "    encoding = tokenizer(\n",
    "        marked_article,\n",
    "        return_offsets_mapping=True,\n",
    "        truncation=False,\n",
    "    )\n",
    "    \n",
    "    # Extract offset mapping (character to token mapping)\n",
    "    offsets = encoding['offset_mapping']\n",
    "    \n",
    "    # Locate tokens corresponding to the entity (adjusted start and end)\n",
    "    entity_token_indices = [\n",
    "        idx for idx, (char_start, char_end) in enumerate(offsets)\n",
    "        if char_start >= adjusted_start and char_end <= adjusted_end\n",
    "    ]\n",
    "    \n",
    "    if not entity_token_indices:\n",
    "        raise ValueError(f\"Entity span does not match any token in the article: {marked_article}\")\n",
    "    \n",
    "    # Get the first and last token indices of the entity (including markers)\n",
    "    entity_start_token = entity_token_indices[0]\n",
    "    entity_end_token = entity_token_indices[-1]\n",
    "    \n",
    "    # Determine the maximum context length available for tokens before and after the entity\n",
    "    max_context_tokens = max_len - (entity_end_token - entity_start_token + 1) - 3  # Account for [CLS], [SEP], [SEP]\n",
    "    half_context_tokens = max_context_tokens // 2\n",
    "    \n",
    "    # Tokens before the entity\n",
    "    tokens_before = encoding['input_ids'][\n",
    "        max(0, entity_start_token - half_context_tokens):entity_start_token\n",
    "    ]\n",
    "    \n",
    "    # Tokens after the entity\n",
    "    tokens_after = encoding['input_ids'][\n",
    "        entity_end_token + 1:min(len(encoding['input_ids']), entity_end_token + 1 + half_context_tokens)\n",
    "    ]\n",
    "    \n",
    "    # Combine context and entity tokens\n",
    "    context_tokens = tokens_before + encoding['input_ids'][entity_start_token:entity_end_token + 1] + tokens_after\n",
    "    context = tokenizer.decode(context_tokens, skip_special_tokens=True)\n",
    "    \n",
    "    return context\n",
    "\n",
    "# Add a context column to the DataFrame\n",
    "def compute_context(row):\n",
    "    file_id = row['file_id']\n",
    "    article = articles[file_id]\n",
    "    start_offset = row['start_offset']\n",
    "    end_offset = row['end_offset'] + 1\n",
    "    return extract_context_within_512(\n",
    "        article=article,\n",
    "        start=start_offset,\n",
    "        end=end_offset,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=512\n",
    "    )\n",
    "\n",
    "# Apply the function to create the context column\n",
    "data['context'] = data.apply(compute_context, axis=1)\n",
    "# article = \"The hero saved the town from destruction.\"\n",
    "# start_offset = 4\n",
    "# end_offset = 8\n",
    "\n",
    "# # Assuming `tokenizer` is defined\n",
    "# context = extract_context_within_512(article, start_offset, end_offset, tokenizer)\n",
    "# print(context)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_id</th>\n",
       "      <th>entity</th>\n",
       "      <th>start_offset</th>\n",
       "      <th>end_offset</th>\n",
       "      <th>broad_roles</th>\n",
       "      <th>fine_grained_roles</th>\n",
       "      <th>context</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EN_UA_103861.txt</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>791</td>\n",
       "      <td>797</td>\n",
       "      <td>Antagonist</td>\n",
       "      <td>[Spy]</td>\n",
       "      <td>the world needs peacemaker trump again by jeff...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EN_UA_103861.txt</td>\n",
       "      <td>China</td>\n",
       "      <td>1516</td>\n",
       "      <td>1520</td>\n",
       "      <td>Antagonist</td>\n",
       "      <td>[Instigator]</td>\n",
       "      <td>include terrorists, drug dealers, and intellig...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EN_UA_103861.txt</td>\n",
       "      <td>Hamas</td>\n",
       "      <td>2121</td>\n",
       "      <td>2125</td>\n",
       "      <td>Antagonist</td>\n",
       "      <td>[Terrorist]</td>\n",
       "      <td>is a communist nation, these individuals are n...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EN_UA_103861.txt</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>4909</td>\n",
       "      <td>4920</td>\n",
       "      <td>Protagonist</td>\n",
       "      <td>[Peacemaker, Guardian]</td>\n",
       "      <td>response is disturbing to u. s. senator marco ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EN_UA_021270.txt</td>\n",
       "      <td>Yermak</td>\n",
       "      <td>667</td>\n",
       "      <td>672</td>\n",
       "      <td>Antagonist</td>\n",
       "      <td>[Incompetent]</td>\n",
       "      <td>ukraine ' s fate will be decided in coming yea...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            file_id        entity  start_offset  end_offset  broad_roles  \\\n",
       "0  EN_UA_103861.txt       Chinese           791         797   Antagonist   \n",
       "1  EN_UA_103861.txt         China          1516        1520   Antagonist   \n",
       "2  EN_UA_103861.txt         Hamas          2121        2125   Antagonist   \n",
       "3  EN_UA_103861.txt  Donald Trump          4909        4920  Protagonist   \n",
       "4  EN_UA_021270.txt        Yermak           667         672   Antagonist   \n",
       "\n",
       "       fine_grained_roles                                            context  \\\n",
       "0                   [Spy]  the world needs peacemaker trump again by jeff...   \n",
       "1            [Instigator]  include terrorists, drug dealers, and intellig...   \n",
       "2             [Terrorist]  is a communist nation, these individuals are n...   \n",
       "3  [Peacemaker, Guardian]  response is disturbing to u. s. senator marco ...   \n",
       "4           [Incompetent]  ukraine ' s fate will be decided in coming yea...   \n",
       "\n",
       "                                              labels  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, ...  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Clean and split roles into proper lists\n",
    "def clean_roles(roles):\n",
    "    # Remove unwanted characters like '[' and ']' and extra quotes\n",
    "    if isinstance(roles, str):\n",
    "        roles = re.sub(r\"[\\[\\]']\", '', roles)  # Remove brackets and single quotes\n",
    "        roles = [role.strip() for role in roles.split(',')]  # Split and strip whitespace\n",
    "    return roles\n",
    "\n",
    "# Apply cleaning to the column\n",
    "data['fine_grained_roles'] = data['fine_grained_roles'].apply(clean_roles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique roles\n",
    "all_roles = set(role for roles in data['fine_grained_roles'] for role in roles)\n",
    "role_to_idx = {role: idx for idx, role in enumerate(sorted(all_roles))}\n",
    "# Convert fine-grained roles to multi-hot encoding\n",
    "def encode_roles(roles):\n",
    "    labels = [0] * len(role_to_idx)\n",
    "    for role in roles:\n",
    "        labels[role_to_idx[role]] = 1\n",
    "    return labels\n",
    "\n",
    "data['labels'] = data['fine_grained_roles'].apply(encode_roles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Spy'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(all_roles)[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_id</th>\n",
       "      <th>entity</th>\n",
       "      <th>start_offset</th>\n",
       "      <th>end_offset</th>\n",
       "      <th>broad_roles</th>\n",
       "      <th>fine_grained_roles</th>\n",
       "      <th>context</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EN_UA_103861.txt</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>791</td>\n",
       "      <td>797</td>\n",
       "      <td>Antagonist</td>\n",
       "      <td>[Spy]</td>\n",
       "      <td>the world needs peacemaker trump again by jeff...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EN_UA_103861.txt</td>\n",
       "      <td>China</td>\n",
       "      <td>1516</td>\n",
       "      <td>1520</td>\n",
       "      <td>Antagonist</td>\n",
       "      <td>[Instigator]</td>\n",
       "      <td>include terrorists, drug dealers, and intellig...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EN_UA_103861.txt</td>\n",
       "      <td>Hamas</td>\n",
       "      <td>2121</td>\n",
       "      <td>2125</td>\n",
       "      <td>Antagonist</td>\n",
       "      <td>[Terrorist]</td>\n",
       "      <td>is a communist nation, these individuals are n...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EN_UA_103861.txt</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>4909</td>\n",
       "      <td>4920</td>\n",
       "      <td>Protagonist</td>\n",
       "      <td>[Peacemaker, Guardian]</td>\n",
       "      <td>response is disturbing to u. s. senator marco ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EN_UA_021270.txt</td>\n",
       "      <td>Yermak</td>\n",
       "      <td>667</td>\n",
       "      <td>672</td>\n",
       "      <td>Antagonist</td>\n",
       "      <td>[Incompetent]</td>\n",
       "      <td>ukraine ' s fate will be decided in coming yea...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            file_id        entity  start_offset  end_offset  broad_roles  \\\n",
       "0  EN_UA_103861.txt       Chinese           791         797   Antagonist   \n",
       "1  EN_UA_103861.txt         China          1516        1520   Antagonist   \n",
       "2  EN_UA_103861.txt         Hamas          2121        2125   Antagonist   \n",
       "3  EN_UA_103861.txt  Donald Trump          4909        4920  Protagonist   \n",
       "4  EN_UA_021270.txt        Yermak           667         672   Antagonist   \n",
       "\n",
       "       fine_grained_roles                                            context  \\\n",
       "0                   [Spy]  the world needs peacemaker trump again by jeff...   \n",
       "1            [Instigator]  include terrorists, drug dealers, and intellig...   \n",
       "2             [Terrorist]  is a communist nation, these individuals are n...   \n",
       "3  [Peacemaker, Guardian]  response is disturbing to u. s. senator marco ...   \n",
       "4           [Incompetent]  ukraine ' s fate will be decided in coming yea...   \n",
       "\n",
       "                                              labels  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, ...  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['labels'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions = data['entity'].tolist()\n",
    "contexts = data['context'].tolist()\n",
    "labels = data['labels'].tolist()\n",
    "# start_offsets = data['start_offset'].tolist()\n",
    "# end_offsets = data['end_offset'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import f1_score\n",
    "class EntityDataset(Dataset):\n",
    "    def __init__(self, mentions, contexts, labels, tokenizer, max_len):\n",
    "        self.mentions = mentions\n",
    "        self.contexts = contexts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mentions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mention = self.mentions[idx]\n",
    "        context = self.contexts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "            context,\n",
    "            truncation=True, max_length=self.max_len, \n",
    "            padding=\"max_length\", return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(0),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(label, dtype=torch.float)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/mohamedelzeftawy/Documents/MBZUAI/NLP/Assignment2/.venv/lib/python3.9/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss=0.5834505185484886, F1=0.025\n",
      "Epoch 1: Loss=0.40211527918775875, F1=0.0\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "role_to_idx = {role: idx for idx, role in enumerate(sorted(all_roles))}\n",
    "idx_to_roles = {idx: role for role, idx in role_to_idx.items()}\n",
    "\n",
    "def train_model(model, dataloader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        loss = torch.nn.BCEWithLogitsLoss()(outputs.logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "def load_model(idx_to_label, role_to_idx, model_name = \"bert-base-uncased\", dropout_rate=0.3):\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=len(role_to_idx),\n",
    "            id2label=idx_to_label,\n",
    "            label2id=role_to_idx,\n",
    "            problem_type=\"multi_label_classification\"\n",
    "        )\n",
    "        # Adjust dropout if supported\n",
    "        model.config.hidden_dropout_prob = dropout_rate\n",
    "        model.config.attention_probs_dropout_prob = dropout_rate\n",
    "        \n",
    "        # Freeze the lower layers of the model to prevent overfitting\n",
    "        for param in model.bert.encoder.layer[:8].parameters():\n",
    "            param.requires_grad = False\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model.to(device)\n",
    "        return model\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].cpu().numpy()\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            preds = torch.sigmoid(outputs.logits).cpu().numpy()\n",
    "            \n",
    "            # Accumulate labels and predictions\n",
    "            all_labels.append(labels)\n",
    "            all_preds.append(preds)\n",
    "    \n",
    "    # Convert list of arrays to 2D arrays\n",
    "    all_labels = np.vstack(all_labels)\n",
    "    all_preds = np.vstack(all_preds)\n",
    "    # Convert predictions to binary (multi-label classification)\n",
    "    binary_preds = all_preds > 0.5\n",
    "    \n",
    "    # Compute F1 score\n",
    "    return f1_score(all_labels, binary_preds, average='micro')\n",
    "\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "mentions_train, mentions_val, contexts_train, contexts_val, labels_train, labels_val = train_test_split(\n",
    "    mentions, contexts, labels, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# Create the training and validation datasets\n",
    "train_dataset = EntityDataset(mentions_train, contexts_train, labels_train, tokenizer, max_len=512)\n",
    "val_dataset = EntityDataset(mentions_val, contexts_val, labels_val, tokenizer, max_len=512)\n",
    "\n",
    "# Create the DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = load_model(idx_to_roles, role_to_idx, model_name = \"bert-base-uncased\", dropout_rate=0.3)\n",
    "# Optimizer and Scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.1)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(2):\n",
    "    train_loss = train_model(model, train_dataloader, optimizer, scheduler, device)\n",
    "    val_f1 = evaluate_model(model, val_dataloader, device)\n",
    "    print(f\"Epoch {epoch}: Loss={train_loss}, F1={val_f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./saved_model/tokenizer_config.json',\n",
       " './saved_model/special_tokens_map.json',\n",
       " './saved_model/vocab.txt',\n",
       " './saved_model/added_tokens.json',\n",
       " './saved_model/tokenizer.json')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the trained model and tokenizer\n",
    "output_dir = \"./saved_model\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "model.save_pretrained(output_dir, safe_serialization=False)\n",
    "# Save tokenizer\n",
    "tokenizer.save_pretrained(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the [ entity ] donald trump [ / entity ] saved the town from destruction.\n",
      "Predicted roles: [['Conspirator', 'Deceiver', 'Guardian', 'Instigator', 'Martyr', 'Peacemaker', 'Saboteur', 'Spy', 'Traitor', 'Tyrant']]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def predict(model, tokenizer, text, device, threshold=0.27):\n",
    "    \"\"\"\n",
    "    Predict the roles for a given input text using a trained model.\n",
    "    \n",
    "    Args:\n",
    "        model: The trained PyTorch model.\n",
    "        tokenizer: The tokenizer used during training.\n",
    "        text: The input text (string).\n",
    "        device: The device (CPU/GPU) for computation.\n",
    "        threshold: The probability threshold for binary classification.\n",
    "        \n",
    "    Returns:\n",
    "        roles: A list of predicted roles for the input text.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Move input data to the device\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "    \n",
    "    # Make predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()  # Convert logits to probabilities\n",
    "    # print(probs)\n",
    "    # Apply threshold to convert probabilities to binary predictions\n",
    "    binary_preds = (probs > threshold).astype(int)\n",
    "    # Map indices to role names\n",
    "    predicted_roles = []\n",
    "    for indices in binary_preds:\n",
    "        predicted_roles.append([idx_to_roles[idx] for idx, value in enumerate(indices) if value == 1])\n",
    "    \n",
    "    return predicted_roles\n",
    "\n",
    "# Example usage\n",
    "sample_text = \"The Donald Trump saved the town from destruction.\"\n",
    "context = extract_context_within_512(sample_text, 4, 16, tokenizer)\n",
    "print(context)\n",
    "predicted_roles = predict(model, tokenizer, context, device)\n",
    "print(f\"Predicted roles: {predicted_roles}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_articles = {}\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "for file_id in test_df['file_id'].unique():\n",
    "    with open(f\"dev-documents_25_October/EN/subtask-1-documents/{file_id}\", \"r\") as f:\n",
    "        test_articles[file_id] = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (578 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# Add a context column to the DataFrame\n",
    "def compute_test_context(row):\n",
    "    file_id = row['file_id']\n",
    "    test_article = test_articles[file_id]\n",
    "    start_offset = row['start_offset']\n",
    "    end_offset = row['end_offset'] + 1\n",
    "    return extract_context_within_512(\n",
    "        article=test_article,\n",
    "        start=start_offset,\n",
    "        end=end_offset,\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=512\n",
    "    )\n",
    "test_df['context'] = test_df.apply(compute_test_context, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'news in june. “ they ’ re sitting on 10 to $ 12 trillion of critical minerals in ukraine. they could be the richest country in all of europe, \" graham said. \" if we help ukraine now, they can become the best business partner we ever dreamed of, that $ 10 to $ 12 trillion of critical mineral assets could be used by ukraine and the west, ” he added. graham is treating ukraine as a future colony of the us with his comments on the ownership of the country \\' s natural resources, said vladimir dzhabarov, official of the russian federation council. while \" promising ukrainians mountains of gold, graham in fact considers it as a future american colony, \" dzhabarov said. * lindsey graham is included on the list of terrorists and extremists in russia. take a look at sputnik \\' s infographics to see ukraine \\' s mineral resources that the westcotes so much. west is fighting for, ukraine is rich in mineral resources ukraine \\' s minerals : what the west is fighting for ukraine is rich in mineral resources, with diverse deposits of fossil fuels, metals and industrial minerals. the country is a major producer of iron ore, coal, natural gas and oil, among others. [ entity ] washington [ / entity ] “ cannot afford ” to allow russia to achieve victory in the ukraine conflict as this would mean losing direct access to vast mineral assets. that was the view of us senator lindsey graham * in an interview with ‘ face the nation ’ on cbs news in june. “ they ’ re sitting on 10 to $ 12 trillion of critical minerals in ukraine. they could be the richest country in all of europe, \" graham said. \" if we help ukraine now, they can become the best business partner we ever dreamed of, that $ 10 to $ 12 trillion of critical mineral assets could be used by ukraine and the west, ” he added. graham is treating ukraine as a future colony of the us with his comments on the ownership of the country \\' s natural resources, said vladimir dzhabarov, official of the russian federation council. while \" promising ukrainians mountains of gold, graham in fact considers it as a future american colony, \" dzhabarov said. \" the fate of the colonies is always the same : the americans will scoop up all the natural resources of ukraine in exchange for \\' the great honor of being a friend of the united states \\', \" he warned. \" well, they will feed the corrupt top of the neo - nazi regime'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['context'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "protagonist_roles = [\n",
    "    \"Guardian\",\n",
    "    \"Martyr\",\n",
    "    \"Peacemaker\",\n",
    "    \"Rebel\",\n",
    "    \"Underdog\",\n",
    "    \"Virtuous\"\n",
    "]\n",
    "antagonist_roles = [\n",
    "    \"Instigator\",\n",
    "    \"Conspirator\",\n",
    "    \"Tyrant\",\n",
    "    \"Foreign Adversary\",\n",
    "    \"Traitor\",\n",
    "    \"Spy\",\n",
    "    \"Saboteur\",\n",
    "    \"Corrupt\",\n",
    "    \"Incompetent\",\n",
    "    \"Terrorist\",\n",
    "    \"Deceiver\",\n",
    "    \"Bigot\"\n",
    "]\n",
    "innocent_roles = [\n",
    "    \"Forgotten\",\n",
    "    \"Exploited\",\n",
    "    \"Victim\",\n",
    "    \"Scapegoat\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_broad_role(predicted_roles):\n",
    "        \"\"\"\n",
    "        Determine the broad role based on the majority of fine-grained roles.\n",
    "        \"\"\"\n",
    "        counts = {\n",
    "            \"Protagonist\": sum(1 for role in predicted_roles if role in protagonist_roles),\n",
    "            \"Antagonist\": sum(1 for role in predicted_roles if role in antagonist_roles),\n",
    "            \"Innocent\": sum(1 for role in predicted_roles if role in innocent_roles),\n",
    "        }\n",
    "        # Return the broad role with the highest count\n",
    "        return max(counts, key=counts.get) if max(counts.values()) > 0 else \"Unknown\"\n",
    "def predict_for_test_set_tab_format(model, tokenizer, test_data, device, output_file, threshold=0.31):\n",
    "    \"\"\"\n",
    "    Predict labels for a test set and save the results in a tab-indented text format.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained PyTorch model.\n",
    "        tokenizer: Tokenizer used during training.\n",
    "        test_data: List of dictionaries with keys `file_id`, `text`, `start`, `end`.\n",
    "                   Example: [{\"file_id\": \"EN_UA_DEV_100012.txt\", \"text\": \"...\", \"start\": 1441, \"end\": 1450}]\n",
    "        device: The device (CPU/GPU) for computation.\n",
    "        output_file: Path to save the prediction results.\n",
    "        threshold: Probability threshold for multi-label predictions.\n",
    "    \"\"\"\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for data in test_data:\n",
    "            file_id = data[\"file_id\"]\n",
    "            text = data[\"context\"]\n",
    "            start = data[\"start_offset\"]\n",
    "            end = data[\"end_offset\"]\n",
    "            \n",
    "            # Extract the entity from the text\n",
    "            entity = data[\"entity\"]\n",
    "            \n",
    "            # Predict roles for the text\n",
    "            predicted_roles = predict(model, tokenizer, text, device, threshold)\n",
    "            broad_role = get_broad_role(predicted_roles[0])\n",
    "\n",
    "            # Convert roles to a comma-separated string\n",
    "            roles_str = \"\\t\".join(predicted_roles[0])\n",
    "            \n",
    "            # Write the result in the desired format\n",
    "            f.write(f\"{file_id}\\t{entity}\\t{start}\\t{end}\\t{broad_role}\\t{roles_str}\\n\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_for_test_set_tab_format(model, tokenizer, test_df.to_dict(orient='records'), device, \"predictions.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
