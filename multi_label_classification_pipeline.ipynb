{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline to Fine-Tune CAMELBERT and Evaluate It Using Three Datasets\n",
    "\n",
    "This document outlines the pipeline to fine-tune the MARBERT model and evaluate its performance on three distinct datasets. After fine-tuning, we will compare the results across the datasets to assess performance consistency and accuracy across varying input types and data structures.\n",
    "\n",
    "---\n",
    "\n",
    "#### Overview of Datasets:\n",
    "1. **Dataset 1:** GPT-4o-generated samples\n",
    "2. **Dataset 2:** Samples classified using 18 binary classifiers on random samples\n",
    "3. **Dataset 3:** Samples classified using 18 binary classifiers on equivalent samples\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali.mekky/.conda/envs/DAI/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    EvalPrediction,\n",
    "    TrainerCallback,\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, hamming_loss, precision_recall_fscore_support\n",
    "from preprocess import final_eliminations\n",
    "from transformers import TrainingArguments, EarlyStoppingCallback\n",
    "from transformers.trainer_utils import IntervalStrategy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertTrainer:\n",
    "    def __init__(self, training_dataset_path, labels, exp_num, threshold=0.5, model_name=\"CAMeL-Lab/bert-base-arabic-camelbert-ca\"):\n",
    "        self.labels = labels\n",
    "        self.label2id = {label: idx for idx, label in enumerate(labels)}\n",
    "        self.id2label = {idx: label for label, idx in self.label2id.items()}\n",
    "        self.model_name = model_name\n",
    "        self.exp_num = exp_num\n",
    "        training_dataset = pd.read_csv(training_dataset_path)\n",
    "        self.training_dataset_processed = pd.DataFrame({\n",
    "            'text': training_dataset['tweet'],\n",
    "            'label': training_dataset[self.labels].values.tolist()\n",
    "        })\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.train_df, self.val_df = train_test_split(self.training_dataset_processed, test_size=0.1, random_state=42)\n",
    "        self.train_df['text'] = self.train_df['text'].astype(str)\n",
    "        self.val_df['text'] = self.val_df['text'].astype(str)\n",
    "        self.train_dataset = self.create_dataset(self.train_df)\n",
    "        self.val_dataset = self.create_dataset(self.val_df)\n",
    "        self.threshold = threshold\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = self.load_model(dropout_rate=0.3)  # Adding dropout rate\n",
    "\n",
    "    def create_dataset(self, df):\n",
    "        encodings = self.tokenizer(\n",
    "            df['text'].tolist(), truncation=True, padding=True, max_length=128\n",
    "        )\n",
    "        return TweetDataset(encodings, df['label'].values)\n",
    "\n",
    "    def load_model(self, dropout_rate=0.3):\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            self.model_name,\n",
    "            num_labels=len(self.labels),\n",
    "            id2label=self.id2label,\n",
    "            label2id=self.label2id,\n",
    "            problem_type=\"multi_label_classification\"\n",
    "        )\n",
    "        # Adjust dropout if supported\n",
    "        model.config.hidden_dropout_prob = dropout_rate\n",
    "        model.config.attention_probs_dropout_prob = dropout_rate\n",
    "        \n",
    "        # Freeze the lower layers of the model to prevent overfitting\n",
    "        for param in model.bert.encoder.layer[:8].parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        model.to(self.device)\n",
    "        return model\n",
    "\n",
    "    \n",
    "    def predict(self, texts):\n",
    "        encodings = self.tokenizer(\n",
    "            texts, \n",
    "            truncation=True, \n",
    "            padding=True, \n",
    "            max_length=128, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        input_ids = encodings['input_ids'].to(self.device)\n",
    "        attention_mask = encodings['attention_mask'].to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "        probabilities = torch.sigmoid(logits).cpu().numpy()\n",
    "        predictions = (probabilities >= self.threshold).astype(int)\n",
    "        variation_score = 1 - (np.sum(probabilities)/18)\n",
    "        return predictions, probabilities, variation_score\n",
    "\n",
    "    \n",
    "    def evaluate(self, dev_path):\n",
    "        if '.tsv' in dev_path:\n",
    "            dev = pd.read_csv(dev_path, sep='\\t')\n",
    "        else:\n",
    "            dev = pd.read_csv(dev_path)\n",
    "        \n",
    "        dev = final_eliminations(dev, column_name=\"sentence\")\n",
    "\n",
    "        df_replaced = dev.replace({'y': 1, 'n': 0})\n",
    "        country_columns = df_replaced.columns.difference(['sentence'])\n",
    "        df_replaced['label'] = df_replaced[country_columns].values.tolist()\n",
    "        df_final = df_replaced[['sentence', 'label']]\n",
    "        \n",
    "        predictions, probabilities, _ = self.predict(df_final['sentence'].tolist())\n",
    "        output_dir = f'./exp_{self.exp_num}'\n",
    "        output_file = os.path.join(self.save_dir, f\"{self.model_name.replace('/', '-')}-experiment-{self.exp_num}_predictions.txt\")\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        with open(output_file, 'w') as f:\n",
    "            for i, (pred, prob) in enumerate(zip(predictions, probabilities)):\n",
    "                pred_str = ','.join(map(str, pred))\n",
    "                f.write(f'{pred_str}\\n')\n",
    "        # with open(output_file, 'w') as f:\n",
    "        #     for pred in predictions:\n",
    "        #         pred_str = ','.join(map(str, pred))\n",
    "        #         f.write(f'{pred_str}\\n')\n",
    "        \n",
    "        indexes = [0, 2, 4, 10, 13, 14, 15, 17]\n",
    "        predictions = [output[indexes] for output in predictions]\n",
    "\n",
    "\n",
    "        subset_accuracy = accuracy_score(df_final['label'].tolist(), predictions)\n",
    "        print(f\"Subset Accuracy: {subset_accuracy:.4f}\")\n",
    "\n",
    "        hamming = hamming_loss(df_final['label'].tolist(), predictions)\n",
    "        print(f\"Hamming Loss: {hamming:.4f}\")\n",
    "\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            df_final['label'].tolist(), predictions, average='micro'  # Use 'micro' for multi-label tasks\n",
    "        )\n",
    "        print(f\"Micro Precision: {precision:.4f}\")\n",
    "        print(f\"Micro Recall: {recall:.4f}\")\n",
    "        print(f\"Micro F1-Score: {f1:.4f}\")\n",
    "\n",
    "        precision_per_label, recall_per_label, f1_per_label, _ = precision_recall_fscore_support(\n",
    "            df_final['label'].tolist(), predictions, average=None  # 'None' gives metrics for each label\n",
    "        )\n",
    "        print(f\"Precision per label: {precision_per_label}\")\n",
    "        print(f\"Recall per label: {recall_per_label}\")\n",
    "        print(f\"F1-Score per label: {f1_per_label}\")\n",
    "        multilabel_check = [np.sum(np.array(prediction)) for prediction in predictions]\n",
    "        print(set(multilabel_check))\n",
    "\n",
    "\n",
    "    def compute_metrics(self, p: EvalPrediction):\n",
    "        preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "        result = self.multi_label_metrics(preds, p.label_ids)\n",
    "        return result\n",
    "\n",
    "    def multi_label_metrics(self, predictions, labels):\n",
    "        sigmoid = torch.nn.Sigmoid()\n",
    "        probs = sigmoid(torch.Tensor(predictions))\n",
    "        y_pred = np.zeros(probs.shape)\n",
    "        y_pred[np.where(probs >= self.threshold)] = 1\n",
    "        f1 = f1_score(labels, y_pred, average='micro')\n",
    "        roc_auc = roc_auc_score(labels, y_pred, average='micro')\n",
    "        accuracy = accuracy_score(labels, y_pred)\n",
    "        return {'f1': f1, 'roc_auc': roc_auc, 'accuracy': accuracy}\n",
    "    \n",
    "    # def train(\n",
    "    #     self,\n",
    "    #     num_train_epochs=3,  \n",
    "    #     metric_for_best_model=\"eval_f1\",  \n",
    "    #     greater_is_better=True,  \n",
    "    #     per_device_train_batch_size=8,\n",
    "    #     per_device_eval_batch_size=16,\n",
    "    #     patience=2\n",
    "    # ):\n",
    "    #     training_args = TrainingArguments(\n",
    "    #         output_dir='./exp_' + str(self.exp_num) + '/results',\n",
    "    #         num_train_epochs=num_train_epochs,\n",
    "    #         per_device_train_batch_size=per_device_train_batch_size,\n",
    "    #         per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "    #         warmup_steps=500,\n",
    "    #         weight_decay=0.01,\n",
    "    #         logging_dir='./exp_' + str(self.exp_num) + '/logs',\n",
    "    #         logging_steps=500,\n",
    "    #         evaluation_strategy=\"epoch\",\n",
    "    #         save_strategy=\"epoch\",\n",
    "    #         load_best_model_at_end=True,\n",
    "    #         metric_for_best_model=metric_for_best_model,\n",
    "    #         greater_is_better=greater_is_better,\n",
    "    #         fp16=True,\n",
    "    #         report_to=[\"tensorboard\"],\n",
    "    #         lr_scheduler_type=\"reduce_lr_on_plateau\",  # Adjust learning rate dynamically\n",
    "    #     )\n",
    "\n",
    "    #     early_stopping_callback = EarlyStoppingCallback(\n",
    "    #         early_stopping_patience=patience\n",
    "    #     )\n",
    "\n",
    "    #     trainer = CustomTrainer(\n",
    "    #         model=self.model,\n",
    "    #         args=training_args,\n",
    "    #         train_dataset=self.train_dataset,\n",
    "    #         eval_dataset=self.val_dataset,\n",
    "    #         tokenizer=self.tokenizer,\n",
    "    #         compute_metrics=self.compute_metrics,\n",
    "    #         callbacks=[early_stopping_callback]  # Register the early stopping callback\n",
    "    #     )\n",
    "        \n",
    "    #     trainer.train()\n",
    "    #     best_metric_value = trainer.state.best_metric \n",
    "    #     num_epochs = training_args.num_train_epochs\n",
    "    #     greater_is_better = training_args.greater_is_better\n",
    "    #     metric_name = training_args.metric_for_best_model\n",
    "    #     save_dir = f'./exp_{self.exp_num}/marbert_finetuned_epochs_{num_epochs}_{metric_name}_{best_metric_value:.4f}_{\"greater\" if greater_is_better else \"less\"}_threshold_{self.threshold}'\n",
    "    #     self.save_dir = save_dir\n",
    "    #     os.makedirs(save_dir, exist_ok=True)\n",
    "    #     self.model.save_pretrained(save_dir, safe_serialization=False)\n",
    "    #     self.tokenizer.save_pretrained(save_dir)\n",
    "    def train(\n",
    "        self,\n",
    "        num_train_epochs=3,  \n",
    "        metric_for_best_model=\"eval_f1\",  \n",
    "        greater_is_better=True,  \n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=16,\n",
    "        patience=2,\n",
    "        warmup_steps=500,  # Number of steps for learning rate warmup\n",
    "        base_learning_rate=5e-5,  # Initial learning rate\n",
    "    ):\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir='./exp_' + str(self.exp_num) + '/results',\n",
    "            num_train_epochs=num_train_epochs,\n",
    "            per_device_train_batch_size=per_device_train_batch_size,\n",
    "            per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "            warmup_steps=warmup_steps,\n",
    "            learning_rate=base_learning_rate,\n",
    "            weight_decay=0.01,\n",
    "            logging_dir='./exp_' + str(self.exp_num) + '/logs',\n",
    "            logging_steps=500,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=metric_for_best_model,\n",
    "            greater_is_better=greater_is_better,\n",
    "            fp16=True,\n",
    "            report_to=[\"tensorboard\"],\n",
    "            lr_scheduler_type=\"linear\",  # Use linear warmup and decay\n",
    "        )\n",
    "\n",
    "        early_stopping_callback = EarlyStoppingCallback(\n",
    "            early_stopping_patience=patience\n",
    "        )\n",
    "\n",
    "        trainer = CustomTrainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=self.train_dataset,\n",
    "            eval_dataset=self.val_dataset,\n",
    "            tokenizer=self.tokenizer,\n",
    "            compute_metrics=self.compute_metrics,\n",
    "            callbacks=[early_stopping_callback]  # Register the early stopping callback\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "\n",
    "        # Save the best model\n",
    "        best_metric_value = trainer.state.best_metric \n",
    "        num_epochs = training_args.num_train_epochs\n",
    "        greater_is_better = training_args.greater_is_better\n",
    "        metric_name = training_args.metric_for_best_model\n",
    "        save_dir = f'./exp_{self.exp_num}/marbert_finetuned_epochs_{num_epochs}_{metric_name}_{best_metric_value:.4f}_{\"greater\" if greater_is_better else \"less\"}_threshold_{self.threshold}'\n",
    "        self.save_dir = save_dir\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        self.model.save_pretrained(save_dir, safe_serialization=False)\n",
    "        self.tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "class TweetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def save_model(self, output_dir=None, **kwargs):\n",
    "        if output_dir is None:\n",
    "            output_dir = self.args.output_dir\n",
    "        for param in self.model.parameters():\n",
    "            param.data = param.data.contiguous()\n",
    "        super().save_model(output_dir, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '/home/ali.mekky/Documents/NLP/Project/NADI2024/subtask1/multilabel/NADIcombined_cleaned_MULTI_LABEL_MODIFIED_FINAL.csv'\n",
    "df = pd.read_csv(dataset_path)\n",
    "# df_200 = df.head(1000)\n",
    "# output_path = '/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/our_data/First_1000.csv'\n",
    "# df_200.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced dataset created and saved as 'balanced_multilabel_dataset_500.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Load the dataset\n",
    "label_columns = df.columns[2:-1]  # Excludes 'id', 'tweet', and 'Computed' columns\n",
    "df[label_columns] = df[label_columns].astype(int)  # Ensure labels are integers\n",
    "\n",
    "threshold = 500  \n",
    "balanced_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over possible combinations of active labels    model_name=\"/home/lara.hassan/Documents/Cross-Country-Dialectal-Arabic-Identification/exp_8/camelbert_finetuned_epochs_2_eval_f1_0.7694_greater_threshold_0.3\",\n",
    "\n",
    "for num_classes in range(1, len(label_columns) + 1):\n",
    "    subset = df[df[label_columns].sum(axis=1) == num_classes]  # Filter rows with num_classes active labels\n",
    "    \n",
    "    # Shuffle and sample the subset if it exceeds the threshold\n",
    "    if len(subset) > threshold:\n",
    "        subset = shuffle(subset).head(threshold)\n",
    "    \n",
    "    # Append sampled subset to the balanced dataset\n",
    "    balanced_df = pd.concat([balanced_df, subset], ignore_index=True)\n",
    "\n",
    "# Shuffle the final balanced DataFrame and save it to a new CSV\n",
    "balanced_df = shuffle(balanced_df).reset_index(drop=True)\n",
    "balanced_df.to_csv('balanced_multilabel_dataset_' + str(threshold) + '.csv', index=False)\n",
    "\n",
    "print(\"Balanced dataset created and saved as 'balanced_multilabel_dataset_500.csv'\")\n",
    "\n",
    "label_columns = ['Algeria', 'Bahrain', 'Egypt', 'Iraq', 'Jordan', 'Kuwait', 'Lebanon', \n",
    "                 'Libya', 'Morocco', 'Oman', 'Palestine', 'Qatar', 'Saudi_Arabia', \n",
    "                 'Sudan', 'Syria', 'Tunisia', 'UAE', 'Yemen']\n",
    "\n",
    "# Initialize a dictionary to store counts\n",
    "counts = {}\n",
    "\n",
    "# Loop to count rows where the sum of 1s in label columns equals i (from 0 to 18)\n",
    "for i in range(19):\n",
    "    counts[i] = (balanced_df[label_columns].sum(axis=1) == i).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0,\n",
       " 1: 500,\n",
       " 2: 500,\n",
       " 3: 500,\n",
       " 4: 500,\n",
       " 5: 500,\n",
       " 6: 500,\n",
       " 7: 500,\n",
       " 8: 500,\n",
       " 9: 500,\n",
       " 10: 168,\n",
       " 11: 137,\n",
       " 12: 334,\n",
       " 13: 273,\n",
       " 14: 256,\n",
       " 15: 500,\n",
       " 16: 11,\n",
       " 17: 26,\n",
       " 18: 500}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPERIMENT 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at CAMeL-Lab/bert-base-arabic-camelbert-ca and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/lara.hassan/.conda/envs/myenv/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "  5%|▍         | 502/10060 [00:27<08:58, 17.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5952, 'grad_norm': 1.13016939163208, 'learning_rate': 4.97e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 1004/10060 [00:56<07:53, 19.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5852, 'grad_norm': 4.487825870513916, 'learning_rate': 4.7400627615062765e-05, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1006/10060 [00:56<08:19, 18.13it/s]\n",
      " 10%|█         | 1006/10060 [00:56<08:19, 18.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5702190399169922, 'eval_f1': 0.721625086283404, 'eval_roc_auc': 0.6566456002973553, 'eval_accuracy': 0.08035714285714286, 'eval_runtime': 0.3316, 'eval_samples_per_second': 1350.871, 'eval_steps_per_second': 57.291, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 1502/10060 [01:29<08:15, 17.28it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5474, 'grad_norm': 1.6961125135421753, 'learning_rate': 4.478556485355649e-05, 'epoch': 1.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 2002/10060 [01:59<08:12, 16.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5359, 'grad_norm': 3.8677239418029785, 'learning_rate': 4.217050209205021e-05, 'epoch': 1.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2012/10060 [01:59<08:10, 16.40it/s]\n",
      " 20%|██        | 2012/10060 [02:00<08:10, 16.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.551264226436615, 'eval_f1': 0.737281067556297, 'eval_roc_auc': 0.6925700472012972, 'eval_accuracy': 0.08035714285714286, 'eval_runtime': 0.3326, 'eval_samples_per_second': 1346.946, 'eval_steps_per_second': 57.125, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 2502/10060 [02:32<07:33, 16.67it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4861, 'grad_norm': 3.64426326751709, 'learning_rate': 3.955543933054394e-05, 'epoch': 2.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 3003/10060 [03:01<06:41, 17.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4749, 'grad_norm': 2.2494912147521973, 'learning_rate': 3.694560669456067e-05, 'epoch': 2.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 3017/10060 [03:02<06:55, 16.94it/s]\n",
      " 30%|███       | 3018/10060 [03:03<06:55, 16.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5411345362663269, 'eval_f1': 0.7527460210715087, 'eval_roc_auc': 0.7294907566665392, 'eval_accuracy': 0.08928571428571429, 'eval_runtime': 0.3123, 'eval_samples_per_second': 1434.531, 'eval_steps_per_second': 60.84, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 3503/10060 [03:33<06:20, 17.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4221, 'grad_norm': 3.1075947284698486, 'learning_rate': 3.433054393305439e-05, 'epoch': 3.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 4003/10060 [04:03<06:08, 16.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4147, 'grad_norm': 5.762308120727539, 'learning_rate': 3.171548117154812e-05, 'epoch': 3.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 4023/10060 [04:04<06:03, 16.60it/s]\n",
      " 40%|████      | 4024/10060 [04:04<06:03, 16.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5831584930419922, 'eval_f1': 0.7425149700598802, 'eval_roc_auc': 0.7443502191112906, 'eval_accuracy': 0.11607142857142858, 'eval_runtime': 0.3143, 'eval_samples_per_second': 1425.53, 'eval_steps_per_second': 60.458, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 4503/10060 [04:36<05:33, 16.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3712, 'grad_norm': 1.8115085363388062, 'learning_rate': 2.9100418410041842e-05, 'epoch': 4.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 5003/10060 [05:07<05:28, 15.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.369, 'grad_norm': 3.180023193359375, 'learning_rate': 2.6485355648535566e-05, 'epoch': 4.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 5029/10060 [05:09<05:41, 14.71it/s]\n",
      " 50%|█████     | 5030/10060 [05:09<05:41, 14.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5870450735092163, 'eval_f1': 0.7595059336401065, 'eval_roc_auc': 0.7547659683134773, 'eval_accuracy': 0.12946428571428573, 'eval_runtime': 0.3544, 'eval_samples_per_second': 1264.012, 'eval_steps_per_second': 53.608, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 5503/10060 [05:41<04:50, 15.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3241, 'grad_norm': 2.609508991241455, 'learning_rate': 2.387029288702929e-05, 'epoch': 5.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 6003/10060 [06:11<04:06, 16.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3189, 'grad_norm': 4.649298191070557, 'learning_rate': 2.1255230125523013e-05, 'epoch': 5.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 6035/10060 [06:13<04:18, 15.56it/s]\n",
      " 60%|██████    | 6036/10060 [06:14<04:18, 15.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.607021152973175, 'eval_f1': 0.7645331767469172, 'eval_roc_auc': 0.7531913915381108, 'eval_accuracy': 0.109375, 'eval_runtime': 0.3174, 'eval_samples_per_second': 1411.641, 'eval_steps_per_second': 59.869, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 6503/10060 [06:45<03:27, 17.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2827, 'grad_norm': 5.2005462646484375, 'learning_rate': 1.8640167364016737e-05, 'epoch': 6.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 7003/10060 [07:16<03:03, 16.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.269, 'grad_norm': 3.7985293865203857, 'learning_rate': 1.602510460251046e-05, 'epoch': 6.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 7041/10060 [07:18<03:09, 15.94it/s]\n",
      " 70%|███████   | 7042/10060 [07:18<03:09, 15.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6416592597961426, 'eval_f1': 0.7568916349809885, 'eval_roc_auc': 0.7478023782895085, 'eval_accuracy': 0.09151785714285714, 'eval_runtime': 0.32, 'eval_samples_per_second': 1399.965, 'eval_steps_per_second': 59.374, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 7503/10060 [07:50<02:30, 16.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2388, 'grad_norm': 2.66792893409729, 'learning_rate': 1.3410041841004184e-05, 'epoch': 7.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 8003/10060 [08:22<02:12, 15.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2438, 'grad_norm': 1.4793015718460083, 'learning_rate': 1.079497907949791e-05, 'epoch': 7.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 8047/10060 [08:24<02:14, 14.91it/s]\n",
      " 80%|████████  | 8048/10060 [08:25<02:14, 14.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6881314516067505, 'eval_f1': 0.756483082242529, 'eval_roc_auc': 0.7560156587990452, 'eval_accuracy': 0.109375, 'eval_runtime': 0.3317, 'eval_samples_per_second': 1350.675, 'eval_steps_per_second': 57.283, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 8502/10060 [08:55<01:48, 14.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2159, 'grad_norm': 2.017803192138672, 'learning_rate': 8.185146443514645e-06, 'epoch': 8.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 9002/10060 [09:26<01:02, 16.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2117, 'grad_norm': 2.7105093002319336, 'learning_rate': 5.570083682008369e-06, 'epoch': 8.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9054/10060 [09:29<00:59, 16.99it/s]\n",
      " 90%|█████████ | 9054/10060 [09:29<00:59, 16.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7002390623092651, 'eval_f1': 0.7545499262174127, 'eval_roc_auc': 0.7531412598940754, 'eval_accuracy': 0.10714285714285714, 'eval_runtime': 0.303, 'eval_samples_per_second': 1478.576, 'eval_steps_per_second': 62.707, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 9502/10060 [09:59<00:34, 16.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1931, 'grad_norm': 1.5471935272216797, 'learning_rate': 2.955020920502092e-06, 'epoch': 9.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 10002/10060 [10:29<00:03, 17.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1922, 'grad_norm': 2.906043529510498, 'learning_rate': 3.3995815899581595e-07, 'epoch': 9.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10060/10060 [10:33<00:00, 16.57it/s]\n",
      "100%|██████████| 10060/10060 [10:35<00:00, 16.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7048290967941284, 'eval_f1': 0.7522365805168986, 'eval_roc_auc': 0.7531335047226225, 'eval_accuracy': 0.09821428571428571, 'eval_runtime': 0.3158, 'eval_samples_per_second': 1418.587, 'eval_steps_per_second': 60.163, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10060/10060 [10:38<00:00, 15.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 638.6339, 'train_samples_per_second': 62.994, 'train_steps_per_second': 15.752, 'train_loss': 0.36358296686327957, 'epoch': 10.0}\n"
     ]
    }
   ],
   "source": [
    "file_name = [\"First_200.csv\", \"NADIcombined_cleaned_MULTI_LABEL_MODIFIED_FINAL.csv\", \"First_1000.csv\", \"balanced_multilabel_dataset.csv\"]\n",
    "dataset_path = f\"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/our_data/{file_name[3]}\"\n",
    "dev_path = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/dev/NADI2024_subtask1_dev2.tsv\"\n",
    "labels = ['Algeria', 'Bahrain', 'Egypt', 'Iraq', 'Jordan', 'Kuwait',\n",
    "       'Lebanon', 'Libya', 'Morocco', 'Oman', 'Palestine', 'Qatar',\n",
    "       'Saudi_Arabia', 'Sudan', 'Syria', 'Tunisia', 'UAE', 'Yemen']\n",
    "trainer = BertTrainer(\n",
    "    training_dataset_path=dataset_path,\n",
    "    labels=labels,\n",
    "    threshold=0.3,\n",
    "    exp_num=6\n",
    ")\n",
    "trainer.train(\n",
    "    num_train_epochs=10,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=24\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset Accuracy: 0.0833\n",
      "Hamming Loss: 0.3354\n",
      "Micro Precision: 0.5590\n",
      "Micro Recall: 0.4522\n",
      "Micro F1-Score: 0.5000\n",
      "Precision per label: [0.42857143 0.45       0.53731343 0.62711864 0.71428571 0.50847458\n",
      " 0.25       0.63934426]\n",
      "Recall per label: [0.08571429 0.23076923 0.72       0.578125   0.11904762 0.65217391\n",
      " 0.0952381  0.66101695]\n",
      "F1-Score per label: [0.14285714 0.30508475 0.61538462 0.60162602 0.20408163 0.57142857\n",
      " 0.13793103 0.65      ]\n",
      "{np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(8)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4025847/3782111259.py:79: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    }
   ],
   "source": [
    "trainer.evaluate(dev_path=dev_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OVERALL SCORES:\n",
      "MACRO AVERAGE PRECISION SCORE: 51.94 %\n",
      "MACRO AVERAGE RECALL SCORE: 39.28 %\n",
      "MACRO AVERAGE F1-SCORE: 40.35 %\n",
      "MACRO AVERAGE ACCURACY: 66.46 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "scorer_script = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/NADI2024-ST1-Scorer.py\"\n",
    "gold_file = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/sample_submission/NADI2024_subtask1_dev2_gold.txt\"\n",
    "predictions_file = \"/home/lara.hassan/Documents/Cross-Country-Dialectal-Arabic-Identification/exp_6/camelbert_finetuned_epochs_10_eval_f1_0.7645_greater_threshold_0.3/CAMeL-Lab-bert-base-arabic-camelbert-ca-experiment-6_predictions.txt\"\n",
    "!python3 \"{scorer_script}\" \"{gold_file}\" \"{predictions_file}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPERIMENT 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at CAMeL-Lab/bert-base-arabic-camelbert-mix and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/lara.hassan/.conda/envs/myenv/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "100%|██████████| 252/252 [00:19<00:00, 13.91it/s]\n",
      "100%|██████████| 252/252 [00:21<00:00, 13.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.48796308040618896, 'eval_f1': 0.7621827981136312, 'eval_roc_auc': 0.7501663991770002, 'eval_accuracy': 0.12667660208643816, 'eval_runtime': 0.4998, 'eval_samples_per_second': 1342.658, 'eval_steps_per_second': 56.027, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 252/252 [00:24<00:00, 10.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 24.2469, 'train_samples_per_second': 248.857, 'train_steps_per_second': 10.393, 'train_loss': 0.5769250052315849, 'epoch': 1.0}\n",
      "Subset Accuracy: 0.1250\n",
      "Hamming Loss: 0.2844\n",
      "Micro Precision: 0.6005\n",
      "Micro Recall: 0.6966\n",
      "Micro F1-Score: 0.6450\n",
      "Precision per label: [0.78571429 0.675      0.49484536 0.65384615 0.88235294 0.4875\n",
      " 0.66666667 0.65277778]\n",
      "Recall per label: [0.31428571 0.69230769 0.96       0.796875   0.35714286 0.84782609\n",
      " 0.47619048 0.79661017]\n",
      "F1-Score per label: [0.44897959 0.6835443  0.65306122 0.71830986 0.50847458 0.61904762\n",
      " 0.55555556 0.71755725]\n",
      "{np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(8)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4025847/3782111259.py:79: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    }
   ],
   "source": [
    "file_name = [\"First_200.csv\", \"NADIcombined_cleaned_MULTI_LABEL_MODIFIED_FINAL.csv\", \"First_1000.csv\", \"balanced_multilabel_dataset.csv\", \"balanced_multilabel_dataset_500.csv\"]\n",
    "dataset_path = f\"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/our_data/{file_name[4]}\"\n",
    "dev_path = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/dev/NADI2024_subtask1_dev2.tsv\"\n",
    "labels = ['Algeria', 'Bahrain', 'Egypt', 'Iraq', 'Jordan', 'Kuwait',\n",
    "       'Lebanon', 'Libya', 'Morocco', 'Oman', 'Palestine', 'Qatar',\n",
    "       'Saudi_Arabia', 'Sudan', 'Syria', 'Tunisia', 'UAE', 'Yemen']\n",
    "trainer = BertTrainer(\n",
    "    training_dataset_path=dataset_path,\n",
    "    model_name=\"CAMeL-Lab/bert-base-arabic-camelbert-mix\",\n",
    "    labels=labels,\n",
    "    threshold=0.3,\n",
    "    exp_num=7\n",
    ")\n",
    "trainer.train(\n",
    "    num_train_epochs=1,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    "    per_device_train_batch_size=24,\n",
    "    per_device_eval_batch_size=24\n",
    ")\n",
    "trainer.evaluate(dev_path=dev_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OVERALL SCORES:\n",
      "MACRO AVERAGE PRECISION SCORE: 66.23 %\n",
      "MACRO AVERAGE RECALL SCORE: 65.52 %\n",
      "MACRO AVERAGE F1-SCORE: 61.31 %\n",
      "MACRO AVERAGE ACCURACY: 71.56 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "scorer_script = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/NADI2024-ST1-Scorer.py\"\n",
    "gold_file = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/sample_submission/NADI2024_subtask1_dev2_gold.txt\"\n",
    "predictions_file = \"/home/lara.hassan/Documents/Cross-Country-Dialectal-Arabic-Identification/exp_7/camelbert_finetuned_epochs_1_eval_f1_0.7622_greater_threshold_0.3/CAMeL-Lab-bert-base-arabic-camelbert-mix-experiment-7_predictions.txt\"\n",
    "!python3 \"{scorer_script}\" \"{gold_file}\" \"{predictions_file}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPERIMENT 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at CAMeL-Lab/bert-base-arabic-camelbert-mix and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/lara.hassan/.conda/envs/myenv/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "                                                 \n",
      " 50%|█████     | 252/504 [00:19<00:19, 13.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.48796144127845764, 'eval_f1': 0.7621827981136312, 'eval_roc_auc': 0.7501663991770002, 'eval_accuracy': 0.12667660208643816, 'eval_runtime': 0.4798, 'eval_samples_per_second': 1398.644, 'eval_steps_per_second': 58.364, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 502/504 [00:42<00:00, 12.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5124, 'grad_norm': 3.563359260559082, 'learning_rate': 4.99e-05, 'epoch': 1.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 504/504 [00:42<00:00, 12.74it/s]\n",
      "100%|██████████| 504/504 [00:45<00:00, 12.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4665817618370056, 'eval_f1': 0.769424942809813, 'eval_roc_auc': 0.7668044404020513, 'eval_accuracy': 0.14754098360655737, 'eval_runtime': 0.4957, 'eval_samples_per_second': 1353.615, 'eval_steps_per_second': 56.485, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 504/504 [00:47<00:00, 10.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 47.8604, 'train_samples_per_second': 252.15, 'train_steps_per_second': 10.531, 'train_loss': 0.5122848169671165, 'epoch': 2.0}\n",
      "Subset Accuracy: 0.1333\n",
      "Hamming Loss: 0.2646\n",
      "Micro Precision: 0.6321\n",
      "Micro Recall: 0.6854\n",
      "Micro F1-Score: 0.6577\n",
      "Precision per label: [0.6875     0.70833333 0.52808989 0.65384615 1.         0.53846154\n",
      " 0.64705882 0.75510204]\n",
      "Recall per label: [0.31428571 0.87179487 0.94       0.796875   0.26190476 0.91304348\n",
      " 0.52380952 0.62711864]\n",
      "F1-Score per label: [0.43137255 0.7816092  0.67625899 0.71830986 0.41509434 0.67741935\n",
      " 0.57894737 0.68518519]\n",
      "{np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4025847/3782111259.py:79: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    }
   ],
   "source": [
    "file_name = [\"First_200.csv\", \"NADIcombined_cleaned_MULTI_LABEL_MODIFIED_FINAL.csv\", \"First_1000.csv\", \"balanced_multilabel_dataset.csv\", \"balanced_multilabel_dataset_500.csv\"]\n",
    "dataset_path = f\"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/our_data/{file_name[4]}\"\n",
    "dev_path = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/dev/NADI2024_subtask1_dev2.tsv\"\n",
    "labels = ['Algeria', 'Bahrain', 'Egypt', 'Iraq', 'Jordan', 'Kuwait',\n",
    "       'Lebanon', 'Libya', 'Morocco', 'Oman', 'Palestine', 'Qatar',\n",
    "       'Saudi_Arabia', 'Sudan', 'Syria', 'Tunisia', 'UAE', 'Yemen']\n",
    "trainer = BertTrainer(\n",
    "    training_dataset_path=dataset_path,\n",
    "    model_name=\"CAMeL-Lab/bert-base-arabic-camelbert-mix\",\n",
    "    labels=labels,\n",
    "    threshold=0.3,\n",
    "    exp_num=8\n",
    ")\n",
    "trainer.train(\n",
    "    num_train_epochs=2,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    "    per_device_train_batch_size=24,\n",
    "    per_device_eval_batch_size=24\n",
    ")\n",
    "trainer.evaluate(dev_path=dev_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OVERALL SCORES:\n",
      "MACRO AVERAGE PRECISION SCORE: 68.98 %\n",
      "MACRO AVERAGE RECALL SCORE: 65.61 %\n",
      "MACRO AVERAGE F1-SCORE: 62.05 %\n",
      "MACRO AVERAGE ACCURACY: 73.54 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "scorer_script = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/NADI2024-ST1-Scorer.py\"\n",
    "gold_file = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/sample_submission/NADI2024_subtask1_dev2_gold.txt\"\n",
    "predictions_file = \"/home/lara.hassan/Documents/Cross-Country-Dialectal-Arabic-Identification/exp_8/camelbert_finetuned_epochs_2_eval_f1_0.7694_greater_threshold_0.3/CAMeL-Lab-bert-base-arabic-camelbert-mix-experiment-8_predictions.txt\"\n",
    "!python3 \"{scorer_script}\" \"{gold_file}\" \"{predictions_file}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPERIMENT 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lara.hassan/.conda/envs/myenv/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      " 50%|█████     | 252/504 [00:19<00:18, 13.85it/s]\n",
      " 50%|█████     | 252/504 [00:19<00:18, 13.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4918179214000702, 'eval_f1': 0.7811874048553802, 'eval_roc_auc': 0.7815018201807157, 'eval_accuracy': 0.16989567809239942, 'eval_runtime': 0.495, 'eval_samples_per_second': 1355.615, 'eval_steps_per_second': 56.568, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 502/504 [00:41<00:00, 13.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3169, 'grad_norm': 3.382401466369629, 'learning_rate': 4.99e-05, 'epoch': 1.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 504/504 [00:41<00:00, 13.63it/s]\n",
      "100%|██████████| 504/504 [00:44<00:00, 13.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5153090953826904, 'eval_f1': 0.7765307806568741, 'eval_roc_auc': 0.7810091627168709, 'eval_accuracy': 0.16095380029806258, 'eval_runtime': 0.4839, 'eval_samples_per_second': 1386.632, 'eval_steps_per_second': 57.862, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 504/504 [00:46<00:00, 10.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 46.7301, 'train_samples_per_second': 258.249, 'train_steps_per_second': 10.785, 'train_loss': 0.3173308519143907, 'epoch': 2.0}\n",
      "Subset Accuracy: 0.1833\n",
      "Hamming Loss: 0.2406\n",
      "Micro Precision: 0.6791\n",
      "Micro Recall: 0.6657\n",
      "Micro F1-Score: 0.6723\n",
      "Precision per label: [0.78571429 0.76744186 0.57746479 0.73134328 0.84210526 0.56060606\n",
      " 0.66666667 0.74074074]\n",
      "Recall per label: [0.31428571 0.84615385 0.82       0.765625   0.38095238 0.80434783\n",
      " 0.47619048 0.6779661 ]\n",
      "F1-Score per label: [0.44897959 0.80487805 0.67768595 0.7480916  0.52459016 0.66071429\n",
      " 0.55555556 0.7079646 ]\n",
      "{np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(8)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4025847/3782111259.py:79: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    }
   ],
   "source": [
    "file_name = [\"First_200.csv\", \"NADIcombined_cleaned_MULTI_LABEL_MODIFIED_FINAL.csv\", \"First_1000.csv\", \"balanced_multilabel_dataset.csv\", \"balanced_multilabel_dataset_500.csv\"]\n",
    "dataset_path = f\"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/our_data/{file_name[4]}\"\n",
    "dev_path = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/dev/NADI2024_subtask1_dev2.tsv\"\n",
    "labels = ['Algeria', 'Bahrain', 'Egypt', 'Iraq', 'Jordan', 'Kuwait',\n",
    "       'Lebanon', 'Libya', 'Morocco', 'Oman', 'Palestine', 'Qatar',\n",
    "       'Saudi_Arabia', 'Sudan', 'Syria', 'Tunisia', 'UAE', 'Yemen']\n",
    "trainer = BertTrainer(\n",
    "    training_dataset_path=dataset_path,\n",
    "    model_name=\"/home/lara.hassan/Documents/Cross-Country-Dialectal-Arabic-Identification/exp_8/camelbert_finetuned_epochs_2_eval_f1_0.7694_greater_threshold_0.3\",\n",
    "    labels=labels,\n",
    "    threshold=0.3,\n",
    "    exp_num=9\n",
    ")\n",
    "trainer.train(\n",
    "    num_train_epochs=2,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    "    per_device_train_batch_size=24,\n",
    "    per_device_eval_batch_size=24\n",
    ")\n",
    "trainer.evaluate(dev_path=dev_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OVERALL SCORES:\n",
      "MACRO AVERAGE PRECISION SCORE: 70.90 %\n",
      "MACRO AVERAGE RECALL SCORE: 63.57 %\n",
      "MACRO AVERAGE F1-SCORE: 64.11 %\n",
      "MACRO AVERAGE ACCURACY: 75.94 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "scorer_script = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/NADI2024-ST1-Scorer.py\"\n",
    "gold_file = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/sample_submission/NADI2024_subtask1_dev2_gold.txt\"\n",
    "predictions_file = \"/home/lara.hassan/Documents/Cross-Country-Dialectal-Arabic-Identification/exp_9/camelbert_finetuned_epochs_2_eval_f1_0.7812_greater_threshold_0.3/-home-lara.hassan-Documents-Cross-Country-Dialectal-Arabic-Identification-exp_8-camelbert_finetuned_epochs_2_eval_f1_0.7694_greater_threshold_0.3-experiment-9_predictions.txt\"\n",
    "!python3 \"{scorer_script}\" \"{gold_file}\" \"{predictions_file}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPERIMENT 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at CAMeL-Lab/bert-base-arabic-camelbert-mix and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/lara.hassan/.conda/envs/myenv/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      " 17%|█▋        | 330/1980 [00:25<03:22,  8.15it/s]\n",
      " 17%|█▋        | 330/1980 [00:26<03:22,  8.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4739709496498108, 'eval_f1': 0.7463900752491357, 'eval_roc_auc': 0.7755923502913882, 'eval_accuracy': 0.11161731207289294, 'eval_runtime': 0.6115, 'eval_samples_per_second': 1435.763, 'eval_steps_per_second': 60.505, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 502/1980 [00:42<02:00, 12.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5121, 'grad_norm': 3.3443150520324707, 'learning_rate': 4.99e-05, 'epoch': 1.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 660/1980 [00:55<02:11, 10.03it/s]\n",
      " 33%|███▎      | 660/1980 [00:56<02:11, 10.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.454266220331192, 'eval_f1': 0.7598915235380015, 'eval_roc_auc': 0.7901453281480804, 'eval_accuracy': 0.17539863325740318, 'eval_runtime': 0.6578, 'eval_samples_per_second': 1334.809, 'eval_steps_per_second': 56.25, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 990/1980 [01:26<01:18, 12.68it/s]\n",
      " 50%|█████     | 990/1980 [01:26<01:18, 12.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.475048303604126, 'eval_f1': 0.7562757722636186, 'eval_roc_auc': 0.7892164231072305, 'eval_accuracy': 0.16856492027334852, 'eval_runtime': 0.6607, 'eval_samples_per_second': 1328.888, 'eval_steps_per_second': 56.001, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 1002/1980 [01:30<02:36,  6.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3603, 'grad_norm': 1.35929274559021, 'learning_rate': 3.314189189189189e-05, 'epoch': 3.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 1320/1980 [01:57<00:52, 12.62it/s]\n",
      " 67%|██████▋   | 1320/1980 [01:57<00:52, 12.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5146341323852539, 'eval_f1': 0.758403801803061, 'eval_roc_auc': 0.7890565235205352, 'eval_accuracy': 0.16287015945330297, 'eval_runtime': 0.6829, 'eval_samples_per_second': 1285.638, 'eval_steps_per_second': 54.178, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 1502/1980 [02:15<00:39, 11.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2479, 'grad_norm': 2.6936798095703125, 'learning_rate': 1.6250000000000002e-05, 'epoch': 4.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 1650/1980 [02:28<00:27, 11.80it/s]\n",
      " 83%|████████▎ | 1650/1980 [02:28<00:27, 11.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5459094643592834, 'eval_f1': 0.7602208400307499, 'eval_roc_auc': 0.7907542852717375, 'eval_accuracy': 0.16173120728929385, 'eval_runtime': 0.7121, 'eval_samples_per_second': 1233.033, 'eval_steps_per_second': 51.962, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1980/1980 [02:58<00:00, 13.02it/s]\n",
      "100%|██████████| 1980/1980 [03:01<00:00, 13.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5531436800956726, 'eval_f1': 0.7577061437539677, 'eval_roc_auc': 0.7889758052175457, 'eval_accuracy': 0.15831435079726652, 'eval_runtime': 0.7475, 'eval_samples_per_second': 1174.636, 'eval_steps_per_second': 49.501, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1980/1980 [03:04<00:00, 10.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 184.5783, 'train_samples_per_second': 256.704, 'train_steps_per_second': 10.727, 'train_loss': 0.32922791375054256, 'epoch': 6.0}\n",
      "Subset Accuracy: 0.1583\n",
      "Hamming Loss: 0.2594\n",
      "Micro Precision: 0.6801\n",
      "Micro Recall: 0.5674\n",
      "Micro F1-Score: 0.6187\n",
      "Precision per label: [0.69230769 0.775      0.6        0.71929825 0.69230769 0.62264151\n",
      " 0.46153846 0.77083333]\n",
      "Recall per label: [0.25714286 0.79487179 0.72       0.640625   0.21428571 0.7173913\n",
      " 0.28571429 0.62711864]\n",
      "F1-Score per label: [0.375      0.78481013 0.65454545 0.67768595 0.32727273 0.66666667\n",
      " 0.35294118 0.69158879]\n",
      "{np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4025847/3782111259.py:79: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    }
   ],
   "source": [
    "file_name = [\"First_200.csv\", \"NADIcombined_cleaned_MULTI_LABEL_MODIFIED_FINAL.csv\", \"First_1000.csv\", \"balanced_multilabel_dataset.csv\", \"balanced_multilabel_dataset_500.csv\", \"balanced_multilabel_dataset_750.csv\"]\n",
    "dataset_path = f\"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/our_data/{file_name[5]}\"\n",
    "dev_path = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/dev/NADI2024_subtask1_dev2.tsv\"\n",
    "labels = ['Algeria', 'Bahrain', 'Egypt', 'Iraq', 'Jordan', 'Kuwait',\n",
    "       'Lebanon', 'Libya', 'Morocco', 'Oman', 'Palestine', 'Qatar',\n",
    "       'Saudi_Arabia', 'Sudan', 'Syria', 'Tunisia', 'UAE', 'Yemen']\n",
    "trainer = BertTrainer(\n",
    "    training_dataset_path=dataset_path,\n",
    "    model_name=\"CAMeL-Lab/bert-base-arabic-camelbert-mix\",\n",
    "    labels=labels,\n",
    "    threshold=0.3,\n",
    "    exp_num=10\n",
    ")\n",
    "trainer.train(\n",
    "    num_train_epochs=6,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    "    per_device_train_batch_size=24,\n",
    "    per_device_eval_batch_size=24\n",
    ")\n",
    "trainer.evaluate(dev_path=dev_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OVERALL SCORES:\n",
      "MACRO AVERAGE PRECISION SCORE: 66.67 %\n",
      "MACRO AVERAGE RECALL SCORE: 53.21 %\n",
      "MACRO AVERAGE F1-SCORE: 56.63 %\n",
      "MACRO AVERAGE ACCURACY: 74.06 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "scorer_script = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/NADI2024-ST1-Scorer.py\"\n",
    "gold_file = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/sample_submission/NADI2024_subtask1_dev2_gold.txt\"\n",
    "predictions_file = \"exp_10/camelbert_finetuned_epochs_6_eval_f1_0.7602_greater_threshold_0.3/CAMeL-Lab-bert-base-arabic-camelbert-mix-experiment-10_predictions.txt\"\n",
    "!python3 \"{scorer_script}\" \"{gold_file}\" \"{predictions_file}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lara.hassan/.conda/envs/myenv/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      " 17%|█▋        | 252/1512 [00:19<01:30, 13.87it/s]\n",
      " 17%|█▋        | 252/1512 [00:19<01:30, 13.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.28491029143333435, 'eval_f1': 0.8595482203884305, 'eval_roc_auc': 0.8654463416464854, 'eval_accuracy': 0.28315946348733234, 'eval_runtime': 0.4594, 'eval_samples_per_second': 1460.564, 'eval_steps_per_second': 60.948, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 502/1512 [00:41<01:19, 12.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2935, 'grad_norm': 3.5294675827026367, 'learning_rate': 5e-05, 'epoch': 1.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 504/1512 [00:41<01:15, 13.29it/s]\n",
      " 33%|███▎      | 504/1512 [00:41<01:15, 13.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.30023330450057983, 'eval_f1': 0.8561597851628063, 'eval_roc_auc': 0.8624923837192617, 'eval_accuracy': 0.2786885245901639, 'eval_runtime': 0.4719, 'eval_samples_per_second': 1422.021, 'eval_steps_per_second': 59.339, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 756/1512 [01:03<00:55, 13.60it/s]\n",
      " 50%|█████     | 756/1512 [01:04<00:55, 13.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.33522099256515503, 'eval_f1': 0.8580319596299412, 'eval_roc_auc': 0.8644862487499366, 'eval_accuracy': 0.28912071535022354, 'eval_runtime': 0.4806, 'eval_samples_per_second': 1396.254, 'eval_steps_per_second': 58.264, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▋   | 1002/1512 [01:25<00:39, 12.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2058, 'grad_norm': 2.2834112644195557, 'learning_rate': 2.5296442687747035e-05, 'epoch': 3.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 1008/1512 [01:25<00:38, 13.25it/s]\n",
      " 67%|██████▋   | 1008/1512 [01:26<00:38, 13.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3422885537147522, 'eval_f1': 0.8558310376492194, 'eval_roc_auc': 0.8618272837252222, 'eval_accuracy': 0.2906110283159464, 'eval_runtime': 0.4943, 'eval_samples_per_second': 1357.393, 'eval_steps_per_second': 56.642, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 1260/1512 [01:48<00:18, 13.55it/s]\n",
      " 83%|████████▎ | 1260/1512 [01:48<00:18, 13.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3467765152454376, 'eval_f1': 0.8621884241656105, 'eval_roc_auc': 0.8688706876949601, 'eval_accuracy': 0.30849478390462, 'eval_runtime': 0.4589, 'eval_samples_per_second': 1462.286, 'eval_steps_per_second': 61.019, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 1502/1512 [02:10<00:00, 13.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1434, 'grad_norm': 1.2461748123168945, 'learning_rate': 5.928853754940711e-07, 'epoch': 5.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1512/1512 [02:10<00:00, 13.59it/s]\n",
      "100%|██████████| 1512/1512 [02:13<00:00, 13.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.35399869084358215, 'eval_f1': 0.8584212747994935, 'eval_roc_auc': 0.8651041609176625, 'eval_accuracy': 0.30998509687034276, 'eval_runtime': 0.5414, 'eval_samples_per_second': 1239.478, 'eval_steps_per_second': 51.722, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1512/1512 [02:16<00:00, 11.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 136.0435, 'train_samples_per_second': 266.121, 'train_steps_per_second': 11.114, 'train_loss': 0.21364403985164784, 'epoch': 6.0}\n",
      "Subset Accuracy: 0.1667\n",
      "Hamming Loss: 0.2687\n",
      "Micro Precision: 0.6960\n",
      "Micro Recall: 0.4888\n",
      "Micro F1-Score: 0.5743\n",
      "Precision per label: [0.69230769 0.74358974 0.63265306 0.73170732 0.76923077 0.65789474\n",
      " 0.5        0.76744186]\n",
      "Recall per label: [0.25714286 0.74358974 0.62       0.46875    0.23809524 0.54347826\n",
      " 0.33333333 0.55932203]\n",
      "F1-Score per label: [0.375      0.74358974 0.62626263 0.57142857 0.36363636 0.5952381\n",
      " 0.4        0.64705882]\n",
      "{np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4025847/3782111259.py:79: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    }
   ],
   "source": [
    "file_name = [\"First_200.csv\", \"NADIcombined_cleaned_MULTI_LABEL_MODIFIED_FINAL.csv\", \"First_1000.csv\", \"balanced_multilabel_dataset.csv\", \"balanced_multilabel_dataset_500.csv\", \"balanced_multilabel_dataset_750.csv\"]\n",
    "dataset_path = f\"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/our_data/{file_name[4]}\"\n",
    "dev_path = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/dev/NADI2024_subtask1_dev2.tsv\"\n",
    "labels = ['Algeria', 'Bahrain', 'Egypt', 'Iraq', 'Jordan', 'Kuwait',\n",
    "       'Lebanon', 'Libya', 'Morocco', 'Oman', 'Palestine', 'Qatar',\n",
    "       'Saudi_Arabia', 'Sudan', 'Syria', 'Tunisia', 'UAE', 'Yemen']\n",
    "trainer = BertTrainer(\n",
    "    training_dataset_path=dataset_path,\n",
    "    model_name=\"/home/lara.hassan/Documents/Cross-Country-Dialectal-Arabic-Identification/exp_10/camelbert_finetuned_epochs_6_eval_f1_0.7602_greater_threshold_0.3\",\n",
    "    labels=labels,\n",
    "    threshold=0.3,\n",
    "    exp_num=10\n",
    ")\n",
    "trainer.train(\n",
    "    num_train_epochs=6,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    "    per_device_train_batch_size=24,\n",
    "    per_device_eval_batch_size=24\n",
    ")\n",
    "trainer.evaluate(dev_path=dev_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OVERALL SCORES:\n",
      "MACRO AVERAGE PRECISION SCORE: 68.69 %\n",
      "MACRO AVERAGE RECALL SCORE: 47.05 %\n",
      "MACRO AVERAGE F1-SCORE: 54.03 %\n",
      "MACRO AVERAGE ACCURACY: 73.12 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "scorer_script = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/NADI2024-ST1-Scorer.py\"\n",
    "gold_file = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/sample_submission/NADI2024_subtask1_dev2_gold.txt\"\n",
    "predictions_file = \"/home/lara.hassan/Documents/Cross-Country-Dialectal-Arabic-Identification/exp_10/camelbert_finetuned_epochs_6_eval_f1_0.8622_greater_threshold_0.3/-home-lara.hassan-Documents-Cross-Country-Dialectal-Arabic-Identification-exp_10-camelbert_finetuned_epochs_6_eval_f1_0.7602_greater_threshold_0.3-experiment-10_predictions.txt\"\n",
    "!python3 \"{scorer_script}\" \"{gold_file}\" \"{predictions_file}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPERIMENT 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at CAMeL-Lab/bert-base-arabic-camelbert-mix and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/lara.hassan/.conda/envs/myenv/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      " 13%|█▎        | 503/3952 [00:27<02:59, 19.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5519, 'grad_norm': 1.2335233688354492, 'learning_rate': 4.97e-05, 'epoch': 0.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 25%|██▌       | 988/3952 [00:54<02:33, 19.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.48687317967414856, 'eval_f1': 0.7363865383373167, 'eval_roc_auc': 0.7617819014921697, 'eval_accuracy': 0.12414578587699317, 'eval_runtime': 0.608, 'eval_samples_per_second': 1444.047, 'eval_steps_per_second': 60.854, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1002/3952 [00:57<05:29,  8.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4956, 'grad_norm': 3.4177091121673584, 'learning_rate': 4.2801274623406724e-05, 'epoch': 1.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 1502/3952 [01:26<02:30, 16.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.409, 'grad_norm': 3.1355440616607666, 'learning_rate': 3.555909617612978e-05, 'epoch': 1.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 50%|█████     | 1976/3952 [01:56<02:11, 15.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.46634554862976074, 'eval_f1': 0.7590600808385285, 'eval_roc_auc': 0.7884490538348937, 'eval_accuracy': 0.17995444191343962, 'eval_runtime': 0.6889, 'eval_samples_per_second': 1274.564, 'eval_steps_per_second': 53.712, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 2002/3952 [02:00<02:16, 14.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3971, 'grad_norm': 3.05737566947937, 'learning_rate': 2.8316917728852837e-05, 'epoch': 2.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 2502/3952 [02:31<01:22, 17.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3024, 'grad_norm': 1.4415926933288574, 'learning_rate': 2.10747392815759e-05, 'epoch': 2.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 75%|███████▌  | 2964/3952 [03:00<01:03, 15.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.48994776606559753, 'eval_f1': 0.7606552726756968, 'eval_roc_auc': 0.7920201280773378, 'eval_accuracy': 0.1765375854214123, 'eval_runtime': 0.695, 'eval_samples_per_second': 1263.324, 'eval_steps_per_second': 53.238, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 3002/3952 [03:05<00:58, 16.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3012, 'grad_norm': 3.860936403274536, 'learning_rate': 1.3832560834298958e-05, 'epoch': 3.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▊ | 3502/3952 [03:35<00:26, 16.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2342, 'grad_norm': 2.205425500869751, 'learning_rate': 6.590382387022016e-06, 'epoch': 3.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|██████████| 3952/3952 [04:05<00:00, 18.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5275660753250122, 'eval_f1': 0.7625708091474929, 'eval_roc_auc': 0.7929900864074229, 'eval_accuracy': 0.17198177676537585, 'eval_runtime': 0.6341, 'eval_samples_per_second': 1384.718, 'eval_steps_per_second': 58.354, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3952/3952 [04:08<00:00, 15.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 248.0563, 'train_samples_per_second': 127.342, 'train_steps_per_second': 15.932, 'train_loss': 0.3670956538273738, 'epoch': 4.0}\n",
      "Subset Accuracy: 0.1833\n",
      "Hamming Loss: 0.2573\n",
      "Micro Precision: 0.6860\n",
      "Micro Recall: 0.5646\n",
      "Micro F1-Score: 0.6194\n",
      "Precision per label: [0.75       0.74418605 0.5862069  0.72222222 0.84615385 0.61538462\n",
      " 0.46666667 0.80434783]\n",
      "Recall per label: [0.25714286 0.82051282 0.68       0.609375   0.26190476 0.69565217\n",
      " 0.33333333 0.62711864]\n",
      "F1-Score per label: [0.38297872 0.7804878  0.62962963 0.66101695 0.4        0.65306122\n",
      " 0.38888889 0.7047619 ]\n",
      "{np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(8)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4025847/3782111259.py:79: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    }
   ],
   "source": [
    "file_name = [\"First_200.csv\", \"NADIcombined_cleaned_MULTI_LABEL_MODIFIED_FINAL.csv\", \"First_1000.csv\", \"balanced_multilabel_dataset.csv\", \"balanced_multilabel_dataset_750.csv\"]\n",
    "dataset_path = f\"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/our_data/{file_name[4]}\"\n",
    "dev_path = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/dev/NADI2024_subtask1_dev2.tsv\"\n",
    "labels = ['Algeria', 'Bahrain', 'Egypt', 'Iraq', 'Jordan', 'Kuwait',\n",
    "       'Lebanon', 'Libya', 'Morocco', 'Oman', 'Palestine', 'Qatar',\n",
    "       'Saudi_Arabia', 'Sudan', 'Syria', 'Tunisia', 'UAE', 'Yemen']\n",
    "trainer = BertTrainer(\n",
    "    training_dataset_path=dataset_path,\n",
    "    model_name=\"CAMeL-Lab/bert-base-arabic-camelbert-mix\",\n",
    "    labels=labels,\n",
    "    threshold=0.3,\n",
    "    exp_num=11\n",
    ")\n",
    "trainer.train(\n",
    "    num_train_epochs=4,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=24\n",
    ")\n",
    "trainer.evaluate(dev_path=dev_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OVERALL SCORES:\n",
      "MACRO AVERAGE PRECISION SCORE: 69.19 %\n",
      "MACRO AVERAGE RECALL SCORE: 53.56 %\n",
      "MACRO AVERAGE F1-SCORE: 57.51 %\n",
      "MACRO AVERAGE ACCURACY: 74.27 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "scorer_script = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/NADI2024-ST1-Scorer.py\"\n",
    "gold_file = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/sample_submission/NADI2024_subtask1_dev2_gold.txt\"\n",
    "predictions_file = \"/home/lara.hassan/Documents/Cross-Country-Dialectal-Arabic-Identification/exp_11/camelbert_finetuned_epochs_4_eval_f1_0.7626_greater_threshold_0.3/CAMeL-Lab-bert-base-arabic-camelbert-mix-experiment-11_predictions.txt\"\n",
    "!python3 \"{scorer_script}\" \"{gold_file}\" \"{predictions_file}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPERIMENT 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lara.hassan/.conda/envs/myenv/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "\n",
      "  5%|▌         | 52/988 [02:54<00:52, 17.78it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3145, 'grad_norm': 3.7815072536468506, 'learning_rate': 4.99e-05, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                  \n",
      "  5%|▌         | 52/988 [02:55<00:52, 17.78it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5411572456359863, 'eval_f1': 0.7787280360752604, 'eval_roc_auc': 0.7744481507891129, 'eval_accuracy': 0.16393442622950818, 'eval_runtime': 0.4921, 'eval_samples_per_second': 1363.594, 'eval_steps_per_second': 56.901, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  5%|▌         | 52/988 [03:26<00:52, 17.78it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3036, 'grad_norm': 2.7231392860412598, 'learning_rate': 3.7617866004962784e-05, 'epoch': 1.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A                                                \n",
      "\n",
      "                                                \n",
      "  5%|▌         | 52/988 [03:27<00:52, 17.78it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5048529505729675, 'eval_f1': 0.7767005978348683, 'eval_roc_auc': 0.7780685608761594, 'eval_accuracy': 0.15797317436661698, 'eval_runtime': 0.4733, 'eval_samples_per_second': 1417.646, 'eval_steps_per_second': 59.157, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  5%|▌         | 52/988 [03:59<00:52, 17.78it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2531, 'grad_norm': 1.3901019096374512, 'learning_rate': 2.5210918114143922e-05, 'epoch': 2.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                   \n",
      "  5%|▌         | 52/988 [04:00<00:52, 17.78it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5629974603652954, 'eval_f1': 0.7656576200417536, 'eval_roc_auc': 0.7780476988897891, 'eval_accuracy': 0.15201192250372578, 'eval_runtime': 0.4748, 'eval_samples_per_second': 1413.085, 'eval_steps_per_second': 58.966, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  5%|▌         | 52/988 [04:31<00:52, 17.78it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2008, 'grad_norm': 1.9427447319030762, 'learning_rate': 1.2803970223325062e-05, 'epoch': 3.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                   \n",
      "  5%|▌         | 52/988 [04:32<00:52, 17.78it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5819849967956543, 'eval_f1': 0.7798867798867799, 'eval_roc_auc': 0.7856383988591031, 'eval_accuracy': 0.16095380029806258, 'eval_runtime': 0.4735, 'eval_samples_per_second': 1416.998, 'eval_steps_per_second': 59.13, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  5%|▌         | 52/988 [05:03<00:52, 17.78it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1635, 'grad_norm': 1.539159893989563, 'learning_rate': 3.970223325062035e-07, 'epoch': 4.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                   \n",
      "  5%|▌         | 52/988 [05:07<00:52, 17.78it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.599375307559967, 'eval_f1': 0.7798488664987405, 'eval_roc_auc': 0.7867945833894436, 'eval_accuracy': 0.15350223546944858, 'eval_runtime': 0.4795, 'eval_samples_per_second': 1399.392, 'eval_steps_per_second': 58.395, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2515/2515 [02:44<00:00, 15.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 164.7329, 'train_samples_per_second': 183.145, 'train_steps_per_second': 15.267, 'train_loss': 0.24653972858937076, 'epoch': 5.0}\n",
      "Subset Accuracy: 0.1583\n",
      "Hamming Loss: 0.2635\n",
      "Micro Precision: 0.6873\n",
      "Micro Recall: 0.5309\n",
      "Micro F1-Score: 0.5990\n",
      "Precision per label: [0.8        0.7804878  0.60377358 0.71111111 0.66666667 0.61904762\n",
      " 0.625      0.72916667]\n",
      "Recall per label: [0.34285714 0.82051282 0.64       0.5        0.23809524 0.56521739\n",
      " 0.47619048 0.59322034]\n",
      "F1-Score per label: [0.48       0.8        0.62135922 0.58715596 0.35087719 0.59090909\n",
      " 0.54054054 0.65420561]\n",
      "{np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(7), np.int64(8)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4025847/525870965.py:81: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    }
   ],
   "source": [
    "file_name = [\"First_200.csv\", \"NADIcombined_cleaned_MULTI_LABEL_MODIFIED_FINAL.csv\", \"First_1000.csv\", \"balanced_multilabel_dataset.csv\", \"balanced_multilabel_dataset_500.csv\"]\n",
    "dataset_path = f\"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/our_data/{file_name[4]}\"\n",
    "dev_path = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/dev/NADI2024_subtask1_dev2.tsv\"\n",
    "labels = ['Algeria', 'Bahrain', 'Egypt', 'Iraq', 'Jordan', 'Kuwait',\n",
    "       'Lebanon', 'Libya', 'Morocco', 'Oman', 'Palestine', 'Qatar',\n",
    "       'Saudi_Arabia', 'Sudan', 'Syria', 'Tunisia', 'UAE', 'Yemen']\n",
    "trainer = BertTrainer(\n",
    "    training_dataset_path=dataset_path,\n",
    "    model_name=\"/home/lara.hassan/Documents/Cross-Country-Dialectal-Arabic-Identification/exp_9/camelbert_finetuned_epochs_2_eval_f1_0.7812_greater_threshold_0.3\",\n",
    "    labels=labels,\n",
    "    threshold=0.3,\n",
    "    exp_num=12\n",
    ")\n",
    "trainer.train(\n",
    "    num_train_epochs=5,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    "    per_device_train_batch_size=12,\n",
    "    per_device_eval_batch_size=24\n",
    ")\n",
    "trainer.evaluate(dev_path=dev_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/lara.hassan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OVERALL SCORES:\n",
      "MACRO AVERAGE PRECISION SCORE: 69.19 %\n",
      "MACRO AVERAGE RECALL SCORE: 52.20 %\n",
      "MACRO AVERAGE F1-SCORE: 57.81 %\n",
      "MACRO AVERAGE ACCURACY: 73.65 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "scorer_script = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/NADI2024-ST1-Scorer.py\"\n",
    "gold_file = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/sample_submission/NADI2024_subtask1_dev2_gold.txt\"\n",
    "predictions_file = \"/home/lara.hassan/Documents/Cross-Country-Dialectal-Arabic-Identification/exp_12/camelbert_finetuned_epochs_5_eval_f1_0.7799_greater_threshold_0.3/-home-lara.hassan-Documents-Cross-Country-Dialectal-Arabic-Identification-exp_9-camelbert_finetuned_epochs_2_eval_f1_0.7812_greater_threshold_0.3-experiment-12_predictions.txt\"\n",
    "!python3 \"{scorer_script}\" \"{gold_file}\" \"{predictions_file}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
       " array([[0.9784671 , 0.98201376, 0.97339284, 0.97706646, 0.9714559 ,\n",
       "         0.98421544, 0.98141783, 0.9826909 , 0.9749646 , 0.98521465,\n",
       "         0.9819447 , 0.9868787 , 0.97414124, 0.9840936 , 0.98228765,\n",
       "         0.97850823, 0.9845754 , 0.987375  ]], dtype=float32))"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.predict([\"الله اكبر\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1]]),\n",
       " array([[0.02100437, 0.56673807, 0.96826136, 0.6016521 , 0.6660625 ,\n",
       "         0.7235049 , 0.4344013 , 0.5190795 , 0.01851105, 0.41750893,\n",
       "         0.5469499 , 0.5705108 , 0.7168137 , 0.7305783 , 0.40751025,\n",
       "         0.01428194, 0.30002728, 0.4107563 ]], dtype=float32))"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.predict([\"انا مصرى ياسطااااا\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0]]),\n",
       " array([[0.0186178 , 0.00944908, 0.05379965, 0.11636177, 0.87768656,\n",
       "         0.0102486 , 0.91181   , 0.02546808, 0.01876109, 0.00818791,\n",
       "         0.8321002 , 0.00912564, 0.02909076, 0.01317195, 0.8830344 ,\n",
       "         0.01879707, 0.00575254, 0.01016966]], dtype=float32))"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.predict(['بدى فتوش و بدك ثومية'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " array([[0.13660839, 0.01159736, 0.08344117, 0.15241033, 0.03704717,\n",
       "         0.02442309, 0.02052779, 0.19329959, 0.10212548, 0.01141961,\n",
       "         0.0226292 , 0.01036816, 0.07504072, 0.01890545, 0.02465686,\n",
       "         0.20323272, 0.00656425, 0.0113756 ]], dtype=float32))"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.predict([' خى ما قصرت'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0]]),\n",
       " array([[0.0082516 , 0.78727776, 0.9410069 , 0.87997437, 0.9512329 ,\n",
       "         0.84376645, 0.8782099 , 0.6583077 , 0.00854433, 0.36274344,\n",
       "         0.9314625 , 0.71452844, 0.9380106 , 0.5837373 , 0.88602656,\n",
       "         0.00919655, 0.32562777, 0.27338037]], dtype=float32))"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.predict(['الحمد لله يا زلمة'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
       " array([[0.97029626, 0.98527133, 0.9717254 , 0.9797453 , 0.9780936 ,\n",
       "         0.9874236 , 0.9840936 , 0.9809491 , 0.9636434 , 0.9873262 ,\n",
       "         0.98481095, 0.98926485, 0.97879386, 0.9832145 , 0.98486924,\n",
       "         0.9704086 , 0.98646784, 0.98844737]], dtype=float32))"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.predict(['الحمد لله  '])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]),\n",
       " array([[0.0349467 , 0.03507867, 0.8701566 , 0.22695492, 0.07303239,\n",
       "         0.05964694, 0.03890198, 0.17497347, 0.03704717, 0.0193451 ,\n",
       "         0.06176271, 0.02640552, 0.09704755, 0.4174496 , 0.03725677,\n",
       "         0.02992975, 0.01802074, 0.03015741]], dtype=float32))"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.predict([\"كل زول ليه الزول بتاعه\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPERIMENT 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE DATA TO BE MORE BALANCED\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle, resample\n",
    "\n",
    "directory = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/our_data/\"\n",
    "dataset_path = directory + \"NADIcombined_cleaned_MULTI_LABEL_MODIFIED_FINAL.csv\"\n",
    "output_path  = directory + \"SORTED_multilabel_dataset.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "label_columns = df.columns.difference(['tweet'])\n",
    "df['binary_label'] = df[label_columns].astype(str).agg(''.join, axis=1)\n",
    "sorted_df = df.sort_values(by='binary_label').reset_index(drop=True)\n",
    "sorted_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_columns = ['Algeria', 'Bahrain', 'Egypt', 'Iraq', 'Jordan', 'Kuwait', 'Lebanon', \n",
    "                     'Libya', 'Morocco', 'Oman', 'Palestine', 'Qatar', 'Saudi_Arabia', \n",
    "                     'Sudan', 'Syria', 'Tunisia', 'UAE', 'Yemen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION USED TO KNOW THE BREAKDOWN OF EVERY DIALECT IN A CLASS\n",
    "def get_class_insight(balanced_df):\n",
    "    # Initialize a dictionary to store counts for each class (count of active labels)\n",
    "    counts = {i: (balanced_df[label_columns].sum(axis=1) == i).sum() for i in range(19)}\n",
    "\n",
    "    # Print the counts for each class to verify the balance\n",
    "    print(\"Counts of rows by number of active labels:\")\n",
    "    for count, value in counts.items():\n",
    "        print(f\"Class with {count} active labels: {value}\")\n",
    "\n",
    "    label_counts_by_class = {}\n",
    "\n",
    "    # Loop over each possible count of active labels (1 to number of label columns)\n",
    "    for num_classes in range(1, len(label_columns) + 1):\n",
    "        # Filter rows where the number of active labels equals num_classes\n",
    "        subset = balanced_df[balanced_df[label_columns].sum(axis=1) == num_classes]\n",
    "        \n",
    "        # Count occurrences of each label being active within this subset\n",
    "        label_counts = subset[label_columns].sum()\n",
    "        \n",
    "        # Store the results in the dictionary\n",
    "        label_counts_by_class[num_classes] = label_counts.to_dict()\n",
    "\n",
    "    # Display the results\n",
    "    for num_classes, counts in label_counts_by_class.items():\n",
    "        print(f\"\\nFor class group with {num_classes} active labels:\")\n",
    "        for label, count in counts.items():\n",
    "            print(f\"  {label}: {count} occurrences\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION USED TO SPLIT THE DATA INTO BALANCED CLASSES\n",
    "\n",
    "def split_balanced_threshold(df, label_columns, threshold=750, uni_label=True):\n",
    "    balanced_df = pd.DataFrame()\n",
    "    for num_classes in range(1, len(label_columns) + 1):\n",
    "        # Filter rows with exactly 'num_classes' active labels\n",
    "        subset = df[df[label_columns].sum(axis=1) == num_classes]\n",
    "\n",
    "        if num_classes == 1 and uni_label:\n",
    "            # For the subset with only 1 active label, balance each label to have equal occurrences\n",
    "            balanced_subset = pd.DataFrame()\n",
    "            \n",
    "            # Iterate over each label (dialect) and resample to match the threshold\n",
    "            for label in label_columns:\n",
    "                # Filter rows where the current dialect is active (set to 1)\n",
    "                label_rows = subset[subset[label] == 1]\n",
    "                \n",
    "                # Downsample or upsample to match the threshold for this dialect\n",
    "                if len(label_rows) > threshold//15:\n",
    "                    label_rows = resample(label_rows, n_samples=threshold//15, random_state=42, replace=False)\n",
    "                elif len(label_rows) < threshold//15:\n",
    "                    label_rows = resample(label_rows, n_samples=threshold//15, random_state=42, replace=True)\n",
    "                \n",
    "                # Append the balanced rows for the current dialect to the balanced_subset DataFrame\n",
    "                balanced_subset = pd.concat([balanced_subset, label_rows], ignore_index=True)\n",
    "            \n",
    "            # Add the balanced subset for one active label to the main balanced DataFrame\n",
    "            balanced_df = pd.concat([balanced_df, balanced_subset], ignore_index=True)\n",
    "        \n",
    "        else:\n",
    "            # For other cases, apply the general threshold limit to the subset\n",
    "            if len(subset) > threshold:\n",
    "                subset = shuffle(subset).head(threshold)\n",
    "            balanced_df = pd.concat([balanced_df, subset], ignore_index=True)\n",
    "\n",
    "    return balanced_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION TO REMOVE UNDESIRED CLASSES\n",
    "def modify_dataset_for_active_labels(df):                     \n",
    "    mask = df[label_columns].sum(axis=1).isin([16, 17])\n",
    "    df.loc[mask, label_columns] = 1  \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>Algeria</th>\n",
       "      <th>Bahrain</th>\n",
       "      <th>Egypt</th>\n",
       "      <th>Iraq</th>\n",
       "      <th>Jordan</th>\n",
       "      <th>Kuwait</th>\n",
       "      <th>Lebanon</th>\n",
       "      <th>Libya</th>\n",
       "      <th>...</th>\n",
       "      <th>Palestine</th>\n",
       "      <th>Qatar</th>\n",
       "      <th>Saudi_Arabia</th>\n",
       "      <th>Sudan</th>\n",
       "      <th>Syria</th>\n",
       "      <th>Tunisia</th>\n",
       "      <th>UAE</th>\n",
       "      <th>Yemen</th>\n",
       "      <th>Computed</th>\n",
       "      <th>binary_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10012</td>\n",
       "      <td>تقريبا كلام السيسي بدل يتكلم عربي بيتكلم انجليزي</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>00no000000000000000110012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10032</td>\n",
       "      <td>وای ایشالا که حالش خوبه</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>00no000000000000000110032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10041</td>\n",
       "      <td>اعجبني فيديو علي قوات مدعومه اماراتيا تابعه لنجل</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>00no000000000000000110041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10064</td>\n",
       "      <td>حداقل دیگه حسرت اینکه چرا نگفتم رو نداری</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>00no000000000000000110064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10087</td>\n",
       "      <td>مرعبه كلمه سامحني لان اغلب الاوقات يكون بعدها ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>00no000000000000000110087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet  Algeria  Bahrain  \\\n",
       "0  10012   تقريبا كلام السيسي بدل يتكلم عربي بيتكلم انجليزي        0        0   \n",
       "1  10032                            وای ایشالا که حالش خوبه        0        0   \n",
       "2  10041   اعجبني فيديو علي قوات مدعومه اماراتيا تابعه لنجل        0        0   \n",
       "3  10064           حداقل دیگه حسرت اینکه چرا نگفتم رو نداری        0        0   \n",
       "4  10087  مرعبه كلمه سامحني لان اغلب الاوقات يكون بعدها ...        0        0   \n",
       "\n",
       "   Egypt  Iraq  Jordan  Kuwait  Lebanon  Libya  ...  Palestine  Qatar  \\\n",
       "0      0     0       0       0        0      0  ...          0      0   \n",
       "1      0     0       0       0        0      0  ...          0      0   \n",
       "2      0     0       0       0        0      0  ...          0      0   \n",
       "3      0     0       0       0        0      0  ...          0      0   \n",
       "4      0     0       0       0        0      0  ...          0      0   \n",
       "\n",
       "   Saudi_Arabia  Sudan  Syria  Tunisia  UAE  Yemen  Computed  \\\n",
       "0             0      0      0        0    0      1        no   \n",
       "1             0      0      0        0    0      1        no   \n",
       "2             0      0      0        0    0      1        no   \n",
       "3             0      0      0        0    0      1        no   \n",
       "4             0      0      0        0    0      1        no   \n",
       "\n",
       "                binary_label  \n",
       "0  00no000000000000000110012  \n",
       "1  00no000000000000000110032  \n",
       "2  00no000000000000000110041  \n",
       "3  00no000000000000000110064  \n",
       "4  00no000000000000000110087  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(output_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts of rows by number of active labels:\n",
      "Class with 0 active labels: 0\n",
      "Class with 1 active labels: 41837\n",
      "Class with 2 active labels: 3220\n",
      "Class with 3 active labels: 2329\n",
      "Class with 4 active labels: 2580\n",
      "Class with 5 active labels: 1416\n",
      "Class with 6 active labels: 1709\n",
      "Class with 7 active labels: 1794\n",
      "Class with 8 active labels: 1108\n",
      "Class with 9 active labels: 536\n",
      "Class with 10 active labels: 168\n",
      "Class with 11 active labels: 137\n",
      "Class with 12 active labels: 334\n",
      "Class with 13 active labels: 273\n",
      "Class with 14 active labels: 256\n",
      "Class with 15 active labels: 532\n",
      "Class with 16 active labels: 11\n",
      "Class with 17 active labels: 26\n",
      "Class with 18 active labels: 502\n",
      "\n",
      "For class group with 1 active labels:\n",
      "  Algeria: 2712 occurrences\n",
      "  Bahrain: 417 occurrences\n",
      "  Egypt: 10609 occurrences\n",
      "  Iraq: 6026 occurrences\n",
      "  Jordan: 762 occurrences\n",
      "  Kuwait: 672 occurrences\n",
      "  Lebanon: 1245 occurrences\n",
      "  Libya: 2567 occurrences\n",
      "  Morocco: 2213 occurrences\n",
      "  Oman: 2077 occurrences\n",
      "  Palestine: 794 occurrences\n",
      "  Qatar: 451 occurrences\n",
      "  Saudi_Arabia: 4176 occurrences\n",
      "  Sudan: 858 occurrences\n",
      "  Syria: 1896 occurrences\n",
      "  Tunisia: 1616 occurrences\n",
      "  UAE: 1418 occurrences\n",
      "  Yemen: 1328 occurrences\n",
      "\n",
      "For class group with 2 active labels:\n",
      "  Algeria: 370 occurrences\n",
      "  Bahrain: 54 occurrences\n",
      "  Egypt: 557 occurrences\n",
      "  Iraq: 816 occurrences\n",
      "  Jordan: 243 occurrences\n",
      "  Kuwait: 136 occurrences\n",
      "  Lebanon: 384 occurrences\n",
      "  Libya: 731 occurrences\n",
      "  Morocco: 352 occurrences\n",
      "  Oman: 63 occurrences\n",
      "  Palestine: 215 occurrences\n",
      "  Qatar: 45 occurrences\n",
      "  Saudi_Arabia: 857 occurrences\n",
      "  Sudan: 174 occurrences\n",
      "  Syria: 339 occurrences\n",
      "  Tunisia: 826 occurrences\n",
      "  UAE: 114 occurrences\n",
      "  Yemen: 164 occurrences\n",
      "\n",
      "For class group with 3 active labels:\n",
      "  Algeria: 1262 occurrences\n",
      "  Bahrain: 89 occurrences\n",
      "  Egypt: 235 occurrences\n",
      "  Iraq: 402 occurrences\n",
      "  Jordan: 386 occurrences\n",
      "  Kuwait: 223 occurrences\n",
      "  Lebanon: 317 occurrences\n",
      "  Libya: 378 occurrences\n",
      "  Morocco: 1134 occurrences\n",
      "  Oman: 25 occurrences\n",
      "  Palestine: 300 occurrences\n",
      "  Qatar: 80 occurrences\n",
      "  Saudi_Arabia: 414 occurrences\n",
      "  Sudan: 103 occurrences\n",
      "  Syria: 188 occurrences\n",
      "  Tunisia: 1340 occurrences\n",
      "  UAE: 43 occurrences\n",
      "  Yemen: 68 occurrences\n",
      "\n",
      "For class group with 4 active labels:\n",
      "  Algeria: 280 occurrences\n",
      "  Bahrain: 436 occurrences\n",
      "  Egypt: 111 occurrences\n",
      "  Iraq: 263 occurrences\n",
      "  Jordan: 1737 occurrences\n",
      "  Kuwait: 480 occurrences\n",
      "  Lebanon: 1670 occurrences\n",
      "  Libya: 320 occurrences\n",
      "  Morocco: 269 occurrences\n",
      "  Oman: 26 occurrences\n",
      "  Palestine: 1691 occurrences\n",
      "  Qatar: 425 occurrences\n",
      "  Saudi_Arabia: 573 occurrences\n",
      "  Sudan: 39 occurrences\n",
      "  Syria: 1629 occurrences\n",
      "  Tunisia: 287 occurrences\n",
      "  UAE: 40 occurrences\n",
      "  Yemen: 44 occurrences\n",
      "\n",
      "For class group with 5 active labels:\n",
      "  Algeria: 53 occurrences\n",
      "  Bahrain: 660 occurrences\n",
      "  Egypt: 116 occurrences\n",
      "  Iraq: 977 occurrences\n",
      "  Jordan: 671 occurrences\n",
      "  Kuwait: 694 occurrences\n",
      "  Lebanon: 618 occurrences\n",
      "  Libya: 98 occurrences\n",
      "  Morocco: 49 occurrences\n",
      "  Oman: 131 occurrences\n",
      "  Palestine: 644 occurrences\n",
      "  Qatar: 684 occurrences\n",
      "  Saudi_Arabia: 740 occurrences\n",
      "  Sudan: 23 occurrences\n",
      "  Syria: 628 occurrences\n",
      "  Tunisia: 52 occurrences\n",
      "  UAE: 150 occurrences\n",
      "  Yemen: 92 occurrences\n",
      "\n",
      "For class group with 6 active labels:\n",
      "  Algeria: 6 occurrences\n",
      "  Bahrain: 1402 occurrences\n",
      "  Egypt: 80 occurrences\n",
      "  Iraq: 453 occurrences\n",
      "  Jordan: 351 occurrences\n",
      "  Kuwait: 1427 occurrences\n",
      "  Lebanon: 276 occurrences\n",
      "  Libya: 95 occurrences\n",
      "  Morocco: 9 occurrences\n",
      "  Oman: 1205 occurrences\n",
      "  Palestine: 300 occurrences\n",
      "  Qatar: 1403 occurrences\n",
      "  Saudi_Arabia: 1616 occurrences\n",
      "  Sudan: 18 occurrences\n",
      "  Syria: 284 occurrences\n",
      "  Tunisia: 5 occurrences\n",
      "  UAE: 1133 occurrences\n",
      "  Yemen: 191 occurrences\n",
      "\n",
      "For class group with 7 active labels:\n",
      "  Algeria: 7 occurrences\n",
      "  Bahrain: 1654 occurrences\n",
      "  Egypt: 100 occurrences\n",
      "  Iraq: 528 occurrences\n",
      "  Jordan: 220 occurrences\n",
      "  Kuwait: 1684 occurrences\n",
      "  Lebanon: 143 occurrences\n",
      "  Libya: 79 occurrences\n",
      "  Morocco: 6 occurrences\n",
      "  Oman: 1574 occurrences\n",
      "  Palestine: 160 occurrences\n",
      "  Qatar: 1646 occurrences\n",
      "  Saudi_Arabia: 1759 occurrences\n",
      "  Sudan: 27 occurrences\n",
      "  Syria: 153 occurrences\n",
      "  Tunisia: 8 occurrences\n",
      "  UAE: 1527 occurrences\n",
      "  Yemen: 1283 occurrences\n",
      "\n",
      "For class group with 8 active labels:\n",
      "  Algeria: 4 occurrences\n",
      "  Bahrain: 1052 occurrences\n",
      "  Egypt: 84 occurrences\n",
      "  Iraq: 891 occurrences\n",
      "  Jordan: 259 occurrences\n",
      "  Kuwait: 1066 occurrences\n",
      "  Lebanon: 116 occurrences\n",
      "  Libya: 100 occurrences\n",
      "  Morocco: 5 occurrences\n",
      "  Oman: 974 occurrences\n",
      "  Palestine: 125 occurrences\n",
      "  Qatar: 1037 occurrences\n",
      "  Saudi_Arabia: 1099 occurrences\n",
      "  Sudan: 26 occurrences\n",
      "  Syria: 117 occurrences\n",
      "  Tunisia: 5 occurrences\n",
      "  UAE: 954 occurrences\n",
      "  Yemen: 950 occurrences\n",
      "\n",
      "For class group with 9 active labels:\n",
      "  Algeria: 4 occurrences\n",
      "  Bahrain: 509 occurrences\n",
      "  Egypt: 66 occurrences\n",
      "  Iraq: 488 occurrences\n",
      "  Jordan: 328 occurrences\n",
      "  Kuwait: 516 occurrences\n",
      "  Lebanon: 176 occurrences\n",
      "  Libya: 206 occurrences\n",
      "  Morocco: 6 occurrences\n",
      "  Oman: 362 occurrences\n",
      "  Palestine: 188 occurrences\n",
      "  Qatar: 501 occurrences\n",
      "  Saudi_Arabia: 534 occurrences\n",
      "  Sudan: 37 occurrences\n",
      "  Syria: 181 occurrences\n",
      "  Tunisia: 2 occurrences\n",
      "  UAE: 354 occurrences\n",
      "  Yemen: 366 occurrences\n",
      "\n",
      "For class group with 10 active labels:\n",
      "  Algeria: 3 occurrences\n",
      "  Bahrain: 159 occurrences\n",
      "  Egypt: 46 occurrences\n",
      "  Iraq: 128 occurrences\n",
      "  Jordan: 153 occurrences\n",
      "  Kuwait: 164 occurrences\n",
      "  Lebanon: 89 occurrences\n",
      "  Libya: 62 occurrences\n",
      "  Morocco: 3 occurrences\n",
      "  Oman: 108 occurrences\n",
      "  Palestine: 105 occurrences\n",
      "  Qatar: 156 occurrences\n",
      "  Saudi_Arabia: 166 occurrences\n",
      "  Sudan: 27 occurrences\n",
      "  Syria: 98 occurrences\n",
      "  Tunisia: 3 occurrences\n",
      "  UAE: 91 occurrences\n",
      "  Yemen: 119 occurrences\n",
      "\n",
      "For class group with 11 active labels:\n",
      "  Algeria: 5 occurrences\n",
      "  Bahrain: 134 occurrences\n",
      "  Egypt: 41 occurrences\n",
      "  Iraq: 108 occurrences\n",
      "  Jordan: 119 occurrences\n",
      "  Kuwait: 134 occurrences\n",
      "  Lebanon: 97 occurrences\n",
      "  Libya: 40 occurrences\n",
      "  Morocco: 5 occurrences\n",
      "  Oman: 110 occurrences\n",
      "  Palestine: 106 occurrences\n",
      "  Qatar: 134 occurrences\n",
      "  Saudi_Arabia: 136 occurrences\n",
      "  Sudan: 29 occurrences\n",
      "  Syria: 106 occurrences\n",
      "  Tunisia: 6 occurrences\n",
      "  UAE: 98 occurrences\n",
      "  Yemen: 99 occurrences\n",
      "\n",
      "For class group with 12 active labels:\n",
      "  Algeria: 7 occurrences\n",
      "  Bahrain: 332 occurrences\n",
      "  Egypt: 58 occurrences\n",
      "  Iraq: 308 occurrences\n",
      "  Jordan: 328 occurrences\n",
      "  Kuwait: 332 occurrences\n",
      "  Lebanon: 318 occurrences\n",
      "  Libya: 29 occurrences\n",
      "  Morocco: 7 occurrences\n",
      "  Oman: 324 occurrences\n",
      "  Palestine: 323 occurrences\n",
      "  Qatar: 331 occurrences\n",
      "  Saudi_Arabia: 334 occurrences\n",
      "  Sudan: 19 occurrences\n",
      "  Syria: 317 occurrences\n",
      "  Tunisia: 8 occurrences\n",
      "  UAE: 322 occurrences\n",
      "  Yemen: 311 occurrences\n",
      "\n",
      "For class group with 13 active labels:\n",
      "  Algeria: 0 occurrences\n",
      "  Bahrain: 273 occurrences\n",
      "  Egypt: 179 occurrences\n",
      "  Iraq: 258 occurrences\n",
      "  Jordan: 273 occurrences\n",
      "  Kuwait: 273 occurrences\n",
      "  Lebanon: 264 occurrences\n",
      "  Libya: 91 occurrences\n",
      "  Morocco: 0 occurrences\n",
      "  Oman: 272 occurrences\n",
      "  Palestine: 273 occurrences\n",
      "  Qatar: 273 occurrences\n",
      "  Saudi_Arabia: 273 occurrences\n",
      "  Sudan: 37 occurrences\n",
      "  Syria: 270 occurrences\n",
      "  Tunisia: 0 occurrences\n",
      "  UAE: 272 occurrences\n",
      "  Yemen: 268 occurrences\n",
      "\n",
      "For class group with 14 active labels:\n",
      "  Algeria: 1 occurrences\n",
      "  Bahrain: 256 occurrences\n",
      "  Egypt: 243 occurrences\n",
      "  Iraq: 252 occurrences\n",
      "  Jordan: 256 occurrences\n",
      "  Kuwait: 256 occurrences\n",
      "  Lebanon: 255 occurrences\n",
      "  Libya: 128 occurrences\n",
      "  Morocco: 0 occurrences\n",
      "  Oman: 256 occurrences\n",
      "  Palestine: 256 occurrences\n",
      "  Qatar: 256 occurrences\n",
      "  Saudi_Arabia: 256 occurrences\n",
      "  Sudan: 149 occurrences\n",
      "  Syria: 255 occurrences\n",
      "  Tunisia: 2 occurrences\n",
      "  UAE: 256 occurrences\n",
      "  Yemen: 251 occurrences\n",
      "\n",
      "For class group with 15 active labels:\n",
      "  Algeria: 4 occurrences\n",
      "  Bahrain: 532 occurrences\n",
      "  Egypt: 532 occurrences\n",
      "  Iraq: 531 occurrences\n",
      "  Jordan: 532 occurrences\n",
      "  Kuwait: 532 occurrences\n",
      "  Lebanon: 532 occurrences\n",
      "  Libya: 530 occurrences\n",
      "  Morocco: 1 occurrences\n",
      "  Oman: 532 occurrences\n",
      "  Palestine: 532 occurrences\n",
      "  Qatar: 532 occurrences\n",
      "  Saudi_Arabia: 532 occurrences\n",
      "  Sudan: 530 occurrences\n",
      "  Syria: 532 occurrences\n",
      "  Tunisia: 1 occurrences\n",
      "  UAE: 532 occurrences\n",
      "  Yemen: 531 occurrences\n",
      "\n",
      "For class group with 16 active labels:\n",
      "  Algeria: 9 occurrences\n",
      "  Bahrain: 11 occurrences\n",
      "  Egypt: 10 occurrences\n",
      "  Iraq: 11 occurrences\n",
      "  Jordan: 11 occurrences\n",
      "  Kuwait: 11 occurrences\n",
      "  Lebanon: 11 occurrences\n",
      "  Libya: 11 occurrences\n",
      "  Morocco: 1 occurrences\n",
      "  Oman: 11 occurrences\n",
      "  Palestine: 11 occurrences\n",
      "  Qatar: 11 occurrences\n",
      "  Saudi_Arabia: 11 occurrences\n",
      "  Sudan: 10 occurrences\n",
      "  Syria: 11 occurrences\n",
      "  Tunisia: 3 occurrences\n",
      "  UAE: 11 occurrences\n",
      "  Yemen: 11 occurrences\n",
      "\n",
      "For class group with 17 active labels:\n",
      "  Algeria: 26 occurrences\n",
      "  Bahrain: 26 occurrences\n",
      "  Egypt: 26 occurrences\n",
      "  Iraq: 25 occurrences\n",
      "  Jordan: 26 occurrences\n",
      "  Kuwait: 26 occurrences\n",
      "  Lebanon: 26 occurrences\n",
      "  Libya: 24 occurrences\n",
      "  Morocco: 5 occurrences\n",
      "  Oman: 26 occurrences\n",
      "  Palestine: 26 occurrences\n",
      "  Qatar: 26 occurrences\n",
      "  Saudi_Arabia: 25 occurrences\n",
      "  Sudan: 26 occurrences\n",
      "  Syria: 26 occurrences\n",
      "  Tunisia: 25 occurrences\n",
      "  UAE: 26 occurrences\n",
      "  Yemen: 26 occurrences\n",
      "\n",
      "For class group with 18 active labels:\n",
      "  Algeria: 502 occurrences\n",
      "  Bahrain: 502 occurrences\n",
      "  Egypt: 502 occurrences\n",
      "  Iraq: 502 occurrences\n",
      "  Jordan: 502 occurrences\n",
      "  Kuwait: 502 occurrences\n",
      "  Lebanon: 502 occurrences\n",
      "  Libya: 502 occurrences\n",
      "  Morocco: 502 occurrences\n",
      "  Oman: 502 occurrences\n",
      "  Palestine: 502 occurrences\n",
      "  Qatar: 502 occurrences\n",
      "  Saudi_Arabia: 502 occurrences\n",
      "  Sudan: 502 occurrences\n",
      "  Syria: 502 occurrences\n",
      "  Tunisia: 502 occurrences\n",
      "  UAE: 502 occurrences\n",
      "  Yemen: 502 occurrences\n"
     ]
    }
   ],
   "source": [
    "get_class_insight(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = modify_dataset_for_active_labels(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts of rows by number of active labels:\n",
      "Class with 0 active labels: 0\n",
      "Class with 1 active labels: 41837\n",
      "Class with 2 active labels: 3220\n",
      "Class with 3 active labels: 2329\n",
      "Class with 4 active labels: 2580\n",
      "Class with 5 active labels: 1416\n",
      "Class with 6 active labels: 1709\n",
      "Class with 7 active labels: 1794\n",
      "Class with 8 active labels: 1108\n",
      "Class with 9 active labels: 536\n",
      "Class with 10 active labels: 168\n",
      "Class with 11 active labels: 137\n",
      "Class with 12 active labels: 334\n",
      "Class with 13 active labels: 273\n",
      "Class with 14 active labels: 256\n",
      "Class with 15 active labels: 532\n",
      "Class with 16 active labels: 0\n",
      "Class with 17 active labels: 0\n",
      "Class with 18 active labels: 539\n",
      "\n",
      "For class group with 1 active labels:\n",
      "  Algeria: 2712 occurrences\n",
      "  Bahrain: 417 occurrences\n",
      "  Egypt: 10609 occurrences\n",
      "  Iraq: 6026 occurrences\n",
      "  Jordan: 762 occurrences\n",
      "  Kuwait: 672 occurrences\n",
      "  Lebanon: 1245 occurrences\n",
      "  Libya: 2567 occurrences\n",
      "  Morocco: 2213 occurrences\n",
      "  Oman: 2077 occurrences\n",
      "  Palestine: 794 occurrences\n",
      "  Qatar: 451 occurrences\n",
      "  Saudi_Arabia: 4176 occurrences\n",
      "  Sudan: 858 occurrences\n",
      "  Syria: 1896 occurrences\n",
      "  Tunisia: 1616 occurrences\n",
      "  UAE: 1418 occurrences\n",
      "  Yemen: 1328 occurrences\n",
      "\n",
      "For class group with 2 active labels:\n",
      "  Algeria: 370 occurrences\n",
      "  Bahrain: 54 occurrences\n",
      "  Egypt: 557 occurrences\n",
      "  Iraq: 816 occurrences\n",
      "  Jordan: 243 occurrences\n",
      "  Kuwait: 136 occurrences\n",
      "  Lebanon: 384 occurrences\n",
      "  Libya: 731 occurrences\n",
      "  Morocco: 352 occurrences\n",
      "  Oman: 63 occurrences\n",
      "  Palestine: 215 occurrences\n",
      "  Qatar: 45 occurrences\n",
      "  Saudi_Arabia: 857 occurrences\n",
      "  Sudan: 174 occurrences\n",
      "  Syria: 339 occurrences\n",
      "  Tunisia: 826 occurrences\n",
      "  UAE: 114 occurrences\n",
      "  Yemen: 164 occurrences\n",
      "\n",
      "For class group with 3 active labels:\n",
      "  Algeria: 1262 occurrences\n",
      "  Bahrain: 89 occurrences\n",
      "  Egypt: 235 occurrences\n",
      "  Iraq: 402 occurrences\n",
      "  Jordan: 386 occurrences\n",
      "  Kuwait: 223 occurrences\n",
      "  Lebanon: 317 occurrences\n",
      "  Libya: 378 occurrences\n",
      "  Morocco: 1134 occurrences\n",
      "  Oman: 25 occurrences\n",
      "  Palestine: 300 occurrences\n",
      "  Qatar: 80 occurrences\n",
      "  Saudi_Arabia: 414 occurrences\n",
      "  Sudan: 103 occurrences\n",
      "  Syria: 188 occurrences\n",
      "  Tunisia: 1340 occurrences\n",
      "  UAE: 43 occurrences\n",
      "  Yemen: 68 occurrences\n",
      "\n",
      "For class group with 4 active labels:\n",
      "  Algeria: 280 occurrences\n",
      "  Bahrain: 436 occurrences\n",
      "  Egypt: 111 occurrences\n",
      "  Iraq: 263 occurrences\n",
      "  Jordan: 1737 occurrences\n",
      "  Kuwait: 480 occurrences\n",
      "  Lebanon: 1670 occurrences\n",
      "  Libya: 320 occurrences\n",
      "  Morocco: 269 occurrences\n",
      "  Oman: 26 occurrences\n",
      "  Palestine: 1691 occurrences\n",
      "  Qatar: 425 occurrences\n",
      "  Saudi_Arabia: 573 occurrences\n",
      "  Sudan: 39 occurrences\n",
      "  Syria: 1629 occurrences\n",
      "  Tunisia: 287 occurrences\n",
      "  UAE: 40 occurrences\n",
      "  Yemen: 44 occurrences\n",
      "\n",
      "For class group with 5 active labels:\n",
      "  Algeria: 53 occurrences\n",
      "  Bahrain: 660 occurrences\n",
      "  Egypt: 116 occurrences\n",
      "  Iraq: 977 occurrences\n",
      "  Jordan: 671 occurrences\n",
      "  Kuwait: 694 occurrences\n",
      "  Lebanon: 618 occurrences\n",
      "  Libya: 98 occurrences\n",
      "  Morocco: 49 occurrences\n",
      "  Oman: 131 occurrences\n",
      "  Palestine: 644 occurrences\n",
      "  Qatar: 684 occurrences\n",
      "  Saudi_Arabia: 740 occurrences\n",
      "  Sudan: 23 occurrences\n",
      "  Syria: 628 occurrences\n",
      "  Tunisia: 52 occurrences\n",
      "  UAE: 150 occurrences\n",
      "  Yemen: 92 occurrences\n",
      "\n",
      "For class group with 6 active labels:\n",
      "  Algeria: 6 occurrences\n",
      "  Bahrain: 1402 occurrences\n",
      "  Egypt: 80 occurrences\n",
      "  Iraq: 453 occurrences\n",
      "  Jordan: 351 occurrences\n",
      "  Kuwait: 1427 occurrences\n",
      "  Lebanon: 276 occurrences\n",
      "  Libya: 95 occurrences\n",
      "  Morocco: 9 occurrences\n",
      "  Oman: 1205 occurrences\n",
      "  Palestine: 300 occurrences\n",
      "  Qatar: 1403 occurrences\n",
      "  Saudi_Arabia: 1616 occurrences\n",
      "  Sudan: 18 occurrences\n",
      "  Syria: 284 occurrences\n",
      "  Tunisia: 5 occurrences\n",
      "  UAE: 1133 occurrences\n",
      "  Yemen: 191 occurrences\n",
      "\n",
      "For class group with 7 active labels:\n",
      "  Algeria: 7 occurrences\n",
      "  Bahrain: 1654 occurrences\n",
      "  Egypt: 100 occurrences\n",
      "  Iraq: 528 occurrences\n",
      "  Jordan: 220 occurrences\n",
      "  Kuwait: 1684 occurrences\n",
      "  Lebanon: 143 occurrences\n",
      "  Libya: 79 occurrences\n",
      "  Morocco: 6 occurrences\n",
      "  Oman: 1574 occurrences\n",
      "  Palestine: 160 occurrences\n",
      "  Qatar: 1646 occurrences\n",
      "  Saudi_Arabia: 1759 occurrences\n",
      "  Sudan: 27 occurrences\n",
      "  Syria: 153 occurrences\n",
      "  Tunisia: 8 occurrences\n",
      "  UAE: 1527 occurrences\n",
      "  Yemen: 1283 occurrences\n",
      "\n",
      "For class group with 8 active labels:\n",
      "  Algeria: 4 occurrences\n",
      "  Bahrain: 1052 occurrences\n",
      "  Egypt: 84 occurrences\n",
      "  Iraq: 891 occurrences\n",
      "  Jordan: 259 occurrences\n",
      "  Kuwait: 1066 occurrences\n",
      "  Lebanon: 116 occurrences\n",
      "  Libya: 100 occurrences\n",
      "  Morocco: 5 occurrences\n",
      "  Oman: 974 occurrences\n",
      "  Palestine: 125 occurrences\n",
      "  Qatar: 1037 occurrences\n",
      "  Saudi_Arabia: 1099 occurrences\n",
      "  Sudan: 26 occurrences\n",
      "  Syria: 117 occurrences\n",
      "  Tunisia: 5 occurrences\n",
      "  UAE: 954 occurrences\n",
      "  Yemen: 950 occurrences\n",
      "\n",
      "For class group with 9 active labels:\n",
      "  Algeria: 4 occurrences\n",
      "  Bahrain: 509 occurrences\n",
      "  Egypt: 66 occurrences\n",
      "  Iraq: 488 occurrences\n",
      "  Jordan: 328 occurrences\n",
      "  Kuwait: 516 occurrences\n",
      "  Lebanon: 176 occurrences\n",
      "  Libya: 206 occurrences\n",
      "  Morocco: 6 occurrences\n",
      "  Oman: 362 occurrences\n",
      "  Palestine: 188 occurrences\n",
      "  Qatar: 501 occurrences\n",
      "  Saudi_Arabia: 534 occurrences\n",
      "  Sudan: 37 occurrences\n",
      "  Syria: 181 occurrences\n",
      "  Tunisia: 2 occurrences\n",
      "  UAE: 354 occurrences\n",
      "  Yemen: 366 occurrences\n",
      "\n",
      "For class group with 10 active labels:\n",
      "  Algeria: 3 occurrences\n",
      "  Bahrain: 159 occurrences\n",
      "  Egypt: 46 occurrences\n",
      "  Iraq: 128 occurrences\n",
      "  Jordan: 153 occurrences\n",
      "  Kuwait: 164 occurrences\n",
      "  Lebanon: 89 occurrences\n",
      "  Libya: 62 occurrences\n",
      "  Morocco: 3 occurrences\n",
      "  Oman: 108 occurrences\n",
      "  Palestine: 105 occurrences\n",
      "  Qatar: 156 occurrences\n",
      "  Saudi_Arabia: 166 occurrences\n",
      "  Sudan: 27 occurrences\n",
      "  Syria: 98 occurrences\n",
      "  Tunisia: 3 occurrences\n",
      "  UAE: 91 occurrences\n",
      "  Yemen: 119 occurrences\n",
      "\n",
      "For class group with 11 active labels:\n",
      "  Algeria: 5 occurrences\n",
      "  Bahrain: 134 occurrences\n",
      "  Egypt: 41 occurrences\n",
      "  Iraq: 108 occurrences\n",
      "  Jordan: 119 occurrences\n",
      "  Kuwait: 134 occurrences\n",
      "  Lebanon: 97 occurrences\n",
      "  Libya: 40 occurrences\n",
      "  Morocco: 5 occurrences\n",
      "  Oman: 110 occurrences\n",
      "  Palestine: 106 occurrences\n",
      "  Qatar: 134 occurrences\n",
      "  Saudi_Arabia: 136 occurrences\n",
      "  Sudan: 29 occurrences\n",
      "  Syria: 106 occurrences\n",
      "  Tunisia: 6 occurrences\n",
      "  UAE: 98 occurrences\n",
      "  Yemen: 99 occurrences\n",
      "\n",
      "For class group with 12 active labels:\n",
      "  Algeria: 7 occurrences\n",
      "  Bahrain: 332 occurrences\n",
      "  Egypt: 58 occurrences\n",
      "  Iraq: 308 occurrences\n",
      "  Jordan: 328 occurrences\n",
      "  Kuwait: 332 occurrences\n",
      "  Lebanon: 318 occurrences\n",
      "  Libya: 29 occurrences\n",
      "  Morocco: 7 occurrences\n",
      "  Oman: 324 occurrences\n",
      "  Palestine: 323 occurrences\n",
      "  Qatar: 331 occurrences\n",
      "  Saudi_Arabia: 334 occurrences\n",
      "  Sudan: 19 occurrences\n",
      "  Syria: 317 occurrences\n",
      "  Tunisia: 8 occurrences\n",
      "  UAE: 322 occurrences\n",
      "  Yemen: 311 occurrences\n",
      "\n",
      "For class group with 13 active labels:\n",
      "  Algeria: 0 occurrences\n",
      "  Bahrain: 273 occurrences\n",
      "  Egypt: 179 occurrences\n",
      "  Iraq: 258 occurrences\n",
      "  Jordan: 273 occurrences\n",
      "  Kuwait: 273 occurrences\n",
      "  Lebanon: 264 occurrences\n",
      "  Libya: 91 occurrences\n",
      "  Morocco: 0 occurrences\n",
      "  Oman: 272 occurrences\n",
      "  Palestine: 273 occurrences\n",
      "  Qatar: 273 occurrences\n",
      "  Saudi_Arabia: 273 occurrences\n",
      "  Sudan: 37 occurrences\n",
      "  Syria: 270 occurrences\n",
      "  Tunisia: 0 occurrences\n",
      "  UAE: 272 occurrences\n",
      "  Yemen: 268 occurrences\n",
      "\n",
      "For class group with 14 active labels:\n",
      "  Algeria: 1 occurrences\n",
      "  Bahrain: 256 occurrences\n",
      "  Egypt: 243 occurrences\n",
      "  Iraq: 252 occurrences\n",
      "  Jordan: 256 occurrences\n",
      "  Kuwait: 256 occurrences\n",
      "  Lebanon: 255 occurrences\n",
      "  Libya: 128 occurrences\n",
      "  Morocco: 0 occurrences\n",
      "  Oman: 256 occurrences\n",
      "  Palestine: 256 occurrences\n",
      "  Qatar: 256 occurrences\n",
      "  Saudi_Arabia: 256 occurrences\n",
      "  Sudan: 149 occurrences\n",
      "  Syria: 255 occurrences\n",
      "  Tunisia: 2 occurrences\n",
      "  UAE: 256 occurrences\n",
      "  Yemen: 251 occurrences\n",
      "\n",
      "For class group with 15 active labels:\n",
      "  Algeria: 4 occurrences\n",
      "  Bahrain: 532 occurrences\n",
      "  Egypt: 532 occurrences\n",
      "  Iraq: 531 occurrences\n",
      "  Jordan: 532 occurrences\n",
      "  Kuwait: 532 occurrences\n",
      "  Lebanon: 532 occurrences\n",
      "  Libya: 530 occurrences\n",
      "  Morocco: 1 occurrences\n",
      "  Oman: 532 occurrences\n",
      "  Palestine: 532 occurrences\n",
      "  Qatar: 532 occurrences\n",
      "  Saudi_Arabia: 532 occurrences\n",
      "  Sudan: 530 occurrences\n",
      "  Syria: 532 occurrences\n",
      "  Tunisia: 1 occurrences\n",
      "  UAE: 532 occurrences\n",
      "  Yemen: 531 occurrences\n",
      "\n",
      "For class group with 16 active labels:\n",
      "  Algeria: 0 occurrences\n",
      "  Bahrain: 0 occurrences\n",
      "  Egypt: 0 occurrences\n",
      "  Iraq: 0 occurrences\n",
      "  Jordan: 0 occurrences\n",
      "  Kuwait: 0 occurrences\n",
      "  Lebanon: 0 occurrences\n",
      "  Libya: 0 occurrences\n",
      "  Morocco: 0 occurrences\n",
      "  Oman: 0 occurrences\n",
      "  Palestine: 0 occurrences\n",
      "  Qatar: 0 occurrences\n",
      "  Saudi_Arabia: 0 occurrences\n",
      "  Sudan: 0 occurrences\n",
      "  Syria: 0 occurrences\n",
      "  Tunisia: 0 occurrences\n",
      "  UAE: 0 occurrences\n",
      "  Yemen: 0 occurrences\n",
      "\n",
      "For class group with 17 active labels:\n",
      "  Algeria: 0 occurrences\n",
      "  Bahrain: 0 occurrences\n",
      "  Egypt: 0 occurrences\n",
      "  Iraq: 0 occurrences\n",
      "  Jordan: 0 occurrences\n",
      "  Kuwait: 0 occurrences\n",
      "  Lebanon: 0 occurrences\n",
      "  Libya: 0 occurrences\n",
      "  Morocco: 0 occurrences\n",
      "  Oman: 0 occurrences\n",
      "  Palestine: 0 occurrences\n",
      "  Qatar: 0 occurrences\n",
      "  Saudi_Arabia: 0 occurrences\n",
      "  Sudan: 0 occurrences\n",
      "  Syria: 0 occurrences\n",
      "  Tunisia: 0 occurrences\n",
      "  UAE: 0 occurrences\n",
      "  Yemen: 0 occurrences\n",
      "\n",
      "For class group with 18 active labels:\n",
      "  Algeria: 539 occurrences\n",
      "  Bahrain: 539 occurrences\n",
      "  Egypt: 539 occurrences\n",
      "  Iraq: 539 occurrences\n",
      "  Jordan: 539 occurrences\n",
      "  Kuwait: 539 occurrences\n",
      "  Lebanon: 539 occurrences\n",
      "  Libya: 539 occurrences\n",
      "  Morocco: 539 occurrences\n",
      "  Oman: 539 occurrences\n",
      "  Palestine: 539 occurrences\n",
      "  Qatar: 539 occurrences\n",
      "  Saudi_Arabia: 539 occurrences\n",
      "  Sudan: 539 occurrences\n",
      "  Syria: 539 occurrences\n",
      "  Tunisia: 539 occurrences\n",
      "  UAE: 539 occurrences\n",
      "  Yemen: 539 occurrences\n"
     ]
    }
   ],
   "source": [
    "get_class_insight(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = split_balanced_threshold(df, label_columns, threshold=500, uni_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts of rows by number of active labels:\n",
      "Class with 0 active labels: 0\n",
      "Class with 1 active labels: 594\n",
      "Class with 2 active labels: 500\n",
      "Class with 3 active labels: 500\n",
      "Class with 4 active labels: 500\n",
      "Class with 5 active labels: 500\n",
      "Class with 6 active labels: 500\n",
      "Class with 7 active labels: 500\n",
      "Class with 8 active labels: 500\n",
      "Class with 9 active labels: 500\n",
      "Class with 10 active labels: 168\n",
      "Class with 11 active labels: 137\n",
      "Class with 12 active labels: 334\n",
      "Class with 13 active labels: 273\n",
      "Class with 14 active labels: 256\n",
      "Class with 15 active labels: 500\n",
      "Class with 16 active labels: 0\n",
      "Class with 17 active labels: 0\n",
      "Class with 18 active labels: 500\n",
      "\n",
      "For class group with 1 active labels:\n",
      "  Algeria: 33 occurrences\n",
      "  Bahrain: 33 occurrences\n",
      "  Egypt: 33 occurrences\n",
      "  Iraq: 33 occurrences\n",
      "  Jordan: 33 occurrences\n",
      "  Kuwait: 33 occurrences\n",
      "  Lebanon: 33 occurrences\n",
      "  Libya: 33 occurrences\n",
      "  Morocco: 33 occurrences\n",
      "  Oman: 33 occurrences\n",
      "  Palestine: 33 occurrences\n",
      "  Qatar: 33 occurrences\n",
      "  Saudi_Arabia: 33 occurrences\n",
      "  Sudan: 33 occurrences\n",
      "  Syria: 33 occurrences\n",
      "  Tunisia: 33 occurrences\n",
      "  UAE: 33 occurrences\n",
      "  Yemen: 33 occurrences\n",
      "\n",
      "For class group with 2 active labels:\n",
      "  Algeria: 57 occurrences\n",
      "  Bahrain: 7 occurrences\n",
      "  Egypt: 88 occurrences\n",
      "  Iraq: 117 occurrences\n",
      "  Jordan: 40 occurrences\n",
      "  Kuwait: 12 occurrences\n",
      "  Lebanon: 51 occurrences\n",
      "  Libya: 114 occurrences\n",
      "  Morocco: 56 occurrences\n",
      "  Oman: 13 occurrences\n",
      "  Palestine: 44 occurrences\n",
      "  Qatar: 8 occurrences\n",
      "  Saudi_Arabia: 136 occurrences\n",
      "  Sudan: 29 occurrences\n",
      "  Syria: 48 occurrences\n",
      "  Tunisia: 136 occurrences\n",
      "  UAE: 22 occurrences\n",
      "  Yemen: 22 occurrences\n",
      "\n",
      "For class group with 3 active labels:\n",
      "  Algeria: 267 occurrences\n",
      "  Bahrain: 18 occurrences\n",
      "  Egypt: 58 occurrences\n",
      "  Iraq: 92 occurrences\n",
      "  Jordan: 80 occurrences\n",
      "  Kuwait: 41 occurrences\n",
      "  Lebanon: 65 occurrences\n",
      "  Libya: 90 occurrences\n",
      "  Morocco: 235 occurrences\n",
      "  Oman: 5 occurrences\n",
      "  Palestine: 65 occurrences\n",
      "  Qatar: 14 occurrences\n",
      "  Saudi_Arabia: 93 occurrences\n",
      "  Sudan: 24 occurrences\n",
      "  Syria: 37 occurrences\n",
      "  Tunisia: 289 occurrences\n",
      "  UAE: 8 occurrences\n",
      "  Yemen: 19 occurrences\n",
      "\n",
      "For class group with 4 active labels:\n",
      "  Algeria: 53 occurrences\n",
      "  Bahrain: 75 occurrences\n",
      "  Egypt: 29 occurrences\n",
      "  Iraq: 39 occurrences\n",
      "  Jordan: 346 occurrences\n",
      "  Kuwait: 84 occurrences\n",
      "  Lebanon: 332 occurrences\n",
      "  Libya: 63 occurrences\n",
      "  Morocco: 53 occurrences\n",
      "  Oman: 7 occurrences\n",
      "  Palestine: 339 occurrences\n",
      "  Qatar: 75 occurrences\n",
      "  Saudi_Arabia: 101 occurrences\n",
      "  Sudan: 8 occurrences\n",
      "  Syria: 325 occurrences\n",
      "  Tunisia: 56 occurrences\n",
      "  UAE: 7 occurrences\n",
      "  Yemen: 8 occurrences\n",
      "\n",
      "For class group with 5 active labels:\n",
      "  Algeria: 21 occurrences\n",
      "  Bahrain: 214 occurrences\n",
      "  Egypt: 39 occurrences\n",
      "  Iraq: 368 occurrences\n",
      "  Jordan: 250 occurrences\n",
      "  Kuwait: 224 occurrences\n",
      "  Lebanon: 237 occurrences\n",
      "  Libya: 38 occurrences\n",
      "  Morocco: 20 occurrences\n",
      "  Oman: 44 occurrences\n",
      "  Palestine: 242 occurrences\n",
      "  Qatar: 219 occurrences\n",
      "  Saudi_Arabia: 245 occurrences\n",
      "  Sudan: 7 occurrences\n",
      "  Syria: 242 occurrences\n",
      "  Tunisia: 23 occurrences\n",
      "  UAE: 42 occurrences\n",
      "  Yemen: 25 occurrences\n",
      "\n",
      "For class group with 6 active labels:\n",
      "  Algeria: 1 occurrences\n",
      "  Bahrain: 424 occurrences\n",
      "  Egypt: 18 occurrences\n",
      "  Iraq: 121 occurrences\n",
      "  Jordan: 84 occurrences\n",
      "  Kuwait: 436 occurrences\n",
      "  Lebanon: 65 occurrences\n",
      "  Libya: 26 occurrences\n",
      "  Morocco: 2 occurrences\n",
      "  Oman: 367 occurrences\n",
      "  Palestine: 75 occurrences\n",
      "  Qatar: 426 occurrences\n",
      "  Saudi_Arabia: 483 occurrences\n",
      "  Sudan: 3 occurrences\n",
      "  Syria: 68 occurrences\n",
      "  Tunisia: 1 occurrences\n",
      "  UAE: 345 occurrences\n",
      "  Yemen: 55 occurrences\n",
      "\n",
      "For class group with 7 active labels:\n",
      "  Algeria: 1 occurrences\n",
      "  Bahrain: 449 occurrences\n",
      "  Egypt: 35 occurrences\n",
      "  Iraq: 145 occurrences\n",
      "  Jordan: 72 occurrences\n",
      "  Kuwait: 460 occurrences\n",
      "  Lebanon: 53 occurrences\n",
      "  Libya: 31 occurrences\n",
      "  Morocco: 0 occurrences\n",
      "  Oman: 427 occurrences\n",
      "  Palestine: 57 occurrences\n",
      "  Qatar: 446 occurrences\n",
      "  Saudi_Arabia: 484 occurrences\n",
      "  Sudan: 9 occurrences\n",
      "  Syria: 58 occurrences\n",
      "  Tunisia: 0 occurrences\n",
      "  UAE: 423 occurrences\n",
      "  Yemen: 350 occurrences\n",
      "\n",
      "For class group with 8 active labels:\n",
      "  Algeria: 2 occurrences\n",
      "  Bahrain: 479 occurrences\n",
      "  Egypt: 31 occurrences\n",
      "  Iraq: 402 occurrences\n",
      "  Jordan: 117 occurrences\n",
      "  Kuwait: 487 occurrences\n",
      "  Lebanon: 55 occurrences\n",
      "  Libya: 36 occurrences\n",
      "  Morocco: 2 occurrences\n",
      "  Oman: 438 occurrences\n",
      "  Palestine: 59 occurrences\n",
      "  Qatar: 471 occurrences\n",
      "  Saudi_Arabia: 497 occurrences\n",
      "  Sudan: 7 occurrences\n",
      "  Syria: 54 occurrences\n",
      "  Tunisia: 3 occurrences\n",
      "  UAE: 430 occurrences\n",
      "  Yemen: 430 occurrences\n",
      "\n",
      "For class group with 9 active labels:\n",
      "  Algeria: 4 occurrences\n",
      "  Bahrain: 475 occurrences\n",
      "  Egypt: 60 occurrences\n",
      "  Iraq: 457 occurrences\n",
      "  Jordan: 307 occurrences\n",
      "  Kuwait: 482 occurrences\n",
      "  Lebanon: 163 occurrences\n",
      "  Libya: 191 occurrences\n",
      "  Morocco: 6 occurrences\n",
      "  Oman: 338 occurrences\n",
      "  Palestine: 175 occurrences\n",
      "  Qatar: 467 occurrences\n",
      "  Saudi_Arabia: 498 occurrences\n",
      "  Sudan: 34 occurrences\n",
      "  Syria: 168 occurrences\n",
      "  Tunisia: 2 occurrences\n",
      "  UAE: 330 occurrences\n",
      "  Yemen: 343 occurrences\n",
      "\n",
      "For class group with 10 active labels:\n",
      "  Algeria: 3 occurrences\n",
      "  Bahrain: 159 occurrences\n",
      "  Egypt: 46 occurrences\n",
      "  Iraq: 128 occurrences\n",
      "  Jordan: 153 occurrences\n",
      "  Kuwait: 164 occurrences\n",
      "  Lebanon: 89 occurrences\n",
      "  Libya: 62 occurrences\n",
      "  Morocco: 3 occurrences\n",
      "  Oman: 108 occurrences\n",
      "  Palestine: 105 occurrences\n",
      "  Qatar: 156 occurrences\n",
      "  Saudi_Arabia: 166 occurrences\n",
      "  Sudan: 27 occurrences\n",
      "  Syria: 98 occurrences\n",
      "  Tunisia: 3 occurrences\n",
      "  UAE: 91 occurrences\n",
      "  Yemen: 119 occurrences\n",
      "\n",
      "For class group with 11 active labels:\n",
      "  Algeria: 5 occurrences\n",
      "  Bahrain: 134 occurrences\n",
      "  Egypt: 41 occurrences\n",
      "  Iraq: 108 occurrences\n",
      "  Jordan: 119 occurrences\n",
      "  Kuwait: 134 occurrences\n",
      "  Lebanon: 97 occurrences\n",
      "  Libya: 40 occurrences\n",
      "  Morocco: 5 occurrences\n",
      "  Oman: 110 occurrences\n",
      "  Palestine: 106 occurrences\n",
      "  Qatar: 134 occurrences\n",
      "  Saudi_Arabia: 136 occurrences\n",
      "  Sudan: 29 occurrences\n",
      "  Syria: 106 occurrences\n",
      "  Tunisia: 6 occurrences\n",
      "  UAE: 98 occurrences\n",
      "  Yemen: 99 occurrences\n",
      "\n",
      "For class group with 12 active labels:\n",
      "  Algeria: 7 occurrences\n",
      "  Bahrain: 332 occurrences\n",
      "  Egypt: 58 occurrences\n",
      "  Iraq: 308 occurrences\n",
      "  Jordan: 328 occurrences\n",
      "  Kuwait: 332 occurrences\n",
      "  Lebanon: 318 occurrences\n",
      "  Libya: 29 occurrences\n",
      "  Morocco: 7 occurrences\n",
      "  Oman: 324 occurrences\n",
      "  Palestine: 323 occurrences\n",
      "  Qatar: 331 occurrences\n",
      "  Saudi_Arabia: 334 occurrences\n",
      "  Sudan: 19 occurrences\n",
      "  Syria: 317 occurrences\n",
      "  Tunisia: 8 occurrences\n",
      "  UAE: 322 occurrences\n",
      "  Yemen: 311 occurrences\n",
      "\n",
      "For class group with 13 active labels:\n",
      "  Algeria: 0 occurrences\n",
      "  Bahrain: 273 occurrences\n",
      "  Egypt: 179 occurrences\n",
      "  Iraq: 258 occurrences\n",
      "  Jordan: 273 occurrences\n",
      "  Kuwait: 273 occurrences\n",
      "  Lebanon: 264 occurrences\n",
      "  Libya: 91 occurrences\n",
      "  Morocco: 0 occurrences\n",
      "  Oman: 272 occurrences\n",
      "  Palestine: 273 occurrences\n",
      "  Qatar: 273 occurrences\n",
      "  Saudi_Arabia: 273 occurrences\n",
      "  Sudan: 37 occurrences\n",
      "  Syria: 270 occurrences\n",
      "  Tunisia: 0 occurrences\n",
      "  UAE: 272 occurrences\n",
      "  Yemen: 268 occurrences\n",
      "\n",
      "For class group with 14 active labels:\n",
      "  Algeria: 1 occurrences\n",
      "  Bahrain: 256 occurrences\n",
      "  Egypt: 243 occurrences\n",
      "  Iraq: 252 occurrences\n",
      "  Jordan: 256 occurrences\n",
      "  Kuwait: 256 occurrences\n",
      "  Lebanon: 255 occurrences\n",
      "  Libya: 128 occurrences\n",
      "  Morocco: 0 occurrences\n",
      "  Oman: 256 occurrences\n",
      "  Palestine: 256 occurrences\n",
      "  Qatar: 256 occurrences\n",
      "  Saudi_Arabia: 256 occurrences\n",
      "  Sudan: 149 occurrences\n",
      "  Syria: 255 occurrences\n",
      "  Tunisia: 2 occurrences\n",
      "  UAE: 256 occurrences\n",
      "  Yemen: 251 occurrences\n",
      "\n",
      "For class group with 15 active labels:\n",
      "  Algeria: 4 occurrences\n",
      "  Bahrain: 500 occurrences\n",
      "  Egypt: 500 occurrences\n",
      "  Iraq: 499 occurrences\n",
      "  Jordan: 500 occurrences\n",
      "  Kuwait: 500 occurrences\n",
      "  Lebanon: 500 occurrences\n",
      "  Libya: 498 occurrences\n",
      "  Morocco: 1 occurrences\n",
      "  Oman: 500 occurrences\n",
      "  Palestine: 500 occurrences\n",
      "  Qatar: 500 occurrences\n",
      "  Saudi_Arabia: 500 occurrences\n",
      "  Sudan: 498 occurrences\n",
      "  Syria: 500 occurrences\n",
      "  Tunisia: 1 occurrences\n",
      "  UAE: 500 occurrences\n",
      "  Yemen: 499 occurrences\n",
      "\n",
      "For class group with 16 active labels:\n",
      "  Algeria: 0 occurrences\n",
      "  Bahrain: 0 occurrences\n",
      "  Egypt: 0 occurrences\n",
      "  Iraq: 0 occurrences\n",
      "  Jordan: 0 occurrences\n",
      "  Kuwait: 0 occurrences\n",
      "  Lebanon: 0 occurrences\n",
      "  Libya: 0 occurrences\n",
      "  Morocco: 0 occurrences\n",
      "  Oman: 0 occurrences\n",
      "  Palestine: 0 occurrences\n",
      "  Qatar: 0 occurrences\n",
      "  Saudi_Arabia: 0 occurrences\n",
      "  Sudan: 0 occurrences\n",
      "  Syria: 0 occurrences\n",
      "  Tunisia: 0 occurrences\n",
      "  UAE: 0 occurrences\n",
      "  Yemen: 0 occurrences\n",
      "\n",
      "For class group with 17 active labels:\n",
      "  Algeria: 0 occurrences\n",
      "  Bahrain: 0 occurrences\n",
      "  Egypt: 0 occurrences\n",
      "  Iraq: 0 occurrences\n",
      "  Jordan: 0 occurrences\n",
      "  Kuwait: 0 occurrences\n",
      "  Lebanon: 0 occurrences\n",
      "  Libya: 0 occurrences\n",
      "  Morocco: 0 occurrences\n",
      "  Oman: 0 occurrences\n",
      "  Palestine: 0 occurrences\n",
      "  Qatar: 0 occurrences\n",
      "  Saudi_Arabia: 0 occurrences\n",
      "  Sudan: 0 occurrences\n",
      "  Syria: 0 occurrences\n",
      "  Tunisia: 0 occurrences\n",
      "  UAE: 0 occurrences\n",
      "  Yemen: 0 occurrences\n",
      "\n",
      "For class group with 18 active labels:\n",
      "  Algeria: 500 occurrences\n",
      "  Bahrain: 500 occurrences\n",
      "  Egypt: 500 occurrences\n",
      "  Iraq: 500 occurrences\n",
      "  Jordan: 500 occurrences\n",
      "  Kuwait: 500 occurrences\n",
      "  Lebanon: 500 occurrences\n",
      "  Libya: 500 occurrences\n",
      "  Morocco: 500 occurrences\n",
      "  Oman: 500 occurrences\n",
      "  Palestine: 500 occurrences\n",
      "  Qatar: 500 occurrences\n",
      "  Saudi_Arabia: 500 occurrences\n",
      "  Sudan: 500 occurrences\n",
      "  Syria: 500 occurrences\n",
      "  Tunisia: 500 occurrences\n",
      "  UAE: 500 occurrences\n",
      "  Yemen: 500 occurrences\n"
     ]
    }
   ],
   "source": [
    "get_class_insight(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(directory + \" BALANCED_NEW_DATASET_500.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lara.hassan/.conda/envs/myenv/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "                                                 \n",
      " 50%|█████     | 374/748 [00:32<00:27, 13.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4429030120372772, 'eval_f1': 0.7262548011197187, 'eval_roc_auc': 0.7910349145094046, 'eval_accuracy': 0.15060240963855423, 'eval_runtime': 0.7421, 'eval_samples_per_second': 1342.187, 'eval_steps_per_second': 56.598, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 501/748 [00:43<00:19, 12.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4215, 'grad_norm': 4.750375270843506, 'learning_rate': 5e-05, 'epoch': 1.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "100%|██████████| 748/748 [01:05<00:00, 13.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4512268900871277, 'eval_f1': 0.7295721119192713, 'eval_roc_auc': 0.7940113323604024, 'eval_accuracy': 0.16967871485943775, 'eval_runtime': 0.7301, 'eval_samples_per_second': 1364.209, 'eval_steps_per_second': 57.527, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 748/748 [01:08<00:00, 10.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 68.2553, 'train_samples_per_second': 262.397, 'train_steps_per_second': 10.959, 'train_loss': 0.4062513361640155, 'epoch': 2.0}\n",
      "Subset Accuracy: 0.1917\n",
      "Hamming Loss: 0.2740\n",
      "Micro Precision: 0.6768\n",
      "Micro Recall: 0.5000\n",
      "Micro F1-Score: 0.5751\n",
      "Precision per label: [0.8        0.89655172 0.58823529 0.66666667 0.66666667 0.55813953\n",
      " 0.64705882 0.73809524]\n",
      "Recall per label: [0.34285714 0.66666667 0.6        0.5        0.28571429 0.52173913\n",
      " 0.52380952 0.52542373]\n",
      "F1-Score per label: [0.48       0.76470588 0.59405941 0.57142857 0.4        0.53932584\n",
      " 0.57894737 0.61386139]\n",
      "{np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(8)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1992904/525870965.py:81: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/our_data/ BALANCED_NEW_DATASET_750.csv\"\n",
    "dev_path = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/dev/NADI2024_subtask1_dev2.tsv\"\n",
    "labels = ['Algeria', 'Bahrain', 'Egypt', 'Iraq', 'Jordan', 'Kuwait',\n",
    "       'Lebanon', 'Libya', 'Morocco', 'Oman', 'Palestine', 'Qatar',\n",
    "       'Saudi_Arabia', 'Sudan', 'Syria', 'Tunisia', 'UAE', 'Yemen']\n",
    "trainer = BertTrainer(\n",
    "    training_dataset_path=dataset_path,\n",
    "    model_name=\"/home/lara.hassan/Documents/Cross-Country-Dialectal-Arabic-Identification/exp_8/camelbert_finetuned_epochs_2_eval_f1_0.7694_greater_threshold_0.3\",\n",
    "    labels=labels,\n",
    "    threshold=0.3,\n",
    "    exp_num=13\n",
    ")\n",
    "trainer.train(\n",
    "    num_train_epochs=2,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    "    per_device_train_batch_size=24,\n",
    "    per_device_eval_batch_size=24\n",
    ")\n",
    "trainer.evaluate(dev_path=dev_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OVERALL SCORES:\n",
      "MACRO AVERAGE PRECISION SCORE: 69.52 %\n",
      "MACRO AVERAGE RECALL SCORE: 49.58 %\n",
      "MACRO AVERAGE F1-SCORE: 56.78 %\n",
      "MACRO AVERAGE ACCURACY: 72.60 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "scorer_script = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/NADI2024-ST1-Scorer.py\"\n",
    "gold_file = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/sample_submission/NADI2024_subtask1_dev2_gold.txt\"\n",
    "predictions_file = \"/home/lara.hassan/Documents/Cross-Country-Dialectal-Arabic-Identification/exp_13/camelbert_finetuned_epochs_2_eval_f1_0.7296_greater_threshold_0.3/-home-lara.hassan-Documents-Cross-Country-Dialectal-Arabic-Identification-exp_8-camelbert_finetuned_epochs_2_eval_f1_0.7694_greater_threshold_0.3-experiment-13_predictions.txt\"\n",
    "!python3 \"{scorer_script}\" \"{gold_file}\" \"{predictions_file}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPERIMENT 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lara.hassan/.conda/envs/myenv/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      " 20%|█▉        | 253/1270 [00:19<01:17, 13.17it/s]\n",
      " 20%|██        | 254/1270 [00:20<01:17, 13.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.41958171129226685, 'eval_f1': 0.7857666313818092, 'eval_roc_auc': 0.7992212978015347, 'eval_accuracy': 0.17429837518463812, 'eval_runtime': 0.514, 'eval_samples_per_second': 1317.176, 'eval_steps_per_second': 56.423, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 501/1270 [00:41<00:58, 13.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3978, 'grad_norm': 2.486278533935547, 'learning_rate': 5e-05, 'epoch': 1.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 507/1270 [00:41<00:58, 13.04it/s]\n",
      " 40%|████      | 508/1270 [00:42<00:58, 13.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.43696141242980957, 'eval_f1': 0.7823587385019711, 'eval_roc_auc': 0.7964695693203278, 'eval_accuracy': 0.16248153618906944, 'eval_runtime': 0.4917, 'eval_samples_per_second': 1376.887, 'eval_steps_per_second': 58.98, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 761/1270 [01:04<00:40, 12.70it/s]\n",
      " 60%|██████    | 762/1270 [01:04<00:39, 12.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4539303481578827, 'eval_f1': 0.7873862732974118, 'eval_roc_auc': 0.8072853133004487, 'eval_accuracy': 0.16691285081240767, 'eval_runtime': 0.502, 'eval_samples_per_second': 1348.666, 'eval_steps_per_second': 57.772, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 1001/1270 [01:25<00:21, 12.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2821, 'grad_norm': 1.5508099794387817, 'learning_rate': 1.7532467532467535e-05, 'epoch': 3.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 1015/1270 [01:26<00:19, 12.87it/s]\n",
      " 80%|████████  | 1016/1270 [01:27<00:19, 12.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.486063688993454, 'eval_f1': 0.7885090785099309, 'eval_roc_auc': 0.8056512106004009, 'eval_accuracy': 0.17872968980797638, 'eval_runtime': 0.5283, 'eval_samples_per_second': 1281.507, 'eval_steps_per_second': 54.895, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1269/1270 [01:49<00:00, 13.10it/s]\n",
      "100%|██████████| 1270/1270 [01:52<00:00, 13.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4963809847831726, 'eval_f1': 0.7906378600823045, 'eval_roc_auc': 0.8081659234473502, 'eval_accuracy': 0.17725258493353027, 'eval_runtime': 0.5111, 'eval_samples_per_second': 1324.562, 'eval_steps_per_second': 56.739, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1270/1270 [01:55<00:00, 10.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 115.591, 'train_samples_per_second': 263.213, 'train_steps_per_second': 10.987, 'train_loss': 0.31099345590185934, 'epoch': 5.0}\n",
      "Subset Accuracy: 0.1583\n",
      "Hamming Loss: 0.2844\n",
      "Micro Precision: 0.6554\n",
      "Micro Recall: 0.4916\n",
      "Micro F1-Score: 0.5618\n",
      "Precision per label: [0.81818182 0.90322581 0.50909091 0.63265306 0.55       0.55813953\n",
      " 0.61538462 0.8       ]\n",
      "Recall per label: [0.25714286 0.71794872 0.56       0.484375   0.26190476 0.52173913\n",
      " 0.38095238 0.61016949]\n",
      "F1-Score per label: [0.39130435 0.8        0.53333333 0.54867257 0.35483871 0.53932584\n",
      " 0.47058824 0.69230769]\n",
      "{np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(8)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1992904/525870965.py:81: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/our_data/ BALANCED_NEW_DATASET_500.csv\"\n",
    "dev_path = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/dev/NADI2024_subtask1_dev2.tsv\"\n",
    "labels = ['Algeria', 'Bahrain', 'Egypt', 'Iraq', 'Jordan', 'Kuwait',\n",
    "       'Lebanon', 'Libya', 'Morocco', 'Oman', 'Palestine', 'Qatar',\n",
    "       'Saudi_Arabia', 'Sudan', 'Syria', 'Tunisia', 'UAE', 'Yemen']\n",
    "trainer = BertTrainer(\n",
    "    training_dataset_path=dataset_path,\n",
    "    model_name=\"/home/lara.hassan/Documents/Cross-Country-Dialectal-Arabic-Identification/exp_8/camelbert_finetuned_epochs_2_eval_f1_0.7694_greater_threshold_0.3\",\n",
    "    labels=labels,\n",
    "    threshold=0.3,\n",
    "    exp_num=14\n",
    ")\n",
    "trainer.train(\n",
    "    num_train_epochs=5,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    "    per_device_train_batch_size=24,\n",
    "    per_device_eval_batch_size=24\n",
    ")\n",
    "trainer.evaluate(dev_path=dev_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OVERALL SCORES:\n",
      "MACRO AVERAGE PRECISION SCORE: 67.33 %\n",
      "MACRO AVERAGE RECALL SCORE: 47.43 %\n",
      "MACRO AVERAGE F1-SCORE: 54.13 %\n",
      "MACRO AVERAGE ACCURACY: 71.56 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "scorer_script = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/NADI2024-ST1-Scorer.py\"\n",
    "gold_file = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/sample_submission/NADI2024_subtask1_dev2_gold.txt\"\n",
    "predictions_file = \"/home/lara.hassan/Documents/Cross-Country-Dialectal-Arabic-Identification/exp_14/camelbert_finetuned_epochs_5_eval_f1_0.7906_greater_threshold_0.3/-home-lara.hassan-Documents-Cross-Country-Dialectal-Arabic-Identification-exp_8-camelbert_finetuned_epochs_2_eval_f1_0.7694_greater_threshold_0.3-experiment-14_predictions.txt\"\n",
    "!python3 \"{scorer_script}\" \"{gold_file}\" \"{predictions_file}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPERIMENT 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lara.hassan/.conda/envs/myenv/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "  5%|▌         | 254/5080 [00:19<06:05, 13.19it/s]\n",
      "  5%|▌         | 254/5080 [00:20<06:05, 13.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.41958171129226685, 'eval_f1': 0.7857666313818092, 'eval_roc_auc': 0.7992212978015347, 'eval_accuracy': 0.17429837518463812, 'eval_runtime': 0.5048, 'eval_samples_per_second': 1340.993, 'eval_steps_per_second': 57.443, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 502/5080 [00:41<05:50, 13.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3978, 'grad_norm': 2.486278533935547, 'learning_rate': 5e-05, 'epoch': 1.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 508/5080 [00:42<05:39, 13.46it/s]\n",
      " 10%|█         | 508/5080 [00:42<05:39, 13.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4369806945323944, 'eval_f1': 0.7821660234830445, 'eval_roc_auc': 0.7962522410096933, 'eval_accuracy': 0.16100443131462333, 'eval_runtime': 0.507, 'eval_samples_per_second': 1335.188, 'eval_steps_per_second': 57.194, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 762/5080 [01:04<05:22, 13.40it/s]\n",
      " 15%|█▌        | 762/5080 [01:04<05:22, 13.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.46652302145957947, 'eval_f1': 0.7806145301442264, 'eval_roc_auc': 0.8020244655851325, 'eval_accuracy': 0.16395864106351551, 'eval_runtime': 0.4911, 'eval_samples_per_second': 1378.409, 'eval_steps_per_second': 59.046, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 1002/5080 [01:25<05:15, 12.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2916, 'grad_norm': 2.4166393280029297, 'learning_rate': 4.4541484716157205e-05, 'epoch': 3.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1016/5080 [01:26<05:00, 13.54it/s]\n",
      " 20%|██        | 1016/5080 [01:27<05:00, 13.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.498250812292099, 'eval_f1': 0.7837244639540774, 'eval_roc_auc': 0.8001604489455602, 'eval_accuracy': 0.17134416543574593, 'eval_runtime': 0.5226, 'eval_samples_per_second': 1295.344, 'eval_steps_per_second': 55.487, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1270/5080 [01:48<04:40, 13.61it/s]\n",
      " 25%|██▌       | 1270/5080 [01:49<04:40, 13.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.520680844783783, 'eval_f1': 0.7880685276980607, 'eval_roc_auc': 0.8067535362052624, 'eval_accuracy': 0.1757754800590842, 'eval_runtime': 0.5062, 'eval_samples_per_second': 1337.525, 'eval_steps_per_second': 57.294, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 1502/5080 [02:09<04:34, 13.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2098, 'grad_norm': 1.620110034942627, 'learning_rate': 3.9082969432314415e-05, 'epoch': 5.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 1524/5080 [02:10<04:24, 13.46it/s]\n",
      " 30%|███       | 1524/5080 [02:11<04:24, 13.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.554932713508606, 'eval_f1': 0.7831502465184672, 'eval_roc_auc': 0.8015938122357977, 'eval_accuracy': 0.15952732644017725, 'eval_runtime': 0.4949, 'eval_samples_per_second': 1367.842, 'eval_steps_per_second': 58.593, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 1778/5080 [02:33<04:04, 13.50it/s]\n",
      " 35%|███▌      | 1778/5080 [02:33<04:04, 13.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5746973752975464, 'eval_f1': 0.7826010852441799, 'eval_roc_auc': 0.8020177751854611, 'eval_accuracy': 0.15214180206794684, 'eval_runtime': 0.5001, 'eval_samples_per_second': 1353.702, 'eval_steps_per_second': 57.987, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 2002/5080 [02:53<03:56, 13.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1568, 'grad_norm': 1.9167410135269165, 'learning_rate': 3.362445414847162e-05, 'epoch': 7.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2032/5080 [02:55<03:45, 13.50it/s]\n",
      " 40%|████      | 2032/5080 [02:55<03:45, 13.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5851396918296814, 'eval_f1': 0.7846004757290107, 'eval_roc_auc': 0.8044437579777531, 'eval_accuracy': 0.14180206794682423, 'eval_runtime': 0.5022, 'eval_samples_per_second': 1347.95, 'eval_steps_per_second': 57.741, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 2286/5080 [03:18<03:30, 13.26it/s]\n",
      " 45%|████▌     | 2286/5080 [03:19<03:30, 13.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6327815651893616, 'eval_f1': 0.7814134964972954, 'eval_roc_auc': 0.8019523244312996, 'eval_accuracy': 0.1447562776957164, 'eval_runtime': 0.5027, 'eval_samples_per_second': 1346.74, 'eval_steps_per_second': 57.689, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 2502/5080 [03:39<03:27, 12.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.116, 'grad_norm': 2.9025323390960693, 'learning_rate': 2.816593886462882e-05, 'epoch': 9.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2540/5080 [03:42<03:09, 13.40it/s]\n",
      " 50%|█████     | 2540/5080 [03:42<03:09, 13.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.626867949962616, 'eval_f1': 0.7873134328358209, 'eval_roc_auc': 0.8076503239579264, 'eval_accuracy': 0.16248153618906944, 'eval_runtime': 0.4999, 'eval_samples_per_second': 1354.185, 'eval_steps_per_second': 58.008, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 2794/5080 [04:05<02:50, 13.41it/s]\n",
      " 55%|█████▌    | 2794/5080 [04:05<02:50, 13.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6606573462486267, 'eval_f1': 0.7840213049267644, 'eval_roc_auc': 0.8044923182147118, 'eval_accuracy': 0.14771048744460857, 'eval_runtime': 0.5358, 'eval_samples_per_second': 1263.519, 'eval_steps_per_second': 54.124, 'epoch': 11.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 3002/5080 [04:24<02:48, 12.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0863, 'grad_norm': 0.8163240551948547, 'learning_rate': 2.2707423580786028e-05, 'epoch': 11.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3048/5080 [04:28<02:41, 12.57it/s]\n",
      " 60%|██████    | 3048/5080 [04:28<02:41, 12.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6866637468338013, 'eval_f1': 0.7799626301272355, 'eval_roc_auc': 0.8008625844553279, 'eval_accuracy': 0.1432791728212703, 'eval_runtime': 0.5534, 'eval_samples_per_second': 1223.363, 'eval_steps_per_second': 52.404, 'epoch': 12.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 3302/5080 [04:51<02:12, 13.38it/s]\n",
      " 65%|██████▌   | 3302/5080 [04:52<02:12, 13.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7243456244468689, 'eval_f1': 0.7818601348952786, 'eval_roc_auc': 0.8024372413091161, 'eval_accuracy': 0.14918759231905465, 'eval_runtime': 0.5135, 'eval_samples_per_second': 1318.463, 'eval_steps_per_second': 56.478, 'epoch': 13.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 3502/5080 [05:10<02:08, 12.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0639, 'grad_norm': 0.9643259048461914, 'learning_rate': 1.7248908296943234e-05, 'epoch': 13.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 3556/5080 [05:15<01:54, 13.26it/s]\n",
      " 70%|███████   | 3556/5080 [05:15<01:54, 13.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7442327737808228, 'eval_f1': 0.7813854728833053, 'eval_roc_auc': 0.8017394929466736, 'eval_accuracy': 0.14771048744460857, 'eval_runtime': 0.515, 'eval_samples_per_second': 1314.548, 'eval_steps_per_second': 56.31, 'epoch': 14.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3810/5080 [05:37<01:34, 13.47it/s]\n",
      " 75%|███████▌  | 3810/5080 [05:38<01:34, 13.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7518910765647888, 'eval_f1': 0.7786892634995042, 'eval_roc_auc': 0.8007468734446191, 'eval_accuracy': 0.1536189069423929, 'eval_runtime': 0.5041, 'eval_samples_per_second': 1343.01, 'eval_steps_per_second': 57.529, 'epoch': 15.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 4002/5080 [05:55<01:25, 12.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0479, 'grad_norm': 0.5296791195869446, 'learning_rate': 1.1790393013100438e-05, 'epoch': 15.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4064/5080 [06:00<01:15, 13.55it/s]\n",
      " 80%|████████  | 4064/5080 [06:01<01:15, 13.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7658830881118774, 'eval_f1': 0.7787119856887299, 'eval_roc_auc': 0.8001246662751869, 'eval_accuracy': 0.1447562776957164, 'eval_runtime': 0.4997, 'eval_samples_per_second': 1354.687, 'eval_steps_per_second': 58.029, 'epoch': 16.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 4318/5080 [06:24<01:02, 12.26it/s]\n",
      " 85%|████████▌ | 4318/5080 [06:24<01:02, 12.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7807112336158752, 'eval_f1': 0.779446354260901, 'eval_roc_auc': 0.8017343928879078, 'eval_accuracy': 0.14771048744460857, 'eval_runtime': 0.6318, 'eval_samples_per_second': 1071.517, 'eval_steps_per_second': 45.9, 'epoch': 17.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▊ | 4502/5080 [06:41<00:44, 12.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0383, 'grad_norm': 2.551100254058838, 'learning_rate': 6.342794759825328e-06, 'epoch': 17.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 4572/5080 [06:47<00:37, 13.57it/s]\n",
      " 90%|█████████ | 4572/5080 [06:47<00:37, 13.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7894988656044006, 'eval_f1': 0.7802996321880327, 'eval_roc_auc': 0.801847005475818, 'eval_accuracy': 0.1536189069423929, 'eval_runtime': 0.4988, 'eval_samples_per_second': 1357.157, 'eval_steps_per_second': 58.135, 'epoch': 18.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 4826/5080 [07:09<00:18, 13.68it/s]\n",
      " 95%|█████████▌| 4826/5080 [07:10<00:18, 13.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7982764840126038, 'eval_f1': 0.7787069508137757, 'eval_roc_auc': 0.8005562244736575, 'eval_accuracy': 0.15066469719350073, 'eval_runtime': 0.5016, 'eval_samples_per_second': 1349.693, 'eval_steps_per_second': 57.816, 'epoch': 19.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 5002/5080 [07:26<00:06, 12.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0324, 'grad_norm': 0.5551343560218811, 'learning_rate': 8.842794759825327e-07, 'epoch': 19.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5080/5080 [07:32<00:00, 13.52it/s]\n",
      "100%|██████████| 5080/5080 [07:35<00:00, 13.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.799960196018219, 'eval_f1': 0.7806718160589187, 'eval_roc_auc': 0.8022875573263056, 'eval_accuracy': 0.155096011816839, 'eval_runtime': 0.5184, 'eval_samples_per_second': 1305.893, 'eval_steps_per_second': 55.939, 'epoch': 20.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5080/5080 [07:41<00:00, 11.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 461.3046, 'train_samples_per_second': 263.817, 'train_steps_per_second': 11.012, 'train_loss': 0.14231886192569582, 'epoch': 20.0}\n",
      "Subset Accuracy: 0.1333\n",
      "Hamming Loss: 0.2969\n",
      "Micro Precision: 0.6473\n",
      "Micro Recall: 0.4382\n",
      "Micro F1-Score: 0.5226\n",
      "Precision per label: [0.76923077 0.88       0.5        0.65853659 0.46153846 0.61764706\n",
      " 0.53333333 0.74      ]\n",
      "Recall per label: [0.28571429 0.56410256 0.5        0.421875   0.14285714 0.45652174\n",
      " 0.38095238 0.62711864]\n",
      "F1-Score per label: [0.41666667 0.6875     0.5        0.51428571 0.21818182 0.525\n",
      " 0.44444444 0.67889908]\n",
      "{np.int64(0), np.int64(1), np.int64(2), np.int64(3), np.int64(4), np.int64(5), np.int64(6), np.int64(8)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1992904/525870965.py:81: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/our_data/ BALANCED_NEW_DATASET_500.csv\"\n",
    "dev_path = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/dev/NADI2024_subtask1_dev2.tsv\"\n",
    "labels = ['Algeria', 'Bahrain', 'Egypt', 'Iraq', 'Jordan', 'Kuwait',\n",
    "       'Lebanon', 'Libya', 'Morocco', 'Oman', 'Palestine', 'Qatar',\n",
    "       'Saudi_Arabia', 'Sudan', 'Syria', 'Tunisia', 'UAE', 'Yemen']\n",
    "trainer = BertTrainer(\n",
    "    training_dataset_path=dataset_path,\n",
    "    model_name=\"/home/lara.hassan/Documents/Cross-Country-Dialectal-Arabic-Identification/exp_8/camelbert_finetuned_epochs_2_eval_f1_0.7694_greater_threshold_0.3\",\n",
    "    labels=labels,\n",
    "    threshold=0.3,\n",
    "    exp_num=14\n",
    ")\n",
    "trainer.train(\n",
    "    num_train_epochs=20,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    "    per_device_train_batch_size=24,\n",
    "    per_device_eval_batch_size=24\n",
    ")\n",
    "trainer.evaluate(dev_path=dev_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OVERALL SCORES:\n",
      "MACRO AVERAGE PRECISION SCORE: 64.50 %\n",
      "MACRO AVERAGE RECALL SCORE: 42.24 %\n",
      "MACRO AVERAGE F1-SCORE: 49.81 %\n",
      "MACRO AVERAGE ACCURACY: 70.31 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "scorer_script = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/NADI2024-ST1-Scorer.py\"\n",
    "gold_file = \"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/sample_submission/NADI2024_subtask1_dev2_gold.txt\"\n",
    "predictions_file = \"/home/lara.hassan/Documents/Cross-Country-Dialectal-Arabic-Identification/exp_14/camelbert_finetuned_epochs_20_eval_f1_0.7881_greater_threshold_0.3/-home-lara.hassan-Documents-Cross-Country-Dialectal-Arabic-Identification-exp_8-camelbert_finetuned_epochs_2_eval_f1_0.7694_greater_threshold_0.3-experiment-14_predictions.txt\"\n",
    "!python3 \"{scorer_script}\" \"{gold_file}\" \"{predictions_file}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
       " array([[0.9507779 , 0.98521465, 0.97288233, 0.97754294, 0.98201376,\n",
       "         0.9885804 , 0.98475236, 0.96628344, 0.9546474 , 0.98197925,\n",
       "         0.9845754 , 0.9857729 , 0.9827572 , 0.97788346, 0.98521465,\n",
       "         0.94539934, 0.98340684, 0.9881765 ]], dtype=float32))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.predict([\"الله اكبر\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1]]),\n",
       " array([[0.00941259, 0.624179  , 0.7916601 , 0.4977589 , 0.6311397 ,\n",
       "         0.73278314, 0.33133605, 0.26999873, 0.00754792, 0.4181622 ,\n",
       "         0.47498128, 0.66093874, 0.86025184, 0.3869372 , 0.33654907,\n",
       "         0.00871138, 0.32637876, 0.47081262]], dtype=float32))"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.predict([\"انا مصرى ياسطااااا\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 16 Mekky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali.mekky/.conda/envs/DAI/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "                                                  \n",
      " 10%|█         | 252/2520 [00:22<03:11, 11.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.467267781496048, 'eval_f1': 0.7741051549922932, 'eval_roc_auc': 0.7878222018813346, 'eval_accuracy': 0.16542473919523099, 'eval_runtime': 0.5593, 'eval_samples_per_second': 1199.706, 'eval_steps_per_second': 50.062, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 502/2520 [00:49<03:01, 11.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4168, 'grad_norm': 6.076703071594238, 'learning_rate': 5e-05, 'epoch': 1.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 20%|██        | 504/2520 [00:49<02:55, 11.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.47096696496009827, 'eval_f1': 0.7664578147798472, 'eval_roc_auc': 0.7806076513088027, 'eval_accuracy': 0.12667660208643816, 'eval_runtime': 0.515, 'eval_samples_per_second': 1302.944, 'eval_steps_per_second': 54.37, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 30%|███       | 756/2520 [01:15<02:28, 11.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5002021789550781, 'eval_f1': 0.772990812594773, 'eval_roc_auc': 0.7912177076066527, 'eval_accuracy': 0.14008941877794337, 'eval_runtime': 0.5085, 'eval_samples_per_second': 1319.577, 'eval_steps_per_second': 55.064, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 1002/2520 [01:41<02:14, 11.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3131, 'grad_norm': 3.2008445262908936, 'learning_rate': 3.7673267326732673e-05, 'epoch': 3.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 40%|████      | 1008/2520 [01:42<02:08, 11.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5147558450698853, 'eval_f1': 0.7785293097467916, 'eval_roc_auc': 0.7934895058052991, 'eval_accuracy': 0.15052160953800298, 'eval_runtime': 0.5046, 'eval_samples_per_second': 1329.877, 'eval_steps_per_second': 55.494, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 50%|█████     | 1260/2520 [02:09<01:46, 11.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.544553816318512, 'eval_f1': 0.7688691564377121, 'eval_roc_auc': 0.7845958507551555, 'eval_accuracy': 0.14307004470938897, 'eval_runtime': 0.5047, 'eval_samples_per_second': 1329.552, 'eval_steps_per_second': 55.481, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 1501/2520 [02:34<01:30, 11.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2126, 'grad_norm': 12.227588653564453, 'learning_rate': 2.5321782178217822e-05, 'epoch': 5.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 60%|██████    | 1512/2520 [02:35<01:28, 11.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5832158327102661, 'eval_f1': 0.7710987601539119, 'eval_roc_auc': 0.784712128920743, 'eval_accuracy': 0.14605067064083457, 'eval_runtime': 0.5035, 'eval_samples_per_second': 1332.608, 'eval_steps_per_second': 55.608, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 70%|███████   | 1764/2520 [03:02<01:03, 11.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6284805536270142, 'eval_f1': 0.7659278574532812, 'eval_roc_auc': 0.7815790613766535, 'eval_accuracy': 0.12965722801788376, 'eval_runtime': 0.5013, 'eval_samples_per_second': 1338.444, 'eval_steps_per_second': 55.852, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 2002/2520 [03:26<00:45, 11.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1538, 'grad_norm': 1.6658316850662231, 'learning_rate': 1.2945544554455447e-05, 'epoch': 7.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 80%|████████  | 2016/2520 [03:28<00:43, 11.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6430134177207947, 'eval_f1': 0.7737251512532412, 'eval_roc_auc': 0.7884810466867804, 'eval_accuracy': 0.15648286140089418, 'eval_runtime': 0.5069, 'eval_samples_per_second': 1323.688, 'eval_steps_per_second': 55.236, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 90%|█████████ | 2268/2520 [03:55<00:21, 11.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6525330543518066, 'eval_f1': 0.7699037620297463, 'eval_roc_auc': 0.7861548168653606, 'eval_accuracy': 0.12965722801788376, 'eval_runtime': 0.5062, 'eval_samples_per_second': 1325.542, 'eval_steps_per_second': 55.313, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 2502/2520 [04:19<00:01, 11.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1132, 'grad_norm': 1.9419139623641968, 'learning_rate': 5.693069306930693e-07, 'epoch': 9.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|██████████| 2520/2520 [04:25<00:00, 11.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6510425209999084, 'eval_f1': 0.7713090465400035, 'eval_roc_auc': 0.7870727950749379, 'eval_accuracy': 0.13710879284649777, 'eval_runtime': 0.5029, 'eval_samples_per_second': 1334.228, 'eval_steps_per_second': 55.676, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2520/2520 [04:30<00:00,  9.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 270.5929, 'train_samples_per_second': 222.992, 'train_steps_per_second': 9.313, 'train_loss': 0.24082989210174197, 'epoch': 10.0}\n",
      "Subset Accuracy: 0.1667\n",
      "Hamming Loss: 0.2510\n",
      "Micro Precision: 0.7061\n",
      "Micro Recall: 0.5534\n",
      "Micro F1-Score: 0.6205\n",
      "Precision per label: [0.66666667 0.90909091 0.6        0.72222222 0.77777778 0.59259259\n",
      " 0.6        0.84444444]\n",
      "Recall per label: [0.17142857 0.76923077 0.78       0.609375   0.16666667 0.69565217\n",
      " 0.28571429 0.6440678 ]\n",
      "F1-Score per label: [0.27272727 0.83333333 0.67826087 0.66101695 0.2745098  0.64\n",
      " 0.38709677 0.73076923]\n",
      "{0, 1, 2, 3, 4, 5, 6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2559847/247078281.py:82: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    }
   ],
   "source": [
    "file_name = [\"First_200.csv\", \"NADIcombined_cleaned_MULTI_LABEL_MODIFIED_FINAL.csv\", \"First_1000.csv\", \"balanced_multilabel_dataset.csv\", \"balanced_multilabel_dataset_500.csv\"]\n",
    "# dataset_path = f\"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/our_data/{file_name[4]}\"\n",
    "dataset_path = '/home/ali.mekky/Documents/NLP/Project/Cross-Country-Dialectal-Arabic-Identification/balanced_multilabel_dataset_500.csv'\n",
    "dev_path = \"/home/ali.mekky/Documents/NLP/Project/NADI2024/subtask1/dev/NADI2024_subtask1_dev2.tsv\"\n",
    "labels = ['Algeria', 'Bahrain', 'Egypt', 'Iraq', 'Jordan', 'Kuwait',\n",
    "       'Lebanon', 'Libya', 'Morocco', 'Oman', 'Palestine', 'Qatar',\n",
    "       'Saudi_Arabia', 'Sudan', 'Syria', 'Tunisia', 'UAE', 'Yemen']\n",
    "trainer = BertTrainer(\n",
    "    training_dataset_path=dataset_path,\n",
    "    model_name=\"/home/ali.mekky/Documents/NLP/Project/Cross-Country-Dialectal-Arabic-Identification/exp_16/marbert_finetuned_epochs_2_eval_f1_0.7607_greater_threshold_0.3\",\n",
    "    labels=labels,\n",
    "    threshold=0.3,\n",
    "    exp_num=17\n",
    ")\n",
    "trainer.train(\n",
    "    num_train_epochs=10,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    "    per_device_train_batch_size=24,\n",
    "    per_device_eval_batch_size=24\n",
    ")\n",
    "trainer.evaluate(dev_path=dev_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OVERALL SCORES:\n",
      "MACRO AVERAGE PRECISION SCORE: 71.41 %\n",
      "MACRO AVERAGE RECALL SCORE: 51.53 %\n",
      "MACRO AVERAGE F1-SCORE: 55.97 %\n",
      "MACRO AVERAGE ACCURACY: 74.90 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "scorer_script = \"/home/ali.mekky/Documents/NLP/Project/NADI2024/subtask1/NADI2024-ST1-Scorer.py\"\n",
    "gold_file = \"/home/ali.mekky/Documents/NLP/Project/NADI2024/subtask1/sample_submission/NADI2024_subtask1_dev2_gold.txt\"\n",
    "predictions_file = \"/home/ali.mekky/Documents/NLP/Project/Cross-Country-Dialectal-Arabic-Identification/exp_17/marbert_finetuned_epochs_10_eval_f1_0.7785_greater_threshold_0.3/-home-ali.mekky-Documents-NLP-Project-Cross-Country-Dialectal-Arabic-Identification-exp_16-marbert_finetuned_epochs_2_eval_f1_0.7607_greater_threshold_0.3-experiment-17_predictions.txt\"\n",
    "!python3 \"{scorer_script}\" \"{gold_file}\" \"{predictions_file}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXP 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced dataset created and saved as 'balanced_multilabel_dataset_500.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "dataset_path = '/home/ali.mekky/Documents/NLP/Project/Cross-Country-Dialectal-Arabic-Identification/lr_binary_classifiers/annotated_multi_label_logisitc_regression.csv'\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Load the dataset\n",
    "label_columns = df.columns[2:-1]  # Excludes 'id', 'tweet', and 'Computed' columns\n",
    "df[label_columns] = df[label_columns].astype(int)  # Ensure labels are integers\n",
    "\n",
    "threshold = 500  \n",
    "balanced_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over possible combinations of active labels    model_name=\"/home/lara.hassan/Documents/Cross-Country-Dialectal-Arabic-Identification/exp_8/camelbert_finetuned_epochs_2_eval_f1_0.7694_greater_threshold_0.3\",\n",
    "\n",
    "for num_classes in range(1, len(label_columns) + 1):\n",
    "    subset = df[df[label_columns].sum(axis=1) == num_classes]  # Filter rows with num_classes active labels\n",
    "    \n",
    "    # Shuffle and sample the subset if it exceeds the threshold\n",
    "    if len(subset) > threshold:\n",
    "        subset = shuffle(subset).head(threshold)\n",
    "    \n",
    "    # Append sampled subset to the balanced dataset\n",
    "    balanced_df = pd.concat([balanced_df, subset], ignore_index=True)\n",
    "\n",
    "# Shuffle the final balanced DataFrame and save it to a new CSV\n",
    "balanced_df = shuffle(balanced_df).reset_index(drop=True)\n",
    "balanced_df.to_csv('balanced_multilabel_dataset_lr_' + str(threshold) + '.csv', index=False)\n",
    "\n",
    "print(\"Balanced dataset created and saved as 'balanced_multilabel_dataset_500.csv'\")\n",
    "\n",
    "label_columns = ['Algeria', 'Bahrain', 'Egypt', 'Iraq', 'Jordan', 'Kuwait', 'Lebanon', \n",
    "                 'Libya', 'Morocco', 'Oman', 'Palestine', 'Qatar', 'Saudi_Arabia', \n",
    "                 'Sudan', 'Syria', 'Tunisia', 'UAE', 'Yemen']\n",
    "\n",
    "# Initialize a dictionary to store counts\n",
    "counts = {}\n",
    "\n",
    "# Loop to count rows where the sum of 1s in label columns equals i (from 0 to 18)\n",
    "for i in range(19):\n",
    "    counts[i] = (balanced_df[label_columns].sum(axis=1) == i).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at CAMeL-Lab/bert-base-arabic-camelbert-ca and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/ali.mekky/.conda/envs/DAI/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|          | 17/22040 [07:48<28:04, 13.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5757, 'grad_norm': 1.2024204730987549, 'learning_rate': 4.99e-05, 'epoch': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 17/22040 [08:27<28:04, 13.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4805, 'grad_norm': 1.5924893617630005, 'learning_rate': 4.884168987929434e-05, 'epoch': 0.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 17/22040 [09:05<28:04, 13.07it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4364, 'grad_norm': 1.4749375581741333, 'learning_rate': 4.768105849582173e-05, 'epoch': 0.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 17/22040 [09:44<28:04, 13.07it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3985, 'grad_norm': 2.038799524307251, 'learning_rate': 4.652042711234912e-05, 'epoch': 0.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                    \n",
      "\n",
      "  0%|          | 17/22040 [10:04<28:04, 13.07it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3685016334056854, 'eval_f1': 0.7405778894472361, 'eval_roc_auc': 0.8159436906682538, 'eval_accuracy': 0.08133401395269696, 'eval_runtime': 5.0018, 'eval_samples_per_second': 1174.988, 'eval_steps_per_second': 48.983, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 17/22040 [10:31<28:04, 13.07it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3649, 'grad_norm': 1.7188373804092407, 'learning_rate': 4.535979572887651e-05, 'epoch': 1.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 17/22040 [11:13<28:04, 13.07it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3397, 'grad_norm': 1.6450800895690918, 'learning_rate': 4.420380687093779e-05, 'epoch': 1.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 17/22040 [11:55<28:04, 13.07it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3251, 'grad_norm': 1.400956392288208, 'learning_rate': 4.3043175487465184e-05, 'epoch': 1.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 17/22040 [12:36<28:04, 13.07it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3164, 'grad_norm': 1.8205997943878174, 'learning_rate': 4.188254410399257e-05, 'epoch': 1.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                    \n",
      "\n",
      "  0%|          | 17/22040 [13:16<28:04, 13.07it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.30282822251319885, 'eval_f1': 0.7869438855354348, 'eval_roc_auc': 0.8536088198945435, 'eval_accuracy': 0.12183086608814021, 'eval_runtime': 5.059, 'eval_samples_per_second': 1161.685, 'eval_steps_per_second': 48.428, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 17/22040 [13:26<28:04, 13.07it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2967, 'grad_norm': 1.816288948059082, 'learning_rate': 4.0721912720519964e-05, 'epoch': 2.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 17/22040 [14:07<28:04, 13.07it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.26, 'grad_norm': 1.423289179801941, 'learning_rate': 3.956128133704736e-05, 'epoch': 2.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 17/22040 [14:49<28:04, 13.07it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2553, 'grad_norm': 2.1077473163604736, 'learning_rate': 3.8400649953574744e-05, 'epoch': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 17/22040 [15:31<28:04, 13.07it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2498, 'grad_norm': 1.6912955045700073, 'learning_rate': 3.724001857010214e-05, 'epoch': 2.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 17/22040 [16:12<28:04, 13.07it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2472, 'grad_norm': 1.5335619449615479, 'learning_rate': 3.6079387186629524e-05, 'epoch': 2.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                    \n",
      "\n",
      "  0%|          | 17/22040 [16:27<28:04, 13.07it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2689467966556549, 'eval_f1': 0.8189405034957306, 'eval_roc_auc': 0.873100242242893, 'eval_accuracy': 0.16590096988259315, 'eval_runtime': 4.7482, 'eval_samples_per_second': 1237.735, 'eval_steps_per_second': 51.599, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 17/22040 [16:58<28:04, 13.07it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2104, 'grad_norm': 1.7322285175323486, 'learning_rate': 3.4918755803156924e-05, 'epoch': 3.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 17/22040 [17:35<28:04, 13.07it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1963, 'grad_norm': 1.6607365608215332, 'learning_rate': 3.376044568245125e-05, 'epoch': 3.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 17/22040 [18:12<28:04, 13.07it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.198, 'grad_norm': 1.6215678453445435, 'learning_rate': 3.2599814298978647e-05, 'epoch': 3.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 17/22040 [18:49<28:04, 13.07it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.197, 'grad_norm': 1.16743004322052, 'learning_rate': 3.143918291550604e-05, 'epoch': 3.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                    \n",
      "\n",
      "  0%|          | 17/22040 [19:16<28:04, 13.07it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.248249351978302, 'eval_f1': 0.8360323886639676, 'eval_roc_auc': 0.8859395341354289, 'eval_accuracy': 0.19567806704100732, 'eval_runtime': 4.4819, 'eval_samples_per_second': 1311.265, 'eval_steps_per_second': 54.664, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 17/22040 [19:32<28:04, 13.07it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1782, 'grad_norm': 1.5679394006729126, 'learning_rate': 3.0278551532033426e-05, 'epoch': 4.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 17/22040 [20:09<28:04, 13.07it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1531, 'grad_norm': 2.017313241958618, 'learning_rate': 2.9117920148560816e-05, 'epoch': 4.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 17/22040 [20:46<28:04, 13.07it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1524, 'grad_norm': 1.603104591369629, 'learning_rate': 2.7957288765088206e-05, 'epoch': 4.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 17/22040 [21:23<28:04, 13.07it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1486, 'grad_norm': 2.064905881881714, 'learning_rate': 2.6796657381615596e-05, 'epoch': 4.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 17/22040 [22:00<28:04, 13.07it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1525, 'grad_norm': 1.762682318687439, 'learning_rate': 2.5636025998142993e-05, 'epoch': 4.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                     \n",
      "\n",
      "  0%|          | 17/22040 [22:06<28:04, 13.07it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2502104341983795, 'eval_f1': 0.8405396056727776, 'eval_roc_auc': 0.8897603483821204, 'eval_accuracy': 0.19925131870001703, 'eval_runtime': 4.5219, 'eval_samples_per_second': 1299.669, 'eval_steps_per_second': 54.181, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 17/22040 [22:44<28:04, 13.07it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1163, 'grad_norm': 1.4346753358840942, 'learning_rate': 2.4475394614670383e-05, 'epoch': 5.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 17/22040 [23:21<28:04, 13.07it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.114, 'grad_norm': 1.737872838973999, 'learning_rate': 2.3314763231197773e-05, 'epoch': 5.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 17/22040 [23:58<28:04, 13.07it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1156, 'grad_norm': 1.7948397397994995, 'learning_rate': 2.2154131847725163e-05, 'epoch': 5.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 17/22040 [24:35<28:04, 13.07it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1145, 'grad_norm': 1.6340928077697754, 'learning_rate': 2.0993500464252556e-05, 'epoch': 5.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                     \n",
      "\n",
      "  0%|          | 17/22040 [24:56<28:04, 13.07it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2612988352775574, 'eval_f1': 0.8399665483061652, 'eval_roc_auc': 0.8906921463061697, 'eval_accuracy': 0.2028245703590267, 'eval_runtime': 4.5313, 'eval_samples_per_second': 1296.973, 'eval_steps_per_second': 54.068, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 17/22040 [25:19<28:04, 13.07it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1008, 'grad_norm': 1.9445706605911255, 'learning_rate': 1.9832869080779946e-05, 'epoch': 6.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 17/22040 [25:56<28:04, 13.07it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0867, 'grad_norm': 1.8219438791275024, 'learning_rate': 1.8672237697307336e-05, 'epoch': 6.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 17/22040 [26:33<28:04, 13.07it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0872, 'grad_norm': 1.6519575119018555, 'learning_rate': 1.7511606313834726e-05, 'epoch': 6.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 17/22040 [27:10<28:04, 13.07it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0848, 'grad_norm': 2.5256311893463135, 'learning_rate': 1.6350974930362116e-05, 'epoch': 6.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                     \n",
      "\n",
      "  0%|          | 17/22040 [27:46<28:04, 13.07it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.26733845472335815, 'eval_f1': 0.848101802757158, 'eval_roc_auc': 0.8923125768656276, 'eval_accuracy': 0.22154160285860133, 'eval_runtime': 4.4769, 'eval_samples_per_second': 1312.737, 'eval_steps_per_second': 54.725, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 17/22040 [27:54<28:04, 13.07it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0812, 'grad_norm': 1.622444748878479, 'learning_rate': 1.5192664809656454e-05, 'epoch': 7.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 17/22040 [28:31<28:04, 13.07it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0644, 'grad_norm': 1.2063298225402832, 'learning_rate': 1.4032033426183844e-05, 'epoch': 7.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 17/22040 [29:08<28:04, 13.07it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0649, 'grad_norm': 2.188608407974243, 'learning_rate': 1.2876044568245126e-05, 'epoch': 7.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 17/22040 [29:45<28:04, 13.07it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0632, 'grad_norm': 1.4193909168243408, 'learning_rate': 1.1715413184772516e-05, 'epoch': 7.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 17/22040 [30:22<28:04, 13.07it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0632, 'grad_norm': 2.0214288234710693, 'learning_rate': 1.0554781801299908e-05, 'epoch': 7.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                     \n",
      "\n",
      "  0%|          | 17/22040 [30:36<28:04, 13.07it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2804032564163208, 'eval_f1': 0.8489762650890235, 'eval_roc_auc': 0.8939247695055154, 'eval_accuracy': 0.2298791900629573, 'eval_runtime': 4.4993, 'eval_samples_per_second': 1306.212, 'eval_steps_per_second': 54.453, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 17/22040 [31:06<28:04, 13.07it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0536, 'grad_norm': 1.3284273147583008, 'learning_rate': 9.394150417827298e-06, 'epoch': 8.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 17/22040 [31:43<28:04, 13.07it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.049, 'grad_norm': 1.6149990558624268, 'learning_rate': 8.23351903435469e-06, 'epoch': 8.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 17/22040 [32:20<28:04, 13.07it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0482, 'grad_norm': 1.2953239679336548, 'learning_rate': 7.0752089136490255e-06, 'epoch': 8.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 17/22040 [32:57<28:04, 13.07it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0472, 'grad_norm': 1.3703434467315674, 'learning_rate': 5.914577530176416e-06, 'epoch': 8.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                     \n",
      "\n",
      "  0%|          | 17/22040 [33:27<28:04, 13.07it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.28599122166633606, 'eval_f1': 0.8500497080527045, 'eval_roc_auc': 0.8953067030531436, 'eval_accuracy': 0.23038965458567295, 'eval_runtime': 4.5147, 'eval_samples_per_second': 1301.752, 'eval_steps_per_second': 54.267, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 17/22040 [33:41<28:04, 13.07it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0435, 'grad_norm': 1.9104195833206177, 'learning_rate': 4.753946146703807e-06, 'epoch': 9.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 17/22040 [34:18<28:04, 13.07it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.039, 'grad_norm': 1.6241123676300049, 'learning_rate': 3.593314763231198e-06, 'epoch': 9.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 17/22040 [34:55<28:04, 13.07it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0374, 'grad_norm': 1.7759971618652344, 'learning_rate': 2.4326833797585887e-06, 'epoch': 9.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 17/22040 [35:32<28:04, 13.07it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0369, 'grad_norm': 2.0826504230499268, 'learning_rate': 1.2720519962859795e-06, 'epoch': 9.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 17/22040 [36:09<28:04, 13.07it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0362, 'grad_norm': 0.8331364393234253, 'learning_rate': 1.1142061281337048e-07, 'epoch': 9.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                     \n",
      "\n",
      "  0%|          | 17/22040 [36:19<28:04, 13.07it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2876950204372406, 'eval_f1': 0.8528719330591406, 'eval_roc_auc': 0.895477467419483, 'eval_accuracy': 0.24025863535817593, 'eval_runtime': 4.5196, 'eval_samples_per_second': 1300.323, 'eval_steps_per_second': 54.208, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 22040/22040 [29:11<00:00, 12.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1751.485, 'train_samples_per_second': 301.978, 'train_steps_per_second': 12.584, 'train_loss': 0.17430943555710754, 'epoch': 10.0}\n",
      "Subset Accuracy: 0.1083\n",
      "Hamming Loss: 0.3323\n",
      "Micro Precision: 0.5481\n",
      "Micro Recall: 0.5927\n",
      "Micro F1-Score: 0.5695\n",
      "Precision per label: [0.5        0.6        0.51851852 0.58490566 0.73076923 0.53846154\n",
      " 0.38636364 0.5862069 ]\n",
      "Recall per label: [0.57142857 0.69230769 0.56       0.484375   0.45238095 0.76086957\n",
      " 0.80952381 0.57627119]\n",
      "F1-Score per label: [0.53333333 0.64285714 0.53846154 0.52991453 0.55882353 0.63063063\n",
      " 0.52307692 0.58119658]\n",
      "{0, 1, 2, 3, 4, 5, 6, 7, 8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2559847/4032790609.py:82: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    }
   ],
   "source": [
    "file_name = [\"First_200.csv\", \"NADIcombined_cleaned_MULTI_LABEL_MODIFIED_FINAL.csv\", \"First_1000.csv\", \"balanced_multilabel_dataset.csv\", \"balanced_multilabel_dataset_500.csv\"]\n",
    "# dataset_path = f\"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/our_data/{file_name[4]}\"\n",
    "dataset_path = '/home/ali.mekky/Documents/NLP/Project/Cross-Country-Dialectal-Arabic-Identification/lr_binary_classifiers/annotated_multi_label_logisitc_regression.csv'\n",
    "dev_path = \"/home/ali.mekky/Documents/NLP/Project/NADI2024/subtask1/dev/NADI2024_subtask1_dev2.tsv\"\n",
    "labels = ['Algeria', 'Bahrain', 'Egypt', 'Iraq', 'Jordan', 'Kuwait',\n",
    "       'Lebanon', 'Libya', 'Morocco', 'Oman', 'Palestine', 'Qatar',\n",
    "       'Saudi_Arabia', 'Sudan', 'Syria', 'Tunisia', 'UAE', 'Yemen']\n",
    "trainer = BertTrainer(\n",
    "    training_dataset_path=dataset_path,\n",
    "    model_name=\"CAMeL-Lab/bert-base-arabic-camelbert-ca\",\n",
    "    labels=labels,\n",
    "    threshold=0.3,\n",
    "    exp_num=18\n",
    ")\n",
    "trainer.train(\n",
    "    num_train_epochs=10,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    "    per_device_train_batch_size=24,\n",
    "    per_device_eval_batch_size=24,\n",
    ")\n",
    "trainer.evaluate(dev_path=dev_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OVERALL SCORES:\n",
      "MACRO AVERAGE PRECISION SCORE: 46.80 %\n",
      "MACRO AVERAGE RECALL SCORE: 64.37 %\n",
      "MACRO AVERAGE F1-SCORE: 52.38 %\n",
      "MACRO AVERAGE ACCURACY: 58.44 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "scorer_script = \"/home/ali.mekky/Documents/NLP/Project/NADI2024/subtask1/NADI2024-ST1-Scorer.py\"\n",
    "gold_file = \"/home/ali.mekky/Documents/NLP/Project/NADI2024/subtask1/sample_submission/NADI2024_subtask1_dev2_gold.txt\"\n",
    "predictions_file = \"/home/ali.mekky/Documents/NLP/Project/Cross-Country-Dialectal-Arabic-Identification/exp_18/marbert_finetuned_epochs_10_eval_f1_0.8353_greater_threshold_0.3/CAMeL-Lab-bert-base-arabic-camelbert-ca-experiment-18_predictions.txt\"\n",
    "!python3 \"{scorer_script}\" \"{gold_file}\" \"{predictions_file}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]),\n",
       " array([[0.99621993, 0.99293363, 0.9789554 , 0.9881765 , 0.99378294,\n",
       "         0.99346113, 0.98922324, 0.99135584, 0.9954261 , 0.9932811 ,\n",
       "         0.994292  , 0.9939731 , 0.994089  , 0.9908035 , 0.99158704,\n",
       "         0.99317604, 0.9935369 , 0.9952632 ]], dtype=float32),\n",
       " 0.007803493075900558)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.predict([\"الله اكبر\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]]),\n",
       " array([[0.00602695, 0.01210039, 0.02551661, 0.13398075, 0.02532306,\n",
       "         0.01596765, 0.12995382, 0.0085113 , 0.00219124, 0.01200735,\n",
       "         0.04689926, 0.00372171, 0.01778039, 0.00564191, 0.43236297,\n",
       "         0.0102883 , 0.01764446, 0.0124797 ]], dtype=float32),\n",
       " 0.9489779008759393)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.predict([\"إيه يا عم الجو حر موت النهارده، لازم نشرب حاجة ساقعة\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at UBC-NLP/MARBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/ali.mekky/.conda/envs/DAI/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "                                                  \n",
      "  0%|          | 17/22040 [3:31:48<28:04, 13.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5377, 'grad_norm': 2.3052988052368164, 'learning_rate': 5e-05, 'epoch': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:32:33<28:04, 13.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4135, 'grad_norm': 1.8364241123199463, 'learning_rate': 4.883936861652739e-05, 'epoch': 0.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:33:18<28:04, 13.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3668, 'grad_norm': 2.281428098678589, 'learning_rate': 4.768105849582173e-05, 'epoch': 0.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:34:04<28:04, 13.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.33, 'grad_norm': 2.3681914806365967, 'learning_rate': 4.652042711234912e-05, 'epoch': 0.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                    \n",
      "\u001b[A                                                 \n",
      "  0%|          | 17/22040 [3:34:27<28:04, 13.07it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.30051612854003906, 'eval_f1': 0.7969495534439868, 'eval_roc_auc': 0.8578380791930998, 'eval_accuracy': 0.12710566615620214, 'eval_runtime': 4.8859, 'eval_samples_per_second': 1202.838, 'eval_steps_per_second': 50.144, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:34:59<28:04, 13.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2879, 'grad_norm': 3.490952968597412, 'learning_rate': 4.535979572887651e-05, 'epoch': 1.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:35:46<28:04, 13.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.257, 'grad_norm': 3.019273042678833, 'learning_rate': 4.4199164345403905e-05, 'epoch': 1.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:36:33<28:04, 13.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2451, 'grad_norm': 3.494957208633423, 'learning_rate': 4.303853296193129e-05, 'epoch': 1.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:37:20<28:04, 13.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2384, 'grad_norm': 4.560043811798096, 'learning_rate': 4.188022284122563e-05, 'epoch': 1.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                    \n",
      "\u001b[A                                                 \n",
      "  0%|          | 17/22040 [3:38:04<28:04, 13.07it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.24287019670009613, 'eval_f1': 0.8345204436711614, 'eval_roc_auc': 0.887667324454597, 'eval_accuracy': 0.18461800238216777, 'eval_runtime': 5.0269, 'eval_samples_per_second': 1169.116, 'eval_steps_per_second': 48.738, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:38:16<28:04, 13.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2193, 'grad_norm': 2.826425075531006, 'learning_rate': 4.071959145775302e-05, 'epoch': 2.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:39:03<28:04, 13.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1644, 'grad_norm': 4.39860725402832, 'learning_rate': 3.955896007428041e-05, 'epoch': 2.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:39:51<28:04, 13.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.162, 'grad_norm': 3.645512104034424, 'learning_rate': 3.83983286908078e-05, 'epoch': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:40:33<28:04, 13.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1606, 'grad_norm': 4.56809663772583, 'learning_rate': 3.724001857010214e-05, 'epoch': 2.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:41:15<28:04, 13.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1592, 'grad_norm': 4.311180591583252, 'learning_rate': 3.6079387186629524e-05, 'epoch': 2.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                    \n",
      "\u001b[A                                                 \n",
      "  0%|          | 17/22040 [3:41:29<28:04, 13.07it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.23717337846755981, 'eval_f1': 0.8507988622051145, 'eval_roc_auc': 0.8942350016502764, 'eval_accuracy': 0.2222222222222222, 'eval_runtime': 4.5348, 'eval_samples_per_second': 1295.974, 'eval_steps_per_second': 54.026, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:42:06<28:04, 13.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1169, 'grad_norm': 2.881850242614746, 'learning_rate': 3.4918755803156924e-05, 'epoch': 3.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:42:48<28:04, 13.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1001, 'grad_norm': 3.451972246170044, 'learning_rate': 3.375812441968431e-05, 'epoch': 3.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:43:30<28:04, 13.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1012, 'grad_norm': 3.4587137699127197, 'learning_rate': 3.2597493036211704e-05, 'epoch': 3.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:44:12<28:04, 13.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1014, 'grad_norm': 3.3298377990722656, 'learning_rate': 3.143686165273909e-05, 'epoch': 3.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                    \n",
      "\u001b[A                                                 \n",
      "  0%|          | 17/22040 [3:44:43<28:04, 13.07it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.24726076424121857, 'eval_f1': 0.8516292944719601, 'eval_roc_auc': 0.8935461907367093, 'eval_accuracy': 0.22596562872213716, 'eval_runtime': 4.5281, 'eval_samples_per_second': 1297.89, 'eval_steps_per_second': 54.106, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:45:02<28:04, 13.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0855, 'grad_norm': 2.2144157886505127, 'learning_rate': 3.027623026926648e-05, 'epoch': 4.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:45:44<28:04, 13.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0616, 'grad_norm': 3.439931631088257, 'learning_rate': 2.911559888579387e-05, 'epoch': 4.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:46:26<28:04, 13.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0616, 'grad_norm': 2.223876714706421, 'learning_rate': 2.7954967502321267e-05, 'epoch': 4.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:47:08<28:04, 13.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0608, 'grad_norm': 4.037909507751465, 'learning_rate': 2.6796657381615596e-05, 'epoch': 4.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:47:50<28:04, 13.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0631, 'grad_norm': 3.489192008972168, 'learning_rate': 2.5636025998142993e-05, 'epoch': 4.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                    \n",
      "\u001b[A                                                  \n",
      "  0%|          | 17/22040 [3:47:57<28:04, 13.07it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.273395299911499, 'eval_f1': 0.850458689212895, 'eval_roc_auc': 0.8915369591754604, 'eval_accuracy': 0.23175089331291476, 'eval_runtime': 4.5245, 'eval_samples_per_second': 1298.938, 'eval_steps_per_second': 54.15, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:48:41<28:04, 13.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0374, 'grad_norm': 2.9217259883880615, 'learning_rate': 2.4475394614670383e-05, 'epoch': 5.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:49:23<28:04, 13.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0372, 'grad_norm': 1.4874317646026611, 'learning_rate': 2.3314763231197773e-05, 'epoch': 5.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:50:05<28:04, 13.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0377, 'grad_norm': 2.8260183334350586, 'learning_rate': 2.2154131847725163e-05, 'epoch': 5.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:50:47<28:04, 13.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0368, 'grad_norm': 2.603254556655884, 'learning_rate': 2.0993500464252556e-05, 'epoch': 5.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                    \n",
      "\u001b[A                                                  \n",
      "  0%|          | 17/22040 [3:51:10<28:04, 13.07it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.30497220158576965, 'eval_f1': 0.8452645367048237, 'eval_roc_auc': 0.8891073677067548, 'eval_accuracy': 0.21728773183597072, 'eval_runtime': 4.5055, 'eval_samples_per_second': 1304.398, 'eval_steps_per_second': 54.378, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:51:37<28:04, 13.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0296, 'grad_norm': 1.689935326576233, 'learning_rate': 1.9835190343546892e-05, 'epoch': 6.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:52:19<28:04, 13.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0228, 'grad_norm': 1.8808188438415527, 'learning_rate': 1.8674558960074282e-05, 'epoch': 6.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:53:01<28:04, 13.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0233, 'grad_norm': 2.7396364212036133, 'learning_rate': 1.7513927576601672e-05, 'epoch': 6.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:53:44<28:04, 13.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0219, 'grad_norm': 5.668574333190918, 'learning_rate': 1.6353296193129066e-05, 'epoch': 6.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                    \n",
      "\u001b[A                                                  \n",
      "  0%|          | 17/22040 [3:54:24<28:04, 13.07it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3278944790363312, 'eval_f1': 0.8469969184254899, 'eval_roc_auc': 0.8873300295222103, 'eval_accuracy': 0.22443423515399014, 'eval_runtime': 4.4972, 'eval_samples_per_second': 1306.805, 'eval_steps_per_second': 54.478, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:54:34<28:04, 13.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.02, 'grad_norm': 1.0567728281021118, 'learning_rate': 1.5192664809656454e-05, 'epoch': 7.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:55:18<28:04, 13.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0134, 'grad_norm': 2.4436826705932617, 'learning_rate': 1.4032033426183844e-05, 'epoch': 7.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:56:04<28:04, 13.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0135, 'grad_norm': 1.4169901609420776, 'learning_rate': 1.2871402042711237e-05, 'epoch': 7.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:56:48<28:04, 13.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0133, 'grad_norm': 1.036405086517334, 'learning_rate': 1.1710770659238625e-05, 'epoch': 7.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:57:32<28:04, 13.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.013, 'grad_norm': 2.0383734703063965, 'learning_rate': 1.0550139275766017e-05, 'epoch': 7.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                    \n",
      "\u001b[A                                                  \n",
      "  0%|          | 17/22040 [3:57:47<28:04, 13.07it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3496192693710327, 'eval_f1': 0.8465674230380112, 'eval_roc_auc': 0.8878549000025162, 'eval_accuracy': 0.22613578356304237, 'eval_runtime': 4.5112, 'eval_samples_per_second': 1302.762, 'eval_steps_per_second': 54.309, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:58:22<28:04, 13.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0097, 'grad_norm': 1.4177751541137695, 'learning_rate': 9.394150417827298e-06, 'epoch': 8.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:59:04<28:04, 13.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0079, 'grad_norm': 0.9546614289283752, 'learning_rate': 8.23351903435469e-06, 'epoch': 8.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [3:59:46<28:04, 13.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0078, 'grad_norm': 0.8336026668548584, 'learning_rate': 7.07288765088208e-06, 'epoch': 8.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [4:00:28<28:04, 13.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0074, 'grad_norm': 0.28554850816726685, 'learning_rate': 5.912256267409471e-06, 'epoch': 8.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                    \n",
      "\u001b[A                                                  \n",
      "  0%|          | 17/22040 [4:01:01<28:04, 13.07it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.36540687084198, 'eval_f1': 0.8474516695957821, 'eval_roc_auc': 0.8865577468982784, 'eval_accuracy': 0.23158073847200952, 'eval_runtime': 4.5015, 'eval_samples_per_second': 1305.557, 'eval_steps_per_second': 54.426, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [4:01:18<28:04, 13.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0063, 'grad_norm': 0.3903147578239441, 'learning_rate': 4.751624883936862e-06, 'epoch': 9.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [4:02:00<28:04, 13.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0055, 'grad_norm': 1.0045398473739624, 'learning_rate': 3.590993500464253e-06, 'epoch': 9.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [4:02:42<28:04, 13.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0049, 'grad_norm': 0.40790706872940063, 'learning_rate': 2.4303621169916438e-06, 'epoch': 9.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [4:03:24<28:04, 13.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0048, 'grad_norm': 0.6210858821868896, 'learning_rate': 1.2697307335190344e-06, 'epoch': 9.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "  0%|          | 17/22040 [4:04:06<28:04, 13.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0047, 'grad_norm': 0.9460890889167786, 'learning_rate': 1.0909935004642526e-07, 'epoch': 9.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                    \n",
      "\u001b[A                                                  \n",
      "  0%|          | 17/22040 [4:04:18<28:04, 13.07it/s]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3677639365196228, 'eval_f1': 0.8469693880523111, 'eval_roc_auc': 0.8870306658177636, 'eval_accuracy': 0.2285179513357155, 'eval_runtime': 4.508, 'eval_samples_per_second': 1303.674, 'eval_steps_per_second': 54.347, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      "100%|██████████| 22040/22040 [33:18<00:00, 11.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1998.5777, 'train_samples_per_second': 264.643, 'train_steps_per_second': 11.028, 'train_loss': 0.10601788969835547, 'epoch': 10.0}\n",
      "Subset Accuracy: 0.0917\n",
      "Hamming Loss: 0.3302\n",
      "Micro Precision: 0.5499\n",
      "Micro Recall: 0.6039\n",
      "Micro F1-Score: 0.5756\n",
      "Precision per label: [0.54901961 0.68181818 0.54       0.58695652 0.63333333 0.50724638\n",
      " 0.34693878 0.61538462]\n",
      "Recall per label: [0.8        0.76923077 0.54       0.421875   0.45238095 0.76086957\n",
      " 0.80952381 0.54237288]\n",
      "F1-Score per label: [0.65116279 0.72289157 0.54       0.49090909 0.52777778 0.60869565\n",
      " 0.48571429 0.57657658]\n",
      "{0, 1, 2, 3, 4, 5, 6, 7, 8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2559847/4032790609.py:82: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    }
   ],
   "source": [
    "file_name = [\"First_200.csv\", \"NADIcombined_cleaned_MULTI_LABEL_MODIFIED_FINAL.csv\", \"First_1000.csv\", \"balanced_multilabel_dataset.csv\", \"balanced_multilabel_dataset_500.csv\"]\n",
    "# dataset_path = f\"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/our_data/{file_name[4]}\"\n",
    "dataset_path = '/home/ali.mekky/Documents/NLP/Project/Cross-Country-Dialectal-Arabic-Identification/lr_binary_classifiers/annotated_multi_label_logisitc_regression.csv'\n",
    "dev_path = \"/home/ali.mekky/Documents/NLP/Project/NADI2024/subtask1/dev/NADI2024_subtask1_dev2.tsv\"\n",
    "labels = ['Algeria', 'Bahrain', 'Egypt', 'Iraq', 'Jordan', 'Kuwait',\n",
    "       'Lebanon', 'Libya', 'Morocco', 'Oman', 'Palestine', 'Qatar',\n",
    "       'Saudi_Arabia', 'Sudan', 'Syria', 'Tunisia', 'UAE', 'Yemen']\n",
    "trainer = BertTrainer(\n",
    "    training_dataset_path=dataset_path,\n",
    "    model_name=\"UBC-NLP/MARBERT\",\n",
    "    labels=labels,\n",
    "    threshold=0.3,\n",
    "    exp_num=18\n",
    ")\n",
    "trainer.train(\n",
    "    num_train_epochs=10,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    "    per_device_train_batch_size=24,\n",
    "    per_device_eval_batch_size=24,\n",
    ")\n",
    "trainer.evaluate(dev_path=dev_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OVERALL SCORES:\n",
      "MACRO AVERAGE PRECISION SCORE: 55.76 %\n",
      "MACRO AVERAGE RECALL SCORE: 63.70 %\n",
      "MACRO AVERAGE F1-SCORE: 57.55 %\n",
      "MACRO AVERAGE ACCURACY: 66.98 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "scorer_script = \"/home/ali.mekky/Documents/NLP/Project/NADI2024/subtask1/NADI2024-ST1-Scorer.py\"\n",
    "gold_file = \"/home/ali.mekky/Documents/NLP/Project/NADI2024/subtask1/sample_submission/NADI2024_subtask1_dev2_gold.txt\"\n",
    "predictions_file = \"/home/ali.mekky/Documents/NLP/Project/Cross-Country-Dialectal-Arabic-Identification/exp_18/marbert_finetuned_epochs_10_eval_f1_0.8516_greater_threshold_0.3/UBC-NLP-MARBERT-experiment-18_predictions.txt\"\n",
    "!python3 \"{scorer_script}\" \"{gold_file}\" \"{predictions_file}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "balancing the dataset produced by the logistic regression and finetuning marbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali.mekky/.conda/envs/DAI/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'triton.language' has no attribute 'core'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 15\u001b[0m\n\u001b[1;32m      5\u001b[0m labels \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mAlgeria\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mBahrain\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mEgypt\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mIraq\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mJordan\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mKuwait\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      6\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mLebanon\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mLibya\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMorocco\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mOman\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mPalestine\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mQatar\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      7\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mSaudi_Arabia\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mSudan\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mSyria\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mTunisia\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mUAE\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mYemen\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m      8\u001b[0m trainer \u001b[39m=\u001b[39m BertTrainer(\n\u001b[1;32m      9\u001b[0m     training_dataset_path\u001b[39m=\u001b[39mdataset_path,\n\u001b[1;32m     10\u001b[0m     model_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/home/ali.mekky/Documents/NLP/Project/Cross-Country-Dialectal-Arabic-Identification/exp_20/marbert_finetuned_epochs_10_eval_f1_0.8621_greater_threshold_0.3\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     exp_num\u001b[39m=\u001b[39m\u001b[39m21\u001b[39m\n\u001b[1;32m     14\u001b[0m )\n\u001b[0;32m---> 15\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain(\n\u001b[1;32m     16\u001b[0m     num_train_epochs\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m,\n\u001b[1;32m     17\u001b[0m     metric_for_best_model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39meval_f1\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     18\u001b[0m     greater_is_better\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     19\u001b[0m     per_device_train_batch_size\u001b[39m=\u001b[39;49m\u001b[39m24\u001b[39;49m,\n\u001b[1;32m     20\u001b[0m     per_device_eval_batch_size\u001b[39m=\u001b[39;49m\u001b[39m24\u001b[39;49m,\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     22\u001b[0m trainer\u001b[39m.\u001b[39mevaluate(dev_path\u001b[39m=\u001b[39mdev_path)\n",
      "Cell \u001b[0;32mIn[4], line 166\u001b[0m, in \u001b[0;36mBertTrainer.train\u001b[0;34m(self, num_train_epochs, metric_for_best_model, greater_is_better, per_device_train_batch_size, per_device_eval_batch_size, patience)\u001b[0m\n\u001b[1;32m    153\u001b[0m early_stopping_callback \u001b[39m=\u001b[39m EarlyStoppingCallback(\n\u001b[1;32m    154\u001b[0m early_stopping_patience\u001b[39m=\u001b[39mpatience\n\u001b[1;32m    155\u001b[0m )\n\u001b[1;32m    156\u001b[0m trainer \u001b[39m=\u001b[39m CustomTrainer(\n\u001b[1;32m    157\u001b[0m     model\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel,\n\u001b[1;32m    158\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[39m# callbacks=[early_stopping_callback] \u001b[39;00m\n\u001b[1;32m    164\u001b[0m )\n\u001b[0;32m--> 166\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m    167\u001b[0m best_metric_value \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mbest_metric \n\u001b[1;32m    168\u001b[0m num_epochs \u001b[39m=\u001b[39m training_args\u001b[39m.\u001b[39mnum_train_epochs\n",
      "File \u001b[0;32m~/.conda/envs/DAI/lib/python3.10/site-packages/transformers/trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1936\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1937\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1938\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1939\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1940\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1941\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1942\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1943\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/DAI/lib/python3.10/site-packages/transformers/trainer.py:2039\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2036\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlr_scheduler \u001b[39m=\u001b[39m deepspeed_init(\u001b[39mself\u001b[39m, num_training_steps\u001b[39m=\u001b[39mmax_steps)\n\u001b[1;32m   2038\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m delay_optimizer_creation:\n\u001b[0;32m-> 2039\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_optimizer_and_scheduler(num_training_steps\u001b[39m=\u001b[39;49mmax_steps)\n\u001b[1;32m   2041\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate \u001b[39m=\u001b[39m TrainerState(\n\u001b[1;32m   2042\u001b[0m     stateful_callbacks\u001b[39m=\u001b[39m[\n\u001b[1;32m   2043\u001b[0m         cb \u001b[39mfor\u001b[39;00m cb \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mcallbacks \u001b[39m+\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol] \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(cb, ExportableState)\n\u001b[1;32m   2044\u001b[0m     ]\n\u001b[1;32m   2045\u001b[0m )\n\u001b[1;32m   2046\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mis_hyper_param_search \u001b[39m=\u001b[39m trial \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/DAI/lib/python3.10/site-packages/transformers/trainer.py:1037\u001b[0m, in \u001b[0;36mTrainer.create_optimizer_and_scheduler\u001b[0;34m(self, num_training_steps)\u001b[0m\n\u001b[1;32m   1029\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_optimizer_and_scheduler\u001b[39m(\u001b[39mself\u001b[39m, num_training_steps: \u001b[39mint\u001b[39m):\n\u001b[1;32m   1030\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1031\u001b[0m \u001b[39m    Setup the optimizer and the learning rate scheduler.\u001b[39;00m\n\u001b[1;32m   1032\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1035\u001b[0m \u001b[39m    `create_scheduler`) in a subclass.\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1037\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_optimizer()\n\u001b[1;32m   1038\u001b[0m     \u001b[39mif\u001b[39;00m IS_SAGEMAKER_MP_POST_1_10 \u001b[39mand\u001b[39;00m smp\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mcfg\u001b[39m.\u001b[39mfp16:\n\u001b[1;32m   1039\u001b[0m         \u001b[39m# If smp >= 1.10 and fp16 is enabled, we unwrap the optimizer\u001b[39;00m\n\u001b[1;32m   1040\u001b[0m         optimizer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39moptimizer\n",
      "File \u001b[0;32m~/.conda/envs/DAI/lib/python3.10/site-packages/transformers/trainer.py:1099\u001b[0m, in \u001b[0;36mTrainer.create_optimizer\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1096\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39moptimizer_dict\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m optimizer_kwargs:\n\u001b[1;32m   1097\u001b[0m     optimizer_grouped_parameters \u001b[39m=\u001b[39m optimizer_kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39moptimizer_dict\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1099\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer \u001b[39m=\u001b[39m optimizer_cls(optimizer_grouped_parameters, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49moptimizer_kwargs)\n\u001b[1;32m   1101\u001b[0m \u001b[39mif\u001b[39;00m optimizer_cls\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mAdam8bit\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1102\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mbitsandbytes\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/DAI/lib/python3.10/site-packages/torch/optim/adamw.py:72\u001b[0m, in \u001b[0;36mAdamW.__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad, maximize, foreach, capturable, differentiable, fused)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid weight_decay value: \u001b[39m\u001b[39m{\u001b[39;00mweight_decay\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     60\u001b[0m defaults \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\n\u001b[1;32m     61\u001b[0m     lr\u001b[39m=\u001b[39mlr,\n\u001b[1;32m     62\u001b[0m     betas\u001b[39m=\u001b[39mbetas,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m     fused\u001b[39m=\u001b[39mfused,\n\u001b[1;32m     71\u001b[0m )\n\u001b[0;32m---> 72\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(params, defaults)\n\u001b[1;32m     74\u001b[0m \u001b[39mif\u001b[39;00m fused:\n\u001b[1;32m     75\u001b[0m     \u001b[39mif\u001b[39;00m differentiable:\n",
      "File \u001b[0;32m~/.conda/envs/DAI/lib/python3.10/site-packages/torch/optim/optimizer.py:367\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m    364\u001b[0m     param_groups \u001b[39m=\u001b[39m [{\u001b[39m\"\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m\"\u001b[39m: param_groups}]\n\u001b[1;32m    366\u001b[0m \u001b[39mfor\u001b[39;00m param_group \u001b[39min\u001b[39;00m param_groups:\n\u001b[0;32m--> 367\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_param_group(cast(\u001b[39mdict\u001b[39;49m, param_group))\n\u001b[1;32m    369\u001b[0m \u001b[39m# Allows _cuda_graph_capture_health_check to rig a poor man's TORCH_WARN_ONCE in python,\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[39m# which I don't think exists\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[39m# https://github.com/pytorch/pytorch/issues/72948\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_warned_capturable_if_run_uncaptured \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/DAI/lib/python3.10/site-packages/torch/_compile.py:26\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m disable_fn \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(fn, \u001b[39m\"\u001b[39m\u001b[39m__dynamo_disable\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m     25\u001b[0m \u001b[39mif\u001b[39;00m disable_fn \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 26\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_dynamo\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     disable_fn \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mdisable(fn, recursive)\n\u001b[1;32m     29\u001b[0m     fn\u001b[39m.\u001b[39m__dynamo_disable \u001b[39m=\u001b[39m disable_fn\n",
      "File \u001b[0;32m~/.conda/envs/DAI/lib/python3.10/site-packages/torch/_dynamo/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m convert_frame, eval_frame, resume_execution\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mbackends\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mregistry\u001b[39;00m \u001b[39mimport\u001b[39;00m list_backends, lookup_backend, register_backend\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcallback\u001b[39;00m \u001b[39mimport\u001b[39;00m callback_handler, on_compile_end, on_compile_start\n",
      "File \u001b[0;32m~/.conda/envs/DAI/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:48\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_python_dispatch\u001b[39;00m \u001b[39mimport\u001b[39;00m _disable_current_modes\n\u001b[1;32m     46\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_traceback\u001b[39;00m \u001b[39mimport\u001b[39;00m format_traceback_short\n\u001b[0;32m---> 48\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m config, exc, trace_rules\n\u001b[1;32m     49\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mbackends\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mregistry\u001b[39;00m \u001b[39mimport\u001b[39;00m CompilerFn\n\u001b[1;32m     50\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mbytecode_analysis\u001b[39;00m \u001b[39mimport\u001b[39;00m remove_dead_code, remove_pointless_jumps\n",
      "File \u001b[0;32m~/.conda/envs/DAI/lib/python3.10/site-packages/torch/_dynamo/exc.py:12\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_guards\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m config\n\u001b[0;32m---> 12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m counters\n\u001b[1;32m     15\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexportdb_error_message\u001b[39m(case_name):\n\u001b[1;32m     16\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m     17\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFor more information about this error, see: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     18\u001b[0m         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttps://pytorch.org/docs/main/generated/exportdb/index.html#\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     19\u001b[0m         \u001b[39m+\u001b[39m case_name\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m-\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/DAI/lib/python3.10/site-packages/torch/_dynamo/utils.py:1063\u001b[0m\n\u001b[1;32m   1044\u001b[0m     \u001b[39mreturn\u001b[39;00m fn\n\u001b[1;32m   1047\u001b[0m common_constant_types \u001b[39m=\u001b[39m {\n\u001b[1;32m   1048\u001b[0m     \u001b[39mint\u001b[39m,\n\u001b[1;32m   1049\u001b[0m     \u001b[39mfloat\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1060\u001b[0m     torch\u001b[39m.\u001b[39mlayout,\n\u001b[1;32m   1061\u001b[0m }\n\u001b[0;32m-> 1063\u001b[0m \u001b[39mif\u001b[39;00m has_triton_package():\n\u001b[1;32m   1064\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtriton\u001b[39;00m\n\u001b[1;32m   1066\u001b[0m     common_constant_types\u001b[39m.\u001b[39madd(triton\u001b[39m.\u001b[39mlanguage\u001b[39m.\u001b[39mdtype)\n",
      "File \u001b[0;32m~/.conda/envs/DAI/lib/python3.10/site-packages/torch/utils/_triton.py:9\u001b[0m, in \u001b[0;36mhas_triton_package\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mlru_cache(\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mhas_triton_package\u001b[39m() \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[1;32m      8\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m----> 9\u001b[0m         \u001b[39mimport\u001b[39;00m \u001b[39mtriton\u001b[39;00m\n\u001b[1;32m     11\u001b[0m         \u001b[39mreturn\u001b[39;00m triton \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/DAI/lib/python3.10/site-packages/triton/__init__.py:21\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mruntime\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      9\u001b[0m     autotune,\n\u001b[1;32m     10\u001b[0m     Config,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     MockTensor,\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mruntime\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mjit\u001b[39;00m \u001b[39mimport\u001b[39;00m jit\n\u001b[0;32m---> 21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcompiler\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39mcompile\u001b[39m, CompilationError\n\u001b[1;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39merrors\u001b[39;00m \u001b[39mimport\u001b[39;00m TritonError\n\u001b[1;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m language\n",
      "File \u001b[0;32m~/.conda/envs/DAI/lib/python3.10/site-packages/triton/compiler/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcompiler\u001b[39;00m \u001b[39mimport\u001b[39;00m CompiledKernel, ASTSource, \u001b[39mcompile\u001b[39m, AttrsDescriptor, make_backend, LazyDict\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39merrors\u001b[39;00m \u001b[39mimport\u001b[39;00m CompilationError\n\u001b[1;32m      4\u001b[0m __all__ \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mcompile\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmake_backend\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mASTSource\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mAttrsDescriptor\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mCompiledKernel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mCompilationError\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mLazyDict\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/DAI/lib/python3.10/site-packages/triton/compiler/compiler.py:13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39m# TODO: this shouldn't be here\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdataclasses\u001b[39;00m \u001b[39mimport\u001b[39;00m dataclass\n\u001b[0;32m---> 13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcode_generator\u001b[39;00m \u001b[39mimport\u001b[39;00m ast_to_ttir\n\u001b[1;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpathlib\u001b[39;00m \u001b[39mimport\u001b[39;00m Path\n\u001b[1;32m     15\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mre\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/DAI/lib/python3.10/site-packages/triton/compiler/code_generator.py:212\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mvisit_Call\u001b[39m(\u001b[39mself\u001b[39m, node: ast\u001b[39m.\u001b[39mCall) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[1;32m    209\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvisit(node\u001b[39m.\u001b[39mfunc)\n\u001b[0;32m--> 212\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mCodeGenerator\u001b[39;00m(ast\u001b[39m.\u001b[39mNodeVisitor):\n\u001b[1;32m    214\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, context, prototype, gscope, attributes, constants, function_name, jit_fn: JITFunction, options,\n\u001b[1;32m    215\u001b[0m                  codegen_fns, debug\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, module\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, is_kernel\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, function_types: Optional[Dict] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    216\u001b[0m                  noinline\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, file_name: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, begin_line\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[1;32m    217\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontext \u001b[39m=\u001b[39m context\n",
      "File \u001b[0;32m~/.conda/envs/DAI/lib/python3.10/site-packages/triton/compiler/code_generator.py:254\u001b[0m, in \u001b[0;36mCodeGenerator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvisiting_arg_default_value \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    252\u001b[0m builtin_namespace: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m {_\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m: _ \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m (\u001b[39mlen\u001b[39m, \u001b[39mlist\u001b[39m, \u001b[39mrange\u001b[39m, \u001b[39mfloat\u001b[39m, \u001b[39mint\u001b[39m, \u001b[39misinstance\u001b[39m, \u001b[39mgetattr\u001b[39m)}\n\u001b[1;32m    253\u001b[0m builtin_namespace\u001b[39m.\u001b[39mupdate((\n\u001b[0;32m--> 254\u001b[0m     (\u001b[39m'\u001b[39m\u001b[39mprint\u001b[39m\u001b[39m'\u001b[39m, language\u001b[39m.\u001b[39;49mcore\u001b[39m.\u001b[39mdevice_print),\n\u001b[1;32m    255\u001b[0m     (\u001b[39m'\u001b[39m\u001b[39mmin\u001b[39m\u001b[39m'\u001b[39m, language\u001b[39m.\u001b[39mminimum),\n\u001b[1;32m    256\u001b[0m     (\u001b[39m'\u001b[39m\u001b[39mmax\u001b[39m\u001b[39m'\u001b[39m, language\u001b[39m.\u001b[39mmaximum),\n\u001b[1;32m    257\u001b[0m ))\n\u001b[1;32m    259\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_unsupported\u001b[39m(\u001b[39mself\u001b[39m, node, message):\n\u001b[1;32m    260\u001b[0m     \u001b[39mreturn\u001b[39;00m UnsupportedLanguageConstruct(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjit_fn\u001b[39m.\u001b[39msrc, node, message)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'triton.language' has no attribute 'core'"
     ]
    }
   ],
   "source": [
    "file_name = [\"First_200.csv\", \"NADIcombined_cleaned_MULTI_LABEL_MODIFIED_FINAL.csv\", \"First_1000.csv\", \"balanced_multilabel_dataset.csv\", \"balanced_multilabel_dataset_500.csv\"]\n",
    "# dataset_path = f\"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/our_data/{file_name[4]}\"\n",
    "dataset_path = '/home/ali.mekky/Documents/NLP/Project/Cross-Country-Dialectal-Arabic-Identification/balanced_multilabel_dataset_lr_500.csv'\n",
    "dev_path = \"/home/ali.mekky/Documents/NLP/Project/NADI2024/subtask1/dev/NADI2024_subtask1_dev2.tsv\"\n",
    "labels = ['Algeria', 'Bahrain', 'Egypt', 'Iraq', 'Jordan', 'Kuwait',\n",
    "       'Lebanon', 'Libya', 'Morocco', 'Oman', 'Palestine', 'Qatar',\n",
    "       'Saudi_Arabia', 'Sudan', 'Syria', 'Tunisia', 'UAE', 'Yemen']\n",
    "trainer = BertTrainer(\n",
    "    training_dataset_path=dataset_path,\n",
    "    model_name=\"/home/ali.mekky/Documents/NLP/Project/Cross-Country-Dialectal-Arabic-Identification/exp_20/marbert_finetuned_epochs_10_eval_f1_0.8621_greater_threshold_0.3\",\n",
    "    labels=labels,\n",
    "    threshold=0.3,\n",
    "    exp_num=21\n",
    ")\n",
    "trainer.train(\n",
    "    num_train_epochs=20,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    "    per_device_train_batch_size=24,\n",
    "    per_device_eval_batch_size=24,\n",
    ")\n",
    "trainer.evaluate(dev_path=dev_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OVERALL SCORES:\n",
      "MACRO AVERAGE PRECISION SCORE: 53.12 %\n",
      "MACRO AVERAGE RECALL SCORE: 64.70 %\n",
      "MACRO AVERAGE F1-SCORE: 55.99 %\n",
      "MACRO AVERAGE ACCURACY: 64.48 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "scorer_script = \"/home/ali.mekky/Documents/NLP/Project/NADI2024/subtask1/NADI2024-ST1-Scorer.py\"\n",
    "gold_file = \"/home/ali.mekky/Documents/NLP/Project/NADI2024/subtask1/sample_submission/NADI2024_subtask1_dev2_gold.txt\"\n",
    "predictions_file = \"/home/ali.mekky/Documents/NLP/Project/Cross-Country-Dialectal-Arabic-Identification/exp_20/marbert_finetuned_epochs_20_eval_f1_0.8577_greater_threshold_0.3/-home-ali.mekky-Documents-NLP-Project-Cross-Country-Dialectal-Arabic-Identification-exp_20-marbert_finetuned_epochs_10_eval_f1_0.8621_greater_threshold_0.3-experiment-20_predictions.txt\"\n",
    "!python3 \"{scorer_script}\" \"{gold_file}\" \"{predictions_file}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXP 21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "balancing the dataset produced by the logistic regression and finetuning camelbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at CAMeL-Lab/bert-base-arabic-camelbert-ca and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/ali.mekky/.conda/envs/DAI/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "                                                   \n",
      "  5%|▌         | 319/6380 [00:30<07:29, 13.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5511866211891174, 'eval_f1': 0.7437626750931472, 'eval_roc_auc': 0.6177656031477913, 'eval_accuracy': 0.07529411764705882, 'eval_runtime': 0.7482, 'eval_samples_per_second': 1136.014, 'eval_steps_per_second': 48.114, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 501/6380 [00:47<07:57, 12.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5907, 'grad_norm': 6.361403942108154, 'learning_rate': 4.96e-05, 'epoch': 1.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 10%|█         | 638/6380 [00:59<07:41, 12.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.49951958656311035, 'eval_f1': 0.7718301581860032, 'eval_roc_auc': 0.6728528798889093, 'eval_accuracy': 0.08235294117647059, 'eval_runtime': 0.5506, 'eval_samples_per_second': 1543.774, 'eval_steps_per_second': 65.383, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 15%|█▌        | 957/6380 [01:28<07:26, 12.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4561315178871155, 'eval_f1': 0.8000838662333578, 'eval_roc_auc': 0.7355714535748665, 'eval_accuracy': 0.0988235294117647, 'eval_runtime': 0.5717, 'eval_samples_per_second': 1486.689, 'eval_steps_per_second': 62.966, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 1001/6380 [01:34<07:38, 11.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4842, 'grad_norm': 1.7892683744430542, 'learning_rate': 4.5782312925170066e-05, 'epoch': 3.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 20%|██        | 1276/6380 [01:57<07:04, 12.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4299485385417938, 'eval_f1': 0.8184115126881248, 'eval_roc_auc': 0.7736265977137893, 'eval_accuracy': 0.10235294117647059, 'eval_runtime': 0.5718, 'eval_samples_per_second': 1486.479, 'eval_steps_per_second': 62.957, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▎       | 1501/6380 [02:18<06:41, 12.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3728, 'grad_norm': 1.6473968029022217, 'learning_rate': 4.153061224489796e-05, 'epoch': 4.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 25%|██▌       | 1595/6380 [02:26<06:27, 12.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.41549763083457947, 'eval_f1': 0.8282073813708261, 'eval_roc_auc': 0.7853263786034731, 'eval_accuracy': 0.11058823529411765, 'eval_runtime': 0.5629, 'eval_samples_per_second': 1510.111, 'eval_steps_per_second': 63.958, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 30%|███       | 1914/6380 [02:55<06:11, 12.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.41730809211730957, 'eval_f1': 0.8345218709641192, 'eval_roc_auc': 0.7993547225951115, 'eval_accuracy': 0.12117647058823529, 'eval_runtime': 0.5703, 'eval_samples_per_second': 1490.544, 'eval_steps_per_second': 63.129, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 2001/6380 [03:05<06:09, 11.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2875, 'grad_norm': 1.2916576862335205, 'learning_rate': 3.727891156462585e-05, 'epoch': 6.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 35%|███▌      | 2233/6380 [03:25<05:33, 12.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.41028809547424316, 'eval_f1': 0.836555163332773, 'eval_roc_auc': 0.8011264284688202, 'eval_accuracy': 0.11647058823529412, 'eval_runtime': 0.5653, 'eval_samples_per_second': 1503.722, 'eval_steps_per_second': 63.687, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 2501/6380 [03:49<05:24, 11.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2105, 'grad_norm': 1.6996122598648071, 'learning_rate': 3.302721088435374e-05, 'epoch': 7.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 40%|████      | 2552/6380 [03:54<05:15, 12.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4211941063404083, 'eval_f1': 0.8384971808184001, 'eval_roc_auc': 0.8023545896758918, 'eval_accuracy': 0.12705882352941175, 'eval_runtime': 0.5642, 'eval_samples_per_second': 1506.544, 'eval_steps_per_second': 63.807, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 45%|████▌     | 2871/6380 [04:23<04:45, 12.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.43933889269828796, 'eval_f1': 0.8359919119299034, 'eval_roc_auc': 0.8011575943107271, 'eval_accuracy': 0.1188235294117647, 'eval_runtime': 0.5867, 'eval_samples_per_second': 1448.751, 'eval_steps_per_second': 61.359, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 3001/6380 [04:37<04:42, 11.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1482, 'grad_norm': 1.660212516784668, 'learning_rate': 2.8775510204081635e-05, 'epoch': 9.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 50%|█████     | 3190/6380 [04:53<04:22, 12.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4412885010242462, 'eval_f1': 0.8401271715680708, 'eval_roc_auc': 0.8090094257064121, 'eval_accuracy': 0.13058823529411764, 'eval_runtime': 0.5685, 'eval_samples_per_second': 1495.264, 'eval_steps_per_second': 63.329, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 3501/6380 [05:21<03:58, 12.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1047, 'grad_norm': 1.1694947481155396, 'learning_rate': 2.4523809523809523e-05, 'epoch': 10.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 55%|█████▌    | 3509/6380 [05:22<03:52, 12.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.46076735854148865, 'eval_f1': 0.8412990901762064, 'eval_roc_auc': 0.8142748018182688, 'eval_accuracy': 0.1388235294117647, 'eval_runtime': 0.5929, 'eval_samples_per_second': 1433.657, 'eval_steps_per_second': 60.72, 'epoch': 11.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 60%|██████    | 3828/6380 [05:52<03:33, 11.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4691307246685028, 'eval_f1': 0.8411923496640055, 'eval_roc_auc': 0.8134435407038278, 'eval_accuracy': 0.1411764705882353, 'eval_runtime': 0.578, 'eval_samples_per_second': 1470.688, 'eval_steps_per_second': 62.288, 'epoch': 12.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 4001/6380 [06:08<03:18, 11.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0726, 'grad_norm': 1.3389629125595093, 'learning_rate': 2.0272108843537416e-05, 'epoch': 12.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 65%|██████▌   | 4147/6380 [06:21<03:01, 12.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.48363080620765686, 'eval_f1': 0.8430549968392621, 'eval_roc_auc': 0.815733507696931, 'eval_accuracy': 0.14941176470588236, 'eval_runtime': 0.5698, 'eval_samples_per_second': 1491.861, 'eval_steps_per_second': 63.185, 'epoch': 13.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 70%|███████   | 4466/6380 [06:50<02:39, 12.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4931025505065918, 'eval_f1': 0.843399930707934, 'eval_roc_auc': 0.8174246158139197, 'eval_accuracy': 0.15764705882352942, 'eval_runtime': 0.5832, 'eval_samples_per_second': 1457.583, 'eval_steps_per_second': 61.733, 'epoch': 14.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 4501/6380 [06:55<02:38, 11.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0538, 'grad_norm': 1.497381567955017, 'learning_rate': 1.6020408163265308e-05, 'epoch': 14.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 75%|███████▌  | 4785/6380 [07:20<02:09, 12.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5058793425559998, 'eval_f1': 0.844122756224667, 'eval_roc_auc': 0.8189954980838167, 'eval_accuracy': 0.14705882352941177, 'eval_runtime': 0.5678, 'eval_samples_per_second': 1497.078, 'eval_steps_per_second': 63.406, 'epoch': 15.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 5001/6380 [07:40<01:55, 11.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0398, 'grad_norm': 0.752790093421936, 'learning_rate': 1.1768707482993198e-05, 'epoch': 15.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 80%|████████  | 5104/6380 [07:49<01:45, 12.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5167987942695618, 'eval_f1': 0.8434346346808143, 'eval_roc_auc': 0.8171050111371445, 'eval_accuracy': 0.14705882352941177, 'eval_runtime': 0.5673, 'eval_samples_per_second': 1498.223, 'eval_steps_per_second': 63.454, 'epoch': 16.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 85%|████████▌ | 5423/6380 [08:18<01:17, 12.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5189736485481262, 'eval_f1': 0.8436211602113698, 'eval_roc_auc': 0.8191901297985077, 'eval_accuracy': 0.14823529411764705, 'eval_runtime': 0.593, 'eval_samples_per_second': 1433.499, 'eval_steps_per_second': 60.713, 'epoch': 17.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 5501/6380 [08:27<01:13, 11.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0323, 'grad_norm': 0.48990684747695923, 'learning_rate': 7.5170068027210886e-06, 'epoch': 17.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 90%|█████████ | 5742/6380 [08:48<00:52, 12.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5246774554252625, 'eval_f1': 0.8456507899762717, 'eval_roc_auc': 0.8205923862880203, 'eval_accuracy': 0.14352941176470588, 'eval_runtime': 0.5655, 'eval_samples_per_second': 1503.036, 'eval_steps_per_second': 63.658, 'epoch': 18.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 6001/6380 [09:09<00:28, 13.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0271, 'grad_norm': 0.5851431488990784, 'learning_rate': 3.26530612244898e-06, 'epoch': 18.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 95%|█████████▌| 6061/6380 [09:14<00:22, 13.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5259179472923279, 'eval_f1': 0.8454016298020954, 'eval_roc_auc': 0.8218422191070139, 'eval_accuracy': 0.14823529411764705, 'eval_runtime': 0.507, 'eval_samples_per_second': 1676.544, 'eval_steps_per_second': 71.007, 'epoch': 19.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|██████████| 6380/6380 [09:42<00:00, 13.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5265424847602844, 'eval_f1': 0.8449036452286974, 'eval_roc_auc': 0.8205344921247426, 'eval_accuracy': 0.14941176470588236, 'eval_runtime': 0.5301, 'eval_samples_per_second': 1603.392, 'eval_steps_per_second': 67.908, 'epoch': 20.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6380/6380 [09:45<00:00, 10.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 585.0119, 'train_samples_per_second': 261.533, 'train_steps_per_second': 10.906, 'train_loss': 0.19146193889988627, 'epoch': 20.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_845921/3956158625.py:82: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset Accuracy: 0.1000\n",
      "Hamming Loss: 0.3635\n",
      "Micro Precision: 0.5081\n",
      "Micro Recall: 0.6152\n",
      "Micro F1-Score: 0.5565\n",
      "Precision per label: [0.42105263 0.50909091 0.49152542 0.62745098 0.65517241 0.53030303\n",
      " 0.29787234 0.56716418]\n",
      "Recall per label: [0.68571429 0.71794872 0.58       0.5        0.45238095 0.76086957\n",
      " 0.66666667 0.6440678 ]\n",
      "F1-Score per label: [0.52173913 0.59574468 0.53211009 0.55652174 0.53521127 0.625\n",
      " 0.41176471 0.6031746 ]\n",
      "{0, 1, 2, 3, 4, 5, 6, 7, 8}\n"
     ]
    }
   ],
   "source": [
    "file_name = [\"First_200.csv\", \"NADIcombined_cleaned_MULTI_LABEL_MODIFIED_FINAL.csv\", \"First_1000.csv\", \"balanced_multilabel_dataset.csv\", \"balanced_multilabel_dataset_500.csv\"]\n",
    "# dataset_path = f\"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/our_data/{file_name[4]}\"\n",
    "dataset_path = '/home/ali.mekky/Documents/NLP/Project/Cross-Country-Dialectal-Arabic-Identification/balanced_multilabel_dataset_lr_500.csv'\n",
    "dev_path = \"/home/ali.mekky/Documents/NLP/Project/NADI2024/subtask1/dev/NADI2024_subtask1_dev2.tsv\"\n",
    "labels = ['Algeria', 'Bahrain', 'Egypt', 'Iraq', 'Jordan', 'Kuwait',\n",
    "       'Lebanon', 'Libya', 'Morocco', 'Oman', 'Palestine', 'Qatar',\n",
    "       'Saudi_Arabia', 'Sudan', 'Syria', 'Tunisia', 'UAE', 'Yemen']\n",
    "trainer = BertTrainer(\n",
    "    training_dataset_path=dataset_path,\n",
    "    model_name=\"CAMeL-Lab/bert-base-arabic-camelbert-ca\",\n",
    "    labels=labels,\n",
    "    threshold=0.3,\n",
    "    exp_num=21\n",
    ")\n",
    "trainer.train(\n",
    "    num_train_epochs=20,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    "    per_device_train_batch_size=24,\n",
    "    per_device_eval_batch_size=24,\n",
    ")\n",
    "trainer.evaluate(dev_path=dev_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OVERALL SCORES:\n",
      "MACRO AVERAGE PRECISION SCORE: 51.25 %\n",
      "MACRO AVERAGE RECALL SCORE: 62.60 %\n",
      "MACRO AVERAGE F1-SCORE: 54.77 %\n",
      "MACRO AVERAGE ACCURACY: 63.65 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "scorer_script = \"/home/ali.mekky/Documents/NLP/Project/NADI2024/subtask1/NADI2024-ST1-Scorer.py\"\n",
    "gold_file = \"/home/ali.mekky/Documents/NLP/Project/NADI2024/subtask1/sample_submission/NADI2024_subtask1_dev2_gold.txt\"\n",
    "predictions_file = \"/home/ali.mekky/Documents/NLP/Project/Cross-Country-Dialectal-Arabic-Identification/exp_21/marbert_finetuned_epochs_20_eval_f1_0.8457_greater_threshold_0.3/CAMeL-Lab-bert-base-arabic-camelbert-ca-experiment-21_predictions.txt\"\n",
    "!python3 \"{scorer_script}\" \"{gold_file}\" \"{predictions_file}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXP 22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "freezing lower layers and adding dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 119/22040 [17:08<52:38:49,  8.65s/it]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at UBC-NLP/MARBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/ali.mekky/.conda/envs/DAI/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "  2%|▏         | 502/22040 [00:34<25:11, 14.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4872, 'grad_norm': 2.7567391395568848, 'learning_rate': 5e-05, 'epoch': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 1002/22040 [01:08<25:13, 13.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4106, 'grad_norm': 2.746913194656372, 'learning_rate': 5e-05, 'epoch': 0.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1502/22040 [01:43<23:29, 14.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3744, 'grad_norm': 3.761868953704834, 'learning_rate': 5e-05, 'epoch': 0.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 2002/22040 [02:17<22:55, 14.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3473, 'grad_norm': 3.082247495651245, 'learning_rate': 5e-05, 'epoch': 0.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2204/22040 [02:30<22:20, 14.80it/s]\n",
      " 10%|█         | 2204/22040 [02:35<22:20, 14.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.31659334897994995, 'eval_f1': 0.7849032858198134, 'eval_roc_auc': 0.8475967977783262, 'eval_accuracy': 0.13850604049685214, 'eval_runtime': 4.8319, 'eval_samples_per_second': 1216.299, 'eval_steps_per_second': 50.705, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 2502/22040 [02:59<22:55, 14.20it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3059, 'grad_norm': 4.013148307800293, 'learning_rate': 5e-05, 'epoch': 1.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 3002/22040 [03:34<22:17, 14.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2746, 'grad_norm': 4.039422512054443, 'learning_rate': 5e-05, 'epoch': 1.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 3502/22040 [04:09<21:44, 14.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2628, 'grad_norm': 3.6592023372650146, 'learning_rate': 5e-05, 'epoch': 1.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 4002/22040 [04:44<21:14, 14.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2578, 'grad_norm': 3.9496212005615234, 'learning_rate': 5e-05, 'epoch': 1.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4408/22040 [05:13<20:41, 14.21it/s]\n",
      " 20%|██        | 4408/22040 [05:18<20:41, 14.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2645597755908966, 'eval_f1': 0.8199577972908584, 'eval_roc_auc': 0.8758989645539288, 'eval_accuracy': 0.158924621405479, 'eval_runtime': 4.9064, 'eval_samples_per_second': 1197.82, 'eval_steps_per_second': 49.935, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4502/22040 [05:27<21:32, 13.57it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.24, 'grad_norm': 3.2590744495391846, 'learning_rate': 5e-05, 'epoch': 2.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 5002/22040 [06:02<20:04, 14.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.186, 'grad_norm': 3.98660945892334, 'learning_rate': 5e-05, 'epoch': 2.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 5502/22040 [06:37<17:24, 15.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.183, 'grad_norm': 4.314870834350586, 'learning_rate': 5e-05, 'epoch': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 6002/22040 [07:08<16:56, 15.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.183, 'grad_norm': 5.021315574645996, 'learning_rate': 5e-05, 'epoch': 2.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 6502/22040 [07:40<16:21, 15.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1821, 'grad_norm': 4.458516597747803, 'learning_rate': 5e-05, 'epoch': 2.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6612/22040 [07:47<16:12, 15.87it/s]\n",
      " 30%|███       | 6612/22040 [07:51<16:12, 15.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.25569793581962585, 'eval_f1': 0.8340886203423967, 'eval_roc_auc': 0.8831668167738843, 'eval_accuracy': 0.19669899608643865, 'eval_runtime': 4.4823, 'eval_samples_per_second': 1311.148, 'eval_steps_per_second': 54.659, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 7002/22040 [08:19<15:44, 15.91it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1389, 'grad_norm': 3.714444875717163, 'learning_rate': 5e-05, 'epoch': 3.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 7502/22040 [08:50<15:16, 15.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1241, 'grad_norm': 4.097795009613037, 'learning_rate': 5e-05, 'epoch': 3.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 8002/22040 [09:21<14:49, 15.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1275, 'grad_norm': 3.6288466453552246, 'learning_rate': 5e-05, 'epoch': 3.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▊      | 8502/22040 [09:53<14:18, 15.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1291, 'grad_norm': 3.088449478149414, 'learning_rate': 5e-05, 'epoch': 3.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8816/22040 [10:12<13:49, 15.94it/s]\n",
      " 40%|████      | 8816/22040 [10:17<13:49, 15.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.27678731083869934, 'eval_f1': 0.8337880728325631, 'eval_roc_auc': 0.8836161360711593, 'eval_accuracy': 0.19550791220010208, 'eval_runtime': 4.4663, 'eval_samples_per_second': 1315.845, 'eval_steps_per_second': 54.855, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 9002/22040 [10:31<13:43, 15.84it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1122, 'grad_norm': 2.8917648792266846, 'learning_rate': 5e-05, 'epoch': 4.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 9502/22040 [11:02<13:13, 15.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0862, 'grad_norm': 3.912954330444336, 'learning_rate': 5e-05, 'epoch': 4.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 10002/22040 [11:34<12:55, 15.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0872, 'grad_norm': 2.9539389610290527, 'learning_rate': 5e-05, 'epoch': 4.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 10502/22040 [12:05<12:12, 15.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0885, 'grad_norm': 4.532630920410156, 'learning_rate': 5e-05, 'epoch': 4.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 11002/22040 [12:36<11:37, 15.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.094, 'grad_norm': 4.640472888946533, 'learning_rate': 5e-05, 'epoch': 4.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 11020/22040 [12:38<11:44, 15.63it/s]\n",
      " 50%|█████     | 11020/22040 [12:42<11:44, 15.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.31265273690223694, 'eval_f1': 0.8298505777940823, 'eval_roc_auc': 0.8786760389194355, 'eval_accuracy': 0.18291645397311554, 'eval_runtime': 4.5224, 'eval_samples_per_second': 1299.54, 'eval_steps_per_second': 54.175, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 11020/22040 [12:46<12:46, 14.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 766.6979, 'train_samples_per_second': 689.854, 'train_steps_per_second': 28.747, 'train_loss': 0.21260312045983523, 'epoch': 5.0}\n",
      "Subset Accuracy: 0.1000\n",
      "Hamming Loss: 0.3156\n",
      "Micro Precision: 0.5692\n",
      "Micro Recall: 0.6124\n",
      "Micro F1-Score: 0.5900\n",
      "Precision per label: [0.54761905 0.71052632 0.54716981 0.64814815 0.73913043 0.53731343\n",
      " 0.34782609 0.58333333]\n",
      "Recall per label: [0.65714286 0.69230769 0.58       0.546875   0.4047619  0.7826087\n",
      " 0.76190476 0.59322034]\n",
      "F1-Score per label: [0.5974026  0.7012987  0.5631068  0.59322034 0.52307692 0.63716814\n",
      " 0.47761194 0.58823529]\n",
      "{0, 1, 2, 3, 4, 5, 6, 7, 8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_875058/1135678770.py:76: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    }
   ],
   "source": [
    "file_name = [\"First_200.csv\", \"NADIcombined_cleaned_MULTI_LABEL_MODIFIED_FINAL.csv\", \"First_1000.csv\", \"balanced_multilabel_dataset.csv\", \"balanced_multilabel_dataset_500.csv\"]\n",
    "# dataset_path = f\"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/our_data/{file_name[4]}\"\n",
    "dataset_path = '/home/ali.mekky/Documents/NLP/Project/Cross-Country-Dialectal-Arabic-Identification/lr_binary_classifiers/annotated_multi_label_logisitc_regression.csv'\n",
    "dev_path = \"/home/ali.mekky/Documents/NLP/Project/NADI2024/subtask1/dev/NADI2024_subtask1_dev2.tsv\"\n",
    "labels = ['Algeria', 'Bahrain', 'Egypt', 'Iraq', 'Jordan', 'Kuwait',\n",
    "       'Lebanon', 'Libya', 'Morocco', 'Oman', 'Palestine', 'Qatar',\n",
    "       'Saudi_Arabia', 'Sudan', 'Syria', 'Tunisia', 'UAE', 'Yemen']\n",
    "trainer = BertTrainer(\n",
    "    training_dataset_path=dataset_path,\n",
    "    model_name=\"UBC-NLP/MARBERT\",\n",
    "    labels=labels,\n",
    "    threshold=0.3,\n",
    "    exp_num=22\n",
    ")\n",
    "trainer.train(\n",
    "    num_train_epochs=10,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    "    per_device_train_batch_size=24,\n",
    "    per_device_eval_batch_size=24,\n",
    ")\n",
    "trainer.evaluate(dev_path=dev_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OVERALL SCORES:\n",
      "MACRO AVERAGE PRECISION SCORE: 58.26 %\n",
      "MACRO AVERAGE RECALL SCORE: 62.74 %\n",
      "MACRO AVERAGE F1-SCORE: 58.51 %\n",
      "MACRO AVERAGE ACCURACY: 68.44 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "scorer_script = \"/home/ali.mekky/Documents/NLP/Project/NADI2024/subtask1/NADI2024-ST1-Scorer.py\"\n",
    "gold_file = \"/home/ali.mekky/Documents/NLP/Project/NADI2024/subtask1/sample_submission/NADI2024_subtask1_dev2_gold.txt\"\n",
    "predictions_file = \"/home/ali.mekky/Documents/NLP/Project/Cross-Country-Dialectal-Arabic-Identification/exp_22/marbert_finetuned_epochs_10_eval_f1_0.8341_greater_threshold_0.3/UBC-NLP-MARBERT-experiment-22_predictions.txt\"\n",
    "!python3 \"{scorer_script}\" \"{gold_file}\" \"{predictions_file}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXP 23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "same as above but using camelbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at CAMeL-Lab/bert-base-arabic-camelbert-ca and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/ali.mekky/.conda/envs/DAI/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "  2%|▏         | 502/22040 [00:28<20:15, 17.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5649, 'grad_norm': 1.48406183719635, 'learning_rate': 5e-05, 'epoch': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 1002/22040 [00:57<20:23, 17.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4982, 'grad_norm': 1.0999702215194702, 'learning_rate': 5e-05, 'epoch': 0.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1502/22040 [01:26<19:48, 17.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4645, 'grad_norm': 1.0644991397857666, 'learning_rate': 5e-05, 'epoch': 0.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 2002/22040 [01:55<19:24, 17.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4377, 'grad_norm': 1.2332335710525513, 'learning_rate': 5e-05, 'epoch': 0.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2204/22040 [02:07<19:18, 17.13it/s]\n",
      " 10%|█         | 2204/22040 [02:12<19:18, 17.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4055422842502594, 'eval_f1': 0.7099067198689322, 'eval_roc_auc': 0.7901697753917134, 'eval_accuracy': 0.0781010719754977, 'eval_runtime': 4.9592, 'eval_samples_per_second': 1185.07, 'eval_steps_per_second': 49.403, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 2502/22040 [02:31<19:16, 16.90it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4105, 'grad_norm': 0.9981072545051575, 'learning_rate': 5e-05, 'epoch': 1.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 3002/22040 [03:00<19:02, 16.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3896, 'grad_norm': 2.099914312362671, 'learning_rate': 5e-05, 'epoch': 1.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 3502/22040 [03:30<18:27, 16.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.376, 'grad_norm': 1.4147392511367798, 'learning_rate': 5e-05, 'epoch': 1.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 4002/22040 [03:59<17:56, 16.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3704, 'grad_norm': 2.7498104572296143, 'learning_rate': 5e-05, 'epoch': 1.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4408/22040 [04:23<17:39, 16.64it/s]\n",
      " 20%|██        | 4408/22040 [04:28<17:39, 16.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3480108380317688, 'eval_f1': 0.7540106951871658, 'eval_roc_auc': 0.8264540125901423, 'eval_accuracy': 0.09783903352050366, 'eval_runtime': 4.9726, 'eval_samples_per_second': 1181.87, 'eval_steps_per_second': 49.27, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4502/22040 [04:35<17:27, 16.74it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3528, 'grad_norm': 1.365309238433838, 'learning_rate': 5e-05, 'epoch': 2.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 5002/22040 [05:05<16:57, 16.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3264, 'grad_norm': 1.3290443420410156, 'learning_rate': 5e-05, 'epoch': 2.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 5502/22040 [05:35<16:46, 16.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3218, 'grad_norm': 2.5377919673919678, 'learning_rate': 5e-05, 'epoch': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 6002/22040 [06:05<16:00, 16.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3147, 'grad_norm': 1.8217381238937378, 'learning_rate': 5e-05, 'epoch': 2.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 6502/22040 [06:34<15:43, 16.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3112, 'grad_norm': 1.522733211517334, 'learning_rate': 5e-05, 'epoch': 2.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6612/22040 [06:41<16:10, 15.89it/s]\n",
      " 30%|███       | 6612/22040 [06:46<16:10, 15.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.32097285985946655, 'eval_f1': 0.782281024953944, 'eval_roc_auc': 0.8434294707617457, 'eval_accuracy': 0.12948783392887528, 'eval_runtime': 5.0717, 'eval_samples_per_second': 1158.791, 'eval_steps_per_second': 48.308, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 7002/22040 [07:12<14:59, 16.73it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2837, 'grad_norm': 1.4617409706115723, 'learning_rate': 5e-05, 'epoch': 3.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 7502/22040 [07:41<14:41, 16.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.273, 'grad_norm': 1.5124166011810303, 'learning_rate': 5e-05, 'epoch': 3.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 8002/22040 [08:11<14:29, 16.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2737, 'grad_norm': 1.4198979139328003, 'learning_rate': 5e-05, 'epoch': 3.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▊      | 8502/22040 [08:41<13:24, 16.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.269, 'grad_norm': 1.253678560256958, 'learning_rate': 5e-05, 'epoch': 3.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8816/22040 [08:59<13:15, 16.62it/s]\n",
      " 40%|████      | 8816/22040 [09:04<13:15, 16.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2930040955543518, 'eval_f1': 0.7997309244666538, 'eval_roc_auc': 0.8588660718502263, 'eval_accuracy': 0.1524587374510805, 'eval_runtime': 5.032, 'eval_samples_per_second': 1167.917, 'eval_steps_per_second': 48.688, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 9002/22040 [09:17<13:05, 16.60it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2539, 'grad_norm': 1.4674493074417114, 'learning_rate': 5e-05, 'epoch': 4.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 9502/22040 [09:47<12:26, 16.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2358, 'grad_norm': 1.907118797302246, 'learning_rate': 5e-05, 'epoch': 4.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 10002/22040 [10:16<12:01, 16.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2363, 'grad_norm': 1.558811068534851, 'learning_rate': 5e-05, 'epoch': 4.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 10502/22040 [10:46<11:35, 16.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2318, 'grad_norm': 1.9798033237457275, 'learning_rate': 5e-05, 'epoch': 4.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 11002/22040 [11:16<10:59, 16.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2345, 'grad_norm': 1.7607308626174927, 'learning_rate': 5e-05, 'epoch': 4.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 11020/22040 [11:17<11:06, 16.55it/s]\n",
      " 50%|█████     | 11020/22040 [11:22<11:06, 16.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.28577277064323425, 'eval_f1': 0.8072094130449569, 'eval_roc_auc': 0.8658987054182624, 'eval_accuracy': 0.15705291815552153, 'eval_runtime': 5.0708, 'eval_samples_per_second': 1158.99, 'eval_steps_per_second': 48.316, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 11502/22040 [11:52<10:33, 16.62it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2022, 'grad_norm': 1.5855951309204102, 'learning_rate': 5e-05, 'epoch': 5.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 12002/22040 [12:22<09:59, 16.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2022, 'grad_norm': 1.5022097826004028, 'learning_rate': 5e-05, 'epoch': 5.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 12502/22040 [12:51<09:31, 16.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2038, 'grad_norm': 2.084074020385742, 'learning_rate': 5e-05, 'epoch': 5.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 13002/22040 [13:21<09:05, 16.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2048, 'grad_norm': 1.5258960723876953, 'learning_rate': 5e-05, 'epoch': 5.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 13223/22040 [13:33<07:56, 18.50it/s]\n",
      " 60%|██████    | 13224/22040 [13:37<07:56, 18.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2837856709957123, 'eval_f1': 0.8147543478561645, 'eval_roc_auc': 0.8696570737623799, 'eval_accuracy': 0.16743236345074017, 'eval_runtime': 4.5042, 'eval_samples_per_second': 1304.769, 'eval_steps_per_second': 54.393, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████▏   | 13503/22040 [13:54<07:34, 18.77it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1872, 'grad_norm': 1.551269769668579, 'learning_rate': 5e-05, 'epoch': 6.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▎   | 14002/22040 [14:20<07:12, 18.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1734, 'grad_norm': 1.4010202884674072, 'learning_rate': 5e-05, 'epoch': 6.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 14502/22040 [14:49<07:28, 16.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1775, 'grad_norm': 1.9627329111099243, 'learning_rate': 5e-05, 'epoch': 6.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 15002/22040 [15:19<07:03, 16.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1772, 'grad_norm': 2.12345027923584, 'learning_rate': 5e-05, 'epoch': 6.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 15428/22040 [15:45<06:38, 16.58it/s]\n",
      " 70%|███████   | 15428/22040 [15:50<06:38, 16.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2869652211666107, 'eval_f1': 0.8176449485332671, 'eval_roc_auc': 0.8722715311141812, 'eval_accuracy': 0.17083546026884464, 'eval_runtime': 5.0575, 'eval_samples_per_second': 1162.034, 'eval_steps_per_second': 48.443, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 15502/22040 [15:56<06:39, 16.38it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1732, 'grad_norm': 1.748171091079712, 'learning_rate': 5e-05, 'epoch': 7.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 16002/22040 [16:25<05:57, 16.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1461, 'grad_norm': 1.665237545967102, 'learning_rate': 5e-05, 'epoch': 7.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 16502/22040 [16:55<05:28, 16.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1513, 'grad_norm': 2.2277581691741943, 'learning_rate': 5e-05, 'epoch': 7.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 17002/22040 [17:24<05:02, 16.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1535, 'grad_norm': 1.8187083005905151, 'learning_rate': 5e-05, 'epoch': 7.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 17502/22040 [17:54<04:30, 16.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1539, 'grad_norm': 2.229557752609253, 'learning_rate': 5e-05, 'epoch': 7.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 17632/22040 [18:02<04:22, 16.82it/s]\n",
      " 80%|████████  | 17632/22040 [18:07<04:22, 16.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.29364240169525146, 'eval_f1': 0.8242024327041532, 'eval_roc_auc': 0.8738783607227875, 'eval_accuracy': 0.1970393057682491, 'eval_runtime': 5.0136, 'eval_samples_per_second': 1172.214, 'eval_steps_per_second': 48.867, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 18002/22040 [18:30<04:01, 16.74it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1374, 'grad_norm': 2.4780025482177734, 'learning_rate': 5e-05, 'epoch': 8.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 18502/22040 [19:00<03:30, 16.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1276, 'grad_norm': 1.5429753065109253, 'learning_rate': 5e-05, 'epoch': 8.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 19002/22040 [19:29<03:01, 16.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1318, 'grad_norm': 1.8818618059158325, 'learning_rate': 5e-05, 'epoch': 8.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 19502/22040 [19:59<02:30, 16.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1332, 'grad_norm': 1.835778832435608, 'learning_rate': 5e-05, 'epoch': 8.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 19836/22040 [20:18<02:09, 16.98it/s]\n",
      " 90%|█████████ | 19836/22040 [20:23<02:09, 16.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.30559656023979187, 'eval_f1': 0.8216548012780985, 'eval_roc_auc': 0.8733135069799041, 'eval_accuracy': 0.18478815722307299, 'eval_runtime': 4.9725, 'eval_samples_per_second': 1181.903, 'eval_steps_per_second': 49.271, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 20002/22040 [20:35<02:00, 16.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1266, 'grad_norm': 2.3318207263946533, 'learning_rate': 5e-05, 'epoch': 9.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 20502/22040 [21:04<01:30, 16.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1119, 'grad_norm': 2.0546348094940186, 'learning_rate': 5e-05, 'epoch': 9.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 21002/22040 [21:35<01:05, 15.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.114, 'grad_norm': 2.1959190368652344, 'learning_rate': 5e-05, 'epoch': 9.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 21502/22040 [22:07<00:33, 16.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1166, 'grad_norm': 1.897908091545105, 'learning_rate': 5e-05, 'epoch': 9.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 22002/22040 [22:36<00:02, 16.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1172, 'grad_norm': 2.7256805896759033, 'learning_rate': 5e-05, 'epoch': 9.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22040/22040 [22:39<00:00, 16.71it/s]\n",
      "100%|██████████| 22040/22040 [22:45<00:00, 16.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3176794946193695, 'eval_f1': 0.8254703494024133, 'eval_roc_auc': 0.8735565425375277, 'eval_accuracy': 0.19499744767738642, 'eval_runtime': 4.9762, 'eval_samples_per_second': 1181.022, 'eval_steps_per_second': 49.234, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22040/22040 [22:47<00:00, 16.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1367.1415, 'train_samples_per_second': 386.873, 'train_steps_per_second': 16.121, 'train_loss': 0.24643782885667417, 'epoch': 10.0}\n",
      "Subset Accuracy: 0.1083\n",
      "Hamming Loss: 0.3344\n",
      "Micro Precision: 0.5482\n",
      "Micro Recall: 0.5590\n",
      "Micro F1-Score: 0.5535\n",
      "Precision per label: [0.54545455 0.59574468 0.57777778 0.61702128 0.64       0.51612903\n",
      " 0.35714286 0.56862745]\n",
      "Recall per label: [0.68571429 0.71794872 0.52       0.453125   0.38095238 0.69565217\n",
      " 0.71428571 0.49152542]\n",
      "F1-Score per label: [0.60759494 0.65116279 0.54736842 0.52252252 0.47761194 0.59259259\n",
      " 0.47619048 0.52727273]\n",
      "{0, 1, 2, 3, 4, 5, 6, 7, 8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_875058/1135678770.py:76: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    }
   ],
   "source": [
    "file_name = [\"First_200.csv\", \"NADIcombined_cleaned_MULTI_LABEL_MODIFIED_FINAL.csv\", \"First_1000.csv\", \"balanced_multilabel_dataset.csv\", \"balanced_multilabel_dataset_500.csv\"]\n",
    "# dataset_path = f\"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/our_data/{file_name[4]}\"\n",
    "dataset_path = '/home/ali.mekky/Documents/NLP/Project/Cross-Country-Dialectal-Arabic-Identification/lr_binary_classifiers/annotated_multi_label_logisitc_regression.csv'\n",
    "dev_path = \"/home/ali.mekky/Documents/NLP/Project/NADI2024/subtask1/dev/NADI2024_subtask1_dev2.tsv\"\n",
    "labels = ['Algeria', 'Bahrain', 'Egypt', 'Iraq', 'Jordan', 'Kuwait',\n",
    "       'Lebanon', 'Libya', 'Morocco', 'Oman', 'Palestine', 'Qatar',\n",
    "       'Saudi_Arabia', 'Sudan', 'Syria', 'Tunisia', 'UAE', 'Yemen']\n",
    "trainer = BertTrainer(\n",
    "    training_dataset_path=dataset_path,\n",
    "    model_name=\"CAMeL-Lab/bert-base-arabic-camelbert-ca\",\n",
    "    labels=labels,\n",
    "    threshold=0.3,\n",
    "    exp_num=23\n",
    ")\n",
    "trainer.train(\n",
    "    num_train_epochs=10,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    "    per_device_train_batch_size=24,\n",
    "    per_device_eval_batch_size=24,\n",
    ")\n",
    "trainer.evaluate(dev_path=dev_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OVERALL SCORES:\n",
      "MACRO AVERAGE PRECISION SCORE: 55.22 %\n",
      "MACRO AVERAGE RECALL SCORE: 58.24 %\n",
      "MACRO AVERAGE F1-SCORE: 55.03 %\n",
      "MACRO AVERAGE ACCURACY: 66.56 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "scorer_script = \"/home/ali.mekky/Documents/NLP/Project/NADI2024/subtask1/NADI2024-ST1-Scorer.py\"\n",
    "gold_file = \"/home/ali.mekky/Documents/NLP/Project/NADI2024/subtask1/sample_submission/NADI2024_subtask1_dev2_gold.txt\"\n",
    "predictions_file = \"/home/ali.mekky/Documents/NLP/Project/Cross-Country-Dialectal-Arabic-Identification/exp_23/marbert_finetuned_epochs_10_eval_f1_0.8255_greater_threshold_0.3/CAMeL-Lab-bert-base-arabic-camelbert-ca-experiment-23_predictions.txt\"\n",
    "!python3 \"{scorer_script}\" \"{gold_file}\" \"{predictions_file}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset Accuracy: 0.1000\n",
      "Hamming Loss: 0.6292\n",
      "Micro Precision: 0.3708\n",
      "Micro Recall: 1.0000\n",
      "Micro F1-Score: 0.5410\n",
      "Precision per label: [0.29166667 0.325      0.41666667 0.53333333 0.35       0.38333333\n",
      " 0.175      0.49166667]\n",
      "Recall per label: [1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "F1-Score per label: [0.4516129  0.49056604 0.58823529 0.69565217 0.51851852 0.55421687\n",
      " 0.29787234 0.65921788]\n",
      "{8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_875058/1135678770.py:76: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_replaced = dev.replace({'y': 1, 'n': 0})\n"
     ]
    }
   ],
   "source": [
    "file_name = [\"First_200.csv\", \"NADIcombined_cleaned_MULTI_LABEL_MODIFIED_FINAL.csv\", \"First_1000.csv\", \"balanced_multilabel_dataset.csv\", \"balanced_multilabel_dataset_500.csv\"]\n",
    "# dataset_path = f\"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/our_data/{file_name[4]}\"\n",
    "dataset_path = '/home/ali.mekky/Documents/NLP/Project/Cross-Country-Dialectal-Arabic-Identification/lr_binary_classifiers/annotated_multi_label_logisitc_regression.csv'\n",
    "dev_path = \"/home/ali.mekky/Documents/NLP/Project/NADI2024/subtask1/dev/NADI2024_subtask1_dev2.tsv\"\n",
    "labels = ['Algeria', 'Bahrain', 'Egypt', 'Iraq', 'Jordan', 'Kuwait',\n",
    "       'Lebanon', 'Libya', 'Morocco', 'Oman', 'Palestine', 'Qatar',\n",
    "       'Saudi_Arabia', 'Sudan', 'Syria', 'Tunisia', 'UAE', 'Yemen']\n",
    "\n",
    "trainer = BertTrainer(\n",
    "    training_dataset_path=dataset_path,\n",
    "    model_name=\"/home/ali.mekky/Documents/NLP/Project/Cross-Country-Dialectal-Arabic-Identification/exp_5/marbert_finetuned\",\n",
    "    labels=labels,\n",
    "    threshold=0.3,\n",
    "    exp_num=5\n",
    ")\n",
    "\n",
    "trainer.save_dir = f'./exp_{trainer.exp_num}'\n",
    "\n",
    "# trainer.train(\n",
    "#     num_train_epochs=10,\n",
    "#     metric_for_best_model=\"eval_f1\",\n",
    "#     greater_is_better=True,\n",
    "#     per_device_train_batch_size=24,\n",
    "#     per_device_eval_batch_size=24,\n",
    "# )\n",
    "trainer.evaluate(dev_path=dev_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OVERALL SCORES:\n",
      "MACRO AVERAGE PRECISION SCORE: 37.08 %\n",
      "MACRO AVERAGE RECALL SCORE: 100.00 %\n",
      "MACRO AVERAGE F1-SCORE: 53.20 %\n",
      "MACRO AVERAGE ACCURACY: 37.08 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "scorer_script = \"/home/ali.mekky/Documents/NLP/Project/NADI2024/subtask1/NADI2024-ST1-Scorer.py\"\n",
    "gold_file = \"/home/ali.mekky/Documents/NLP/Project/NADI2024/subtask1/sample_submission/NADI2024_subtask1_dev2_gold.txt\"\n",
    "predictions_file = \"/home/ali.mekky/Documents/NLP/Project/Cross-Country-Dialectal-Arabic-Identification/exp_5/-home-ali.mekky-Documents-NLP-Project-Cross-Country-Dialectal-Arabic-Identification-exp_5-marbert_finetuned-experiment-5_predictions.txt\"\n",
    "!python3 \"{scorer_script}\" \"{gold_file}\" \"{predictions_file}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## exp 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"/home/ali.mekky/Documents/NLP/Project/NADI2024/subtask1/multilabel/NADIcombined_cleaned_MULTI_LABEL_MODIFIED_FINAL.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset[dataset['Computed'] == 'yes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>Algeria</th>\n",
       "      <th>Bahrain</th>\n",
       "      <th>Egypt</th>\n",
       "      <th>Iraq</th>\n",
       "      <th>Jordan</th>\n",
       "      <th>Kuwait</th>\n",
       "      <th>Lebanon</th>\n",
       "      <th>Libya</th>\n",
       "      <th>...</th>\n",
       "      <th>Oman</th>\n",
       "      <th>Palestine</th>\n",
       "      <th>Qatar</th>\n",
       "      <th>Saudi_Arabia</th>\n",
       "      <th>Sudan</th>\n",
       "      <th>Syria</th>\n",
       "      <th>Tunisia</th>\n",
       "      <th>UAE</th>\n",
       "      <th>Yemen</th>\n",
       "      <th>Computed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>الفار العور يشوف فقط كيسي ومايشوف ماتويد</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ديني ربنا يستر</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>اساسا نسبكم قذر ونجس بلاش تتفاخروا بنجاستكم وه...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>المشاعر تحتاج الي المشاعر تحتاج الي رفيق يخذل ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>ني حاضرها لايف</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58742</th>\n",
       "      <td>58734</td>\n",
       "      <td>جماعه الخير المنخفض الي جاي ايام منخفض قوي ماط...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58743</th>\n",
       "      <td>58735</td>\n",
       "      <td>انا بايع الكل وشاري عيونك</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58744</th>\n",
       "      <td>58736</td>\n",
       "      <td>USER USER USER شكلها نست يوم ترامب قال للامريك...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58745</th>\n",
       "      <td>58737</td>\n",
       "      <td>السطلات البنقو الحشيش معاكم URL ΉМĄDĄ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58746</th>\n",
       "      <td>58738</td>\n",
       "      <td>USER USER USER ميرسي نيسو لعقوبه ليك الفانزالع...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31760 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              tweet  Algeria  \\\n",
       "0          0           الفار العور يشوف فقط كيسي ومايشوف ماتويد        0   \n",
       "1          1                                     ديني ربنا يستر        0   \n",
       "2          2  اساسا نسبكم قذر ونجس بلاش تتفاخروا بنجاستكم وه...        0   \n",
       "3          3  المشاعر تحتاج الي المشاعر تحتاج الي رفيق يخذل ...        0   \n",
       "4          4                                     ني حاضرها لايف        0   \n",
       "...      ...                                                ...      ...   \n",
       "58742  58734  جماعه الخير المنخفض الي جاي ايام منخفض قوي ماط...        0   \n",
       "58743  58735                          انا بايع الكل وشاري عيونك        0   \n",
       "58744  58736  USER USER USER شكلها نست يوم ترامب قال للامريك...        0   \n",
       "58745  58737              السطلات البنقو الحشيش معاكم URL ΉМĄDĄ        0   \n",
       "58746  58738  USER USER USER ميرسي نيسو لعقوبه ليك الفانزالع...        1   \n",
       "\n",
       "       Bahrain  Egypt  Iraq  Jordan  Kuwait  Lebanon  Libya  ...  Oman  \\\n",
       "0            0      0     0       0       0        0      1  ...     0   \n",
       "1            0      1     1       1       0        1      0  ...     0   \n",
       "2            0      1     1       1       0        1      0  ...     0   \n",
       "3            0      1     0       1       0        1      0  ...     0   \n",
       "4            0      1     0       0       0        0      0  ...     0   \n",
       "...        ...    ...   ...     ...     ...      ...    ...  ...   ...   \n",
       "58742        0      0     0       1       0        1      0  ...     0   \n",
       "58743        0      1     0       1       0        0      0  ...     0   \n",
       "58744        0      0     0       0       0        0      0  ...     0   \n",
       "58745        0      0     0       0       0        0      0  ...     0   \n",
       "58746        0      0     0       0       0        0      0  ...     0   \n",
       "\n",
       "       Palestine  Qatar  Saudi_Arabia  Sudan  Syria  Tunisia  UAE  Yemen  \\\n",
       "0              0      0             0      0      0        0    0      0   \n",
       "1              1      0             0      1      1        0    0      0   \n",
       "2              1      0             1      0      1        0    0      1   \n",
       "3              1      0             0      0      0        0    0      0   \n",
       "4              0      0             1      0      0        0    0      0   \n",
       "...          ...    ...           ...    ...    ...      ...  ...    ...   \n",
       "58742          1      0             0      0      1        0    0      0   \n",
       "58743          1      0             1      1      0        0    0      0   \n",
       "58744          0      0             1      0      0        0    0      0   \n",
       "58745          0      0             0      1      0        0    0      0   \n",
       "58746          0      0             0      0      0        1    0      0   \n",
       "\n",
       "       Computed  \n",
       "0           yes  \n",
       "1           yes  \n",
       "2           yes  \n",
       "3           yes  \n",
       "4           yes  \n",
       "...         ...  \n",
       "58742       yes  \n",
       "58743       yes  \n",
       "58744       yes  \n",
       "58745       yes  \n",
       "58746       yes  \n",
       "\n",
       "[31760 rows x 21 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_columns = ['Algeria', 'Bahrain', 'Egypt', 'Iraq', 'Jordan', 'Kuwait', 'Lebanon', \n",
    "                 'Libya', 'Morocco', 'Oman', 'Palestine', 'Qatar', 'Saudi_Arabia', \n",
    "                 'Sudan', 'Syria', 'Tunisia', 'UAE', 'Yemen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['dialect_sum'] = dataset[label_columns].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dialect_sum\n",
       "1     14829\n",
       "2      3220\n",
       "4      2580\n",
       "3      2329\n",
       "7      1794\n",
       "6      1709\n",
       "5      1416\n",
       "8      1108\n",
       "9       536\n",
       "15      532\n",
       "18      502\n",
       "12      334\n",
       "13      273\n",
       "14      256\n",
       "10      168\n",
       "11      137\n",
       "17       26\n",
       "16       11\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['dialect_sum'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_samples = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dialect_sum\n",
      "1     14829\n",
      "2      3220\n",
      "3      2329\n",
      "4      2580\n",
      "5      1416\n",
      "6      1709\n",
      "7      1794\n",
      "8      1108\n",
      "9       536\n",
      "10      168\n",
      "11      137\n",
      "12      334\n",
      "13      273\n",
      "14      256\n",
      "15      532\n",
      "18      539\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for index, row in dataset.iterrows():\n",
    "    if row['dialect_sum'] in [16, 17]:\n",
    "        # Update `dialect_sum` to 18\n",
    "        dataset.at[index, 'dialect_sum'] = 18\n",
    "        \n",
    "        # Set all dialect columns with 0 to 1\n",
    "        dataset.loc[index, label_columns] = row[label_columns].apply(lambda x: 1 if x == 0 else x)\n",
    "\n",
    "# Verify the changes\n",
    "print(dataset['dialect_sum'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced rows with dialect_sum = 1:\n",
      "Algeria         150\n",
      "Bahrain         150\n",
      "Egypt           150\n",
      "Iraq            150\n",
      "Jordan          150\n",
      "Kuwait          150\n",
      "Lebanon         150\n",
      "Libya           150\n",
      "Morocco         150\n",
      "Oman            150\n",
      "Palestine       150\n",
      "Qatar           150\n",
      "Saudi_Arabia    150\n",
      "Sudan           150\n",
      "Syria           150\n",
      "Tunisia         150\n",
      "UAE             150\n",
      "Yemen           150\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Define dialect columns\n",
    "dialect_columns = ['Algeria', 'Bahrain', 'Egypt', 'Iraq', 'Jordan', 'Kuwait',\n",
    "       'Lebanon', 'Libya', 'Morocco', 'Oman', 'Palestine', 'Qatar',\n",
    "       'Saudi_Arabia', 'Sudan', 'Syria', 'Tunisia', 'UAE', 'Yemen']\n",
    "\n",
    "# Filter rows with `dialect_sum` equal to 1\n",
    "rows_with_sum_1 = dataset[dataset['dialect_sum'] == 1]\n",
    "\n",
    "# Create an empty DataFrame to store balanced rows\n",
    "balanced_rows = pd.DataFrame()\n",
    "\n",
    "# Balance rows across all dialects\n",
    "for dialect in dialect_columns:\n",
    "    # Select rows where the specific dialect column is 1\n",
    "    dialect_rows = rows_with_sum_1[rows_with_sum_1[dialect] == 1]\n",
    "    \n",
    "    # Resample to the size of the smallest group\n",
    "    resampled_rows = resample(dialect_rows, replace=True, \n",
    "                              n_samples=150, \n",
    "                              random_state=42)\n",
    "    balanced_rows = pd.concat([balanced_rows, resampled_rows])\n",
    "\n",
    "# Combine balanced rows with the rest of the dataset\n",
    "other_rows = dataset[dataset['dialect_sum'] != 1]\n",
    "balanced_dataset = pd.concat([balanced_rows, other_rows]).reset_index(drop=True)\n",
    "\n",
    "# Verify the distribution\n",
    "print(\"Balanced rows with dialect_sum = 1:\")\n",
    "print(balanced_rows[dialect_columns].sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced dataset distribution:\n",
      "dialect_sum\n",
      "3     1500\n",
      "7     1500\n",
      "14    1500\n",
      "9     1500\n",
      "12    1500\n",
      "2     1500\n",
      "11    1500\n",
      "18    1500\n",
      "10    1500\n",
      "15    1500\n",
      "4     1500\n",
      "8     1500\n",
      "6     1500\n",
      "5     1500\n",
      "13    1500\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "import pandas as pd\n",
    "\n",
    "# Define the desired number of samples per group\n",
    "desired_samples = 1500\n",
    "\n",
    "# Filter rows with `dialect_sum` values between 2-16 (inclusive) and 18, excluding 1, 16, and 17\n",
    "filtered_dataset = dataset[dataset['dialect_sum'].isin(list(range(2, 16)) + [18])]\n",
    "\n",
    "# Create an empty DataFrame for the balanced dataset\n",
    "balanced_dataset_2 = pd.DataFrame()\n",
    "\n",
    "# Group by `dialect_sum` and resample\n",
    "for value, group in filtered_dataset.groupby('dialect_sum'):\n",
    "    if len(group) > desired_samples:\n",
    "        # If group is larger than desired samples, undersample\n",
    "        group = resample(group, replace=False, n_samples=desired_samples, random_state=42)\n",
    "    elif len(group) < desired_samples:\n",
    "        # If group is smaller than desired samples, oversample\n",
    "        group = resample(group, replace=True, n_samples=desired_samples, random_state=42)\n",
    "    balanced_dataset_2 = pd.concat([balanced_dataset_2, group])\n",
    "\n",
    "# Shuffle the dataset\n",
    "balanced_dataset_2 = balanced_dataset_2.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Verify the distribution\n",
    "print(\"Balanced dataset distribution:\")\n",
    "print(balanced_dataset_2['dialect_sum'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_dataset['dialect_sum'].value_counts()\n",
    "balanced_dataset = balanced_dataset[balanced_dataset['dialect_sum'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = pd.concat([balanced_dataset, balanced_dataset_2]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dialect_sum\n",
       "1     2700\n",
       "3     1500\n",
       "7     1500\n",
       "14    1500\n",
       "9     1500\n",
       "12    1500\n",
       "2     1500\n",
       "11    1500\n",
       "18    1500\n",
       "10    1500\n",
       "15    1500\n",
       "4     1500\n",
       "8     1500\n",
       "6     1500\n",
       "5     1500\n",
       "13    1500\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset['dialect_sum'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset.to_csv(\"/home/ali.mekky/Documents/NLP/Project/NADI2024/subtask1/multilabel/NADIcombined_oversampled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at CAMeL-Lab/bert-base-arabic-camelbert-mix and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/ali.mekky/.conda/envs/DAI/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6616' max='9450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6614/9450 06:11 < 02:39, 17.80 it/s, Epoch 7.00/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.516000</td>\n",
       "      <td>0.361579</td>\n",
       "      <td>0.830442</td>\n",
       "      <td>0.835948</td>\n",
       "      <td>0.182143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.323300</td>\n",
       "      <td>0.299061</td>\n",
       "      <td>0.855057</td>\n",
       "      <td>0.860643</td>\n",
       "      <td>0.249603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.230900</td>\n",
       "      <td>0.272866</td>\n",
       "      <td>0.875414</td>\n",
       "      <td>0.882615</td>\n",
       "      <td>0.357937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.174200</td>\n",
       "      <td>0.270055</td>\n",
       "      <td>0.883791</td>\n",
       "      <td>0.891444</td>\n",
       "      <td>0.438095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.139200</td>\n",
       "      <td>0.282849</td>\n",
       "      <td>0.890102</td>\n",
       "      <td>0.897536</td>\n",
       "      <td>0.475397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.115200</td>\n",
       "      <td>0.269263</td>\n",
       "      <td>0.898637</td>\n",
       "      <td>0.906114</td>\n",
       "      <td>0.516270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.094600</td>\n",
       "      <td>0.287619</td>\n",
       "      <td>0.897030</td>\n",
       "      <td>0.904455</td>\n",
       "      <td>0.540079</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_name = [\"First_200.csv\", \"NADIcombined_cleaned_MULTI_LABEL_MODIFIED_FINAL.csv\", \"First_1000.csv\", \"balanced_multilabel_dataset.csv\", \"balanced_multilabel_dataset_500.csv\"]\n",
    "# dataset_path = f\"/home/lara.hassan/Downloads/NADI2024_subtask1/subtask1/our_data/{file_name[4]}\"\n",
    "dataset_path = '/home/ali.mekky/Documents/NLP/Project/NADI2024/subtask1/multilabel/NADIcombined_oversampled.csv'\n",
    "dev_path = \"/home/ali.mekky/Documents/NLP/Project/NADI2024/subtask1/dev/NADI2024_subtask1_dev2.tsv\"\n",
    "labels = ['Algeria', 'Bahrain', 'Egypt', 'Iraq', 'Jordan', 'Kuwait',\n",
    "       'Lebanon', 'Libya', 'Morocco', 'Oman', 'Palestine', 'Qatar',\n",
    "       'Saudi_Arabia', 'Sudan', 'Syria', 'Tunisia', 'UAE', 'Yemen']\n",
    "\n",
    "trainer = BertTrainer(\n",
    "    training_dataset_path=dataset_path,\n",
    "    model_name=\"CAMeL-Lab/bert-base-arabic-camelbert-mix\",\n",
    "    labels=labels,\n",
    "    threshold=0.3,\n",
    "    exp_num=26\n",
    ")\n",
    "\n",
    "trainer.save_dir = f'./exp_{trainer.exp_num}'\n",
    "\n",
    "trainer.train(\n",
    "    num_train_epochs=10,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,\n",
    "    per_device_train_batch_size=24,\n",
    "    per_device_eval_batch_size=24,\n",
    ")\n",
    "trainer.evaluate(dev_path=dev_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OVERALL SCORES:\n",
      "MACRO AVERAGE PRECISION SCORE: 70.54 %\n",
      "MACRO AVERAGE RECALL SCORE: 44.46 %\n",
      "MACRO AVERAGE F1-SCORE: 52.90 %\n",
      "MACRO AVERAGE ACCURACY: 72.50 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "scorer_script = \"/home/ali.mekky/Documents/NLP/Project/NADI2024/subtask1/NADI2024-ST1-Scorer.py\"\n",
    "gold_file = \"/home/ali.mekky/Documents/NLP/Project/NADI2024/subtask1/sample_submission/NADI2024_subtask1_dev2_gold.txt\"\n",
    "predictions_file = \"/home/ali.mekky/Documents/NLP/Project/Cross-Country-Dialectal-Arabic-Identification/exp_24/marbert_finetuned_epochs_3_eval_f1_0.9034_greater_threshold_0.3/CAMeL-Lab-bert-base-arabic-camelbert-mix-experiment-24_predictions.txt\"\n",
    "!python3 \"{scorer_script}\" \"{gold_file}\" \"{predictions_file}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
